[
  {
    "path": "qiskit_algorithms/gradients/base/base_sampler_gradient.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\"\"\"\nAbstract base class of gradient for ``Sampler``.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom typing import Any\n\nfrom qiskit.circuit import Parameter, ParameterExpression, QuantumCircuit\nfrom qiskit.primitives import BaseSamplerV2\nfrom qiskit.transpiler.passes import TranslateParameterizedGates\n\nfrom .sampler_gradient_result import SamplerGradientResult\nfrom ..utils import (\n    GradientCircuit,\n    _assign_unique_parameters,\n    _make_gradient_parameters,\n    _make_gradient_parameter_values,\n)\n\nfrom ...algorithm_job import AlgorithmJob\nfrom ...custom_types import Transpiler\nfrom ...utils.circuit_key import _circuit_key\n\n\nclass BaseSamplerGradient(ABC):\n    \"\"\"Base class for a ``SamplerGradient`` to compute the gradients of the sampling probability.\"\"\"\n\n    def __init__(\n        self,\n        sampler: BaseSamplerV2,\n        shots: int | None = None,\n        *,\n        transpiler: Transpiler | None = None,\n        transpiler_options: dict[str, Any] | None = None,\n    ):\n        \"\"\"\n        Args:\n            sampler: The sampler used to compute the gradients.\n            shots: Number of shots to be used by the underlying Sampler. If provided, this number\n                takes precedence over the default number of shots of the primitive. Otherwise, the\n                default number of shots of the primitive is used.\n            transpiler: An optional object with a `run` method allowing to transpile the circuits\n                that are run when using this algorithm. If set to `None`, these won't be\n                transpiled.\n            transpiler_options: A dictionary of options to be passed to the transpiler's `run`\n                method as keyword arguments.\n        \"\"\"\n        self._sampler: BaseSamplerV2 = sampler\n        self._shots = shots\n        self._gradient_circuit_cache: dict[tuple, GradientCircuit] = {}\n\n        self._transpiler = transpiler\n        self._transpiler_options = transpiler_options if transpiler_options is not None else {}\n\n    def run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter] | None] | None = None,\n        *,\n        shots: int | Sequence[int] | None = None,\n    ) -> AlgorithmJob:\n        \"\"\"Run the job of the sampler gradient on the given circuits.\n\n        Args:\n            circuits: The list of quantum circuits to compute the gradients.\n            parameter_values: The list of parameter values to be bound to the circuit.\n            parameters: The sequence of parameters to calculate only the gradients of\n                the specified parameters. Each sequence of parameters corresponds to a circuit in\n                ``circuits``. Defaults to None, which means that the gradients of all parameters in\n                each circuit are calculated. None in the sequence means that the gradients of all\n                parameters in the corresponding circuit are calculated.\n            shots: Number of shots to be used by the underlying Sampler. If a single integer is\n                provided, this number will be used for all circuits. If a sequence of integers is\n                provided, they will be used on a per-circuit basis. If not set, the gradient's default\n                number of shots will be used for all circuits, and if that is None (not set) then the\n                underlying primitive's default number of shots will be used for all circuits.\n        Returns:\n            The job object of the gradients of the sampling probability. The i-th result\n            corresponds to ``circuits[i]`` evaluated with parameters bound as ``parameter_values[i]``.\n            The j-th quasi-probability distribution in the i-th result corresponds to the gradients of\n            the sampling probability for the j-th parameter in ``circuits[i]``.\n\n        Raises:\n            ValueError: Invalid arguments are given.\n        \"\"\"\n        if isinstance(circuits, QuantumCircuit):\n            # Allow a single circuit to be passed in.\n            circuits = (circuits,)\n        if parameters is None:\n            # If parameters is None, we calculate the gradients of all parameters in each circuit.\n            parameters = [circuit.parameters for circuit in circuits]\n        else:\n            # If parameters is not None, we calculate the gradients of the specified parameters.\n            # None in parameters means that the gradients of all parameters in the corresponding\n            # circuit are calculated.\n            parameters = [\n                params if params is not None else circuits[i].parameters\n                for i, params in enumerate(parameters)\n            ]\n        # Validate the arguments.\n        self._validate_arguments(circuits, parameter_values, parameters)\n\n        if shots is None:\n            shots = self.shots\n\n        job = AlgorithmJob(self._run, circuits, parameter_values, parameters, shots=shots)\n        job._submit()\n        return job\n\n    @abstractmethod\n    def _run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        shots: int | Sequence[int] | None,\n    ) -> SamplerGradientResult:\n        \"\"\"Compute the sampler gradients on the given circuits.\"\"\"\n        raise NotImplementedError()\n\n    def _preprocess(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        supported_gates: Sequence[str],\n    ) -> tuple[Sequence[QuantumCircuit], Sequence[Sequence[float]], Sequence[Sequence[Parameter]]]:\n        \"\"\"Preprocess the gradient. This makes a gradient circuit for each circuit. The gradient\n        circuit is a transpiled circuit by using the supported gates, and has unique parameters.\n        ``parameter_values`` and ``parameters`` are also updated to match the gradient circuit.\n\n        Args:\n            circuits: The list of quantum circuits to compute the gradients.\n            parameter_values: The list of parameter values to be bound to the circuit.\n            parameters: The sequence of parameters to calculate only the gradients of the specified\n                parameters.\n            supported_gates: The supported gates used to transpile the circuit.\n\n        Returns:\n            The list of gradient circuits, the list of parameter values, and the list of parameters.\n            parameter_values and parameters are updated to match the gradient circuit.\n        \"\"\"\n        translator = TranslateParameterizedGates(supported_gates)\n        g_circuits: list[QuantumCircuit] = []\n        g_parameter_values: list[Sequence[float]] = []\n        g_parameters: list[Sequence[Parameter]] = []\n        for circuit, parameter_value_, parameters_ in zip(circuits, parameter_values, parameters):\n            circuit_key = _circuit_key(circuit)\n            if circuit_key not in self._gradient_circuit_cache:\n                unrolled = translator(circuit)\n                self._gradient_circuit_cache[circuit_key] = _assign_unique_parameters(unrolled)\n            gradient_circuit = self._gradient_circuit_cache[circuit_key]\n            g_circuits.append(gradient_circuit.gradient_circuit)\n            g_parameter_values.append(\n                _make_gradient_parameter_values(  # type: ignore[arg-type]\n                    circuit, gradient_circuit, parameter_value_\n                )\n            )\n            g_parameters.append(_make_gradient_parameters(gradient_circuit, parameters_))\n        return g_circuits, g_parameter_values, g_parameters\n\n    def _postprocess(\n        self,\n        results: SamplerGradientResult,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter] | None],\n    ) -> SamplerGradientResult:\n        \"\"\"Postprocess the gradient. This computes the gradient of the original circuit from the\n        gradient of the gradient circuit by using the chain rule.\n\n        Args:\n            results: The results of the gradient of the gradient circuits.\n            circuits: The list of quantum circuits to compute the gradients.\n            parameter_values: The list of parameter values to be bound to the circuit.\n            parameters: The sequence of parameters to calculate only the gradients of the specified\n                parameters.\n\n        Returns:\n            The results of the gradient of the original circuits.\n        \"\"\"\n        gradients, metadata = [], []\n        for idx, (circuit, parameter_values_, parameters_) in enumerate(\n            zip(circuits, parameter_values, parameters)\n        ):\n            gradient_circuit = self._gradient_circuit_cache[_circuit_key(circuit)]\n            g_parameters = _make_gradient_parameters(gradient_circuit, parameters_)\n            # Make a map from the gradient parameter to the respective index in the gradient.\n            g_parameter_indices = {param: i for i, param in enumerate(g_parameters)}\n            # Compute the original gradient from the gradient of the gradient circuit\n            # by using the chain rule.\n            gradient = []\n            for parameter in parameters_:\n                grad_dist: dict[int, float] = defaultdict(float)\n                for g_parameter, coeff in gradient_circuit.parameter_map[parameter]:\n                    # Compute the coefficient\n                    if isinstance(coeff, ParameterExpression):\n                        local_map = {\n                            p: parameter_values_[circuit.parameters.data.index(p)]\n                            for p in coeff.parameters\n                        }\n                        bound_coeff = coeff.bind(local_map)\n                    else:\n                        bound_coeff = coeff\n                    # The original gradient is a sum of the gradients of the parameters in the\n                    # gradient circuit multiplied by the coefficients.\n                    unique_gradient = results.gradients[idx][g_parameter_indices[g_parameter]]\n                    for key, value in unique_gradient.items():\n                        grad_dist[key] += float(bound_coeff) * value\n                gradient.append(dict(grad_dist))\n            gradients.append(gradient)\n            metadata.append([{\"parameters\": parameters_}])\n        return SamplerGradientResult(gradients=gradients, metadata=metadata, shots=results.shots)\n\n    @staticmethod\n    def _validate_arguments(\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n    ) -> None:\n        \"\"\"Validate the arguments of the ``run`` method.\n\n        Args:\n            circuits: The list of quantum circuits to compute the gradients.\n            parameter_values: The list of parameter values to be bound to the circuit.\n            parameters: The sequence of parameters to calculate only the gradients of the specified\n                parameters.\n\n        Raises:\n            ValueError: Invalid arguments are given.\n        \"\"\"\n        if len(circuits) != len(parameter_values):\n            raise ValueError(\n                f\"The number of circuits ({len(circuits)}) does not match \"\n                f\"the number of parameter value sets ({len(parameter_values)}).\"\n            )\n\n        for i, (circuit, parameter_value) in enumerate(zip(circuits, parameter_values)):\n            if not circuit.num_parameters:\n                raise ValueError(f\"The {i}-th circuit is not parameterised.\")\n\n            if len(parameter_value) != circuit.num_parameters:\n                raise ValueError(\n                    f\"The number of values ({len(parameter_value)}) does not match \"\n                    f\"the number of parameters ({circuit.num_parameters}) for the {i}-th circuit.\"\n                )\n\n        if len(circuits) != len(parameters):\n            raise ValueError(\n                f\"The number of circuits ({len(circuits)}) does not match \"\n                f\"the number of the specified parameter sets ({len(parameters)}).\"\n            )\n\n        for i, (circuit, parameters_) in enumerate(zip(circuits, parameters)):\n            if not set(parameters_).issubset(circuit.parameters):\n                raise ValueError(\n                    f\"The {i}-th parameter set contains parameters not present in the \"\n                    f\"{i}-th circuit.\"\n                )\n\n    @property\n    def shots(self) -> int | None:\n        \"\"\"Return the number of shots used by the `run` method of the Sampler primitive. If None,\n        the default number of shots of the primitive is used.\n\n        Returns:\n            The default number of shots.\n        \"\"\"\n        return self._shots\n\n    @shots.setter\n    def shots(self, shots: int | None):\n        \"\"\"Update the gradient's default number of shots setting.\n\n        Args:\n            shots: The new default number of shots.\n        \"\"\"\n\n        self._shots = shots\n",
    "is_binary": false,
    "size": 13656
  },
  {
    "path": "qiskit_algorithms/gradients/base/estimator_gradient_result.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\"\"\"\nEstimator result class\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Sequence\n\nimport numpy as np\n\n\n@dataclass(frozen=True)\nclass EstimatorGradientResult:\n    \"\"\"Result of EstimatorGradient.\"\"\"\n\n    gradients: list[np.ndarray]\n    \"\"\"The gradients of the expectation values.\"\"\"\n    metadata: list[dict[str, Any]]\n    \"\"\"Additional information about the job.\"\"\"\n    precision: float | Sequence[float]\n    \"\"\"Precision for the execution of the job.\"\"\"\n",
    "is_binary": false,
    "size": 1000
  },
  {
    "path": "qiskit_algorithms/gradients/base/qgt_result.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\"\"\"\nQGT result class\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Sequence\n\nimport numpy as np\n\nfrom ..utils import DerivativeType\n\n\n@dataclass(frozen=True)\nclass QGTResult:\n    \"\"\"Result of QGT.\"\"\"\n\n    qgts: list[np.ndarray]\n    \"\"\"The QGT.\"\"\"\n    derivative_type: DerivativeType\n    \"\"\"The type of derivative.\"\"\"\n    metadata: list[dict[str, Any]] | list[list[dict[str, Any]]]\n    \"\"\"Additional information about the job.\"\"\"\n    precision: float | Sequence[float]\n    \"\"\"Precision for the execution of the job.\"\"\"\n",
    "is_binary": false,
    "size": 1064
  },
  {
    "path": "qiskit_algorithms/gradients/base/sampler_gradient_result.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\"\"\"\nSampler result class\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Sequence\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass SamplerGradientResult:\n    \"\"\"Result of SamplerGradient.\"\"\"\n\n    gradients: list[list[dict[int, float]]]\n    \"\"\"The gradients of the sample probabilities.\"\"\"\n    metadata: list[dict[str, Any]] | list[list[dict[str, Any]]]\n    \"\"\"Additional information about the job.\"\"\"\n    shots: int | Sequence[int]\n    \"\"\"Primitive number of shots for the execution of the job.\"\"\"\n",
    "is_binary": false,
    "size": 1025
  },
  {
    "path": "qiskit_algorithms/gradients/base/__init__.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2023\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n",
    "is_binary": false,
    "size": 492
  },
  {
    "path": "qiskit_algorithms/gradients/finite_diff/finite_diff_estimator_gradient.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\"\"\"Gradient of Sampler with Finite difference method.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import Literal, Any\n\nimport numpy as np\n\nfrom qiskit.circuit import Parameter, QuantumCircuit\nfrom qiskit.primitives import BaseEstimatorV2\nfrom qiskit.quantum_info.operators.base_operator import BaseOperator\n\nfrom ..base.base_estimator_gradient import BaseEstimatorGradient\nfrom ..base.estimator_gradient_result import EstimatorGradientResult\nfrom ...custom_types import Transpiler\n\nfrom ...exceptions import AlgorithmError\n\n\nclass FiniteDiffEstimatorGradient(BaseEstimatorGradient):\n    \"\"\"\n    Compute the gradients of the expectation values by finite difference method [1].\n\n    **Reference:**\n    [1] `Finite difference method <https://en.wikipedia.org/wiki/Finite_difference_method>`_\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator: BaseEstimatorV2,\n        epsilon: float,\n        precision: float | None = None,\n        *,\n        method: Literal[\"central\", \"forward\", \"backward\"] = \"central\",\n        transpiler: Transpiler | None = None,\n        transpiler_options: dict[str, Any] | None = None,\n    ):\n        r\"\"\"\n        Args:\n            estimator: The estimator used to compute the gradients.\n            epsilon: The offset size for the finite difference gradients.\n            precision: Precision to be used by the underlying Estimator. If provided, this number\n                takes precedence over the default precision of the primitive. If None, the default\n                precision of the primitive is used.\n            method: The computation method of the gradients.\n\n                    - ``central`` computes :math:`\\frac{f(x+e)-f(x-e)}{2e}`,\n                    - ``forward`` computes :math:`\\frac{f(x+e) - f(x)}{e}`,\n                    - ``backward`` computes :math:`\\frac{f(x)-f(x-e)}{e}`\n\n                where :math:`e` is epsilon.\n            transpiler: An optional object with a `run` method allowing to transpile the circuits\n                that are run when using this algorithm. If set to `None`, these won't be\n                transpiled.\n            transpiler_options: A dictionary of options to be passed to the transpiler's `run`\n                method as keyword arguments.\n\n        Raises:\n            ValueError: If ``epsilon`` is not positive.\n            TypeError: If ``method`` is invalid.\n        \"\"\"\n        if epsilon <= 0:\n            raise ValueError(f\"epsilon ({epsilon}) should be positive.\")\n        self._epsilon = epsilon\n        if method not in (\"central\", \"forward\", \"backward\"):\n            raise TypeError(\n                f\"The argument method should be central, forward, or backward: {method} is given.\"\n            )\n        self._method = method\n        super().__init__(\n            estimator, precision, transpiler=transpiler, transpiler_options=transpiler_options\n        )\n\n    def _run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        observables: Sequence[BaseOperator],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        precision: float | Sequence[float] | None,\n    ) -> EstimatorGradientResult:\n        \"\"\"Compute the estimator gradients on the given circuits.\"\"\"\n        metadata = []\n        all_n = []\n        has_transformed_precision = False\n\n        if isinstance(precision, float) or precision is None:\n            precision = [precision] * len(circuits)\n            has_transformed_precision = True\n\n        if self._transpiler is not None:\n            circuits = self._transpiler.run(circuits, **self._transpiler_options)\n            observables = [\n                obs.apply_layout(circuit.layout) for (circuit, obs) in zip(circuits, observables)\n            ]\n\n        pubs = []\n\n        for circuit, observable, parameter_values_, parameters_, precision_ in zip(\n            circuits, observables, parameter_values, parameters, precision\n        ):\n            # Indices of parameters to be differentiated\n            indices = [circuit.parameters.data.index(p) for p in parameters_]\n            metadata.append({\"parameters\": parameters_})\n\n            # Combine inputs into a single job to reduce overhead.\n            offset = np.identity(circuit.num_parameters)[indices, :]\n            if self._method == \"central\":\n                plus = parameter_values_ + self._epsilon * offset\n                minus = parameter_values_ - self._epsilon * offset\n                n = 2 * len(indices)\n                all_n.append(n)\n                pubs.append((circuit, observable, plus.tolist() + minus.tolist(), precision_))\n            elif self._method == \"forward\":\n                plus = parameter_values_ + self._epsilon * offset\n                n = len(indices) + 1\n                all_n.append(n)\n                pubs.append((circuit, observable, [parameter_values_] + plus.tolist(), precision_))\n            elif self._method == \"backward\":\n                minus = parameter_values_ - self._epsilon * offset\n                n = len(indices) + 1\n                all_n.append(n)\n                pubs.append((circuit, observable, [parameter_values_] + minus.tolist(), precision_))\n\n        # Run the single job with all circuits.\n        job = self._estimator.run(pubs)\n        try:\n            results = job.result()\n        except Exception as exc:\n            raise AlgorithmError(\"Estimator job failed.\") from exc\n\n        # Compute the gradients\n        gradients = []\n        partial_sum_n = 0\n        for n, result_n in zip(all_n, results):\n            # Ensure gradient is always defined for the append below after the if block\n            # otherwise lint errors out. I left the if block as it has been coded though\n            # as the values are checked in the constructor I could have made the last elif\n            # a simple else instead of defining this here.\n            gradient = None\n            result = result_n.data.evs\n            if self._method == \"central\":\n                gradient = (result[: n // 2] - result[n // 2 :]) / (2 * self._epsilon)\n            elif self._method == \"forward\":\n                gradient = (result[1:] - result[0]) / self._epsilon\n            elif self._method == \"backward\":\n                gradient = (result[0] - result[1:]) / self._epsilon\n            partial_sum_n += n\n            gradients.append(gradient)\n\n        if has_transformed_precision:\n            precision = precision[0]\n\n            if precision is None:\n                precision = results[0].metadata[\"target_precision\"]\n        else:\n            for i, (precision_, result) in enumerate(zip(precision, results)):\n                if precision_ is None:\n                    precision[i] = results[i].metadata[\"target_precision\"]\n\n        return EstimatorGradientResult(gradients=gradients, metadata=metadata, precision=precision)\n",
    "is_binary": false,
    "size": 7382
  },
  {
    "path": "qiskit_algorithms/gradients/finite_diff/finite_diff_sampler_gradient.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n\"\"\"Gradient of Sampler with Finite difference method.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom typing import Literal, Sequence, Any\n\nimport numpy as np\n\nfrom qiskit.circuit import Parameter, QuantumCircuit\nfrom qiskit.primitives import BaseSamplerV2\n\nfrom ..base.base_sampler_gradient import BaseSamplerGradient\nfrom ..base.sampler_gradient_result import SamplerGradientResult\nfrom ...custom_types import Transpiler\n\nfrom ...exceptions import AlgorithmError\n\n\nclass FiniteDiffSamplerGradient(BaseSamplerGradient):\n    \"\"\"\n    Compute the gradients of the sampling probability by finite difference method [1].\n\n    **Reference:**\n    [1] `Finite difference method <https://en.wikipedia.org/wiki/Finite_difference_method>`_\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: BaseSamplerV2,\n        epsilon: float,\n        shots: int | None = None,\n        *,\n        method: Literal[\"central\", \"forward\", \"backward\"] = \"central\",\n        transpiler: Transpiler | None = None,\n        transpiler_options: dict[str, Any] | None = None,\n    ):\n        r\"\"\"\n        Args:\n            sampler: The sampler used to compute the gradients.\n            epsilon: The offset size for the finite difference gradients.\n            shots: Number of shots to be used by the underlying Sampler. If provided, this number\n                takes precedence over the default precision of the primitive. If None, the default\n                number of shots of the primitive is used.\n            method: The computation method of the gradients.\n\n                    - ``central`` computes :math:`\\frac{f(x+e)-f(x-e)}{2e}`,\n                    - ``forward`` computes :math:`\\frac{f(x+e) - f(x)}{e}`,\n                    - ``backward`` computes :math:`\\frac{f(x)-f(x-e)}{e}`\n\n                where :math:`e` is epsilon.\n            transpiler: An optional object with a `run` method allowing to transpile the circuits\n                that are run when using this algorithm. If set to `None`, these won't be\n                transpiled.\n            transpiler_options: A dictionary of options to be passed to the transpiler's `run`\n                method as keyword arguments.\n\n        Raises:\n            ValueError: If ``epsilon`` is not positive.\n            TypeError: If ``method`` is invalid.\n        \"\"\"\n        if epsilon <= 0:\n            raise ValueError(f\"epsilon ({epsilon}) should be positive.\")\n        self._epsilon = epsilon\n        if method not in (\"central\", \"forward\", \"backward\"):\n            raise TypeError(\n                f\"The argument method should be central, forward, or backward: {method} is given.\"\n            )\n        self._method = method\n        super().__init__(\n            sampler, shots, transpiler=transpiler, transpiler_options=transpiler_options\n        )\n\n    def _run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter] | None] | None,\n        *,\n        shots: int | Sequence[int] | None,\n    ) -> SamplerGradientResult:\n        \"\"\"Compute the sampler gradients on the given circuits.\"\"\"\n        metadata = []\n        all_n = []\n        has_transformed_shots = False\n\n        if isinstance(shots, int) or shots is None:\n            shots = [shots] * len(circuits)\n            has_transformed_shots = True\n\n        if self._transpiler is not None:\n            circuits = self._transpiler.run(circuits, **self._transpiler_options)\n\n        pubs = []\n        for circuit, parameter_values_, parameters_, shots_ in zip(\n            circuits, parameter_values, parameters, shots\n        ):\n            # Indices of parameters to be differentiated\n            indices = [circuit.parameters.data.index(p) for p in parameters_]\n            metadata.append({\"parameters\": parameters_})\n            # Combine inputs into a single job to reduce overhead.\n            offset = np.identity(circuit.num_parameters)[indices, :]\n            if self._method == \"central\":\n                plus = parameter_values_ + self._epsilon * offset\n                minus = parameter_values_ - self._epsilon * offset\n                n = 2 * len(indices)\n                all_n.append(n)\n                pubs.append((circuit, plus.tolist() + minus.tolist(), shots_))\n            elif self._method == \"forward\":\n                plus = parameter_values_ + self._epsilon * offset\n                n = len(indices) + 1\n                pubs.append((circuit, [parameter_values_] + plus.tolist(), shots_))\n                all_n.append(n)\n            elif self._method == \"backward\":\n                minus = parameter_values_ - self._epsilon * offset\n                n = len(indices) + 1\n                pubs.append((circuit, [parameter_values_] + minus.tolist(), shots_))\n                all_n.append(n)\n\n        # Run the single job with all circuits.\n        job = self._sampler.run(pubs)\n        try:\n            results = job.result()\n        except Exception as exc:\n            raise AlgorithmError(\"Sampler job failed.\") from exc\n\n        # Compute the gradients.\n        gradients = []\n\n        for n, result_n in zip(all_n, results):\n            gradient = []\n            result = [\n                {label: value / res.num_shots for label, value in res.get_int_counts().items()}\n                for res in getattr(result_n.data, next(iter(result_n.data)))\n            ]\n            if self._method == \"central\":\n                for dist_plus, dist_minus in zip(result[: n // 2], result[n // 2 :]):\n                    grad_dist: dict[int, float] = defaultdict(float)\n                    for key, value in dist_plus.items():\n                        grad_dist[key] += value / (2 * self._epsilon)\n                    for key, value in dist_minus.items():\n                        grad_dist[key] -= value / (2 * self._epsilon)\n                    gradient.append(dict(grad_dist))\n            elif self._method == \"forward\":\n                dist_zero = result[0]\n                for dist_plus in result[1:]:\n                    grad_dist = defaultdict(float)\n                    for key, value in dist_plus.items():\n                        grad_dist[key] += value / self._epsilon\n                    for key, value in dist_zero.items():\n                        grad_dist[key] -= value / self._epsilon\n                    gradient.append(dict(grad_dist))\n            elif self._method == \"backward\":\n                dist_zero = result[0]\n                for dist_minus in result[1:]:\n                    grad_dist = defaultdict(float)\n                    for key, value in dist_zero.items():\n                        grad_dist[key] += value / self._epsilon\n                    for key, value in dist_minus.items():\n                        grad_dist[key] -= value / self._epsilon\n                    gradient.append(dict(grad_dist))\n\n            gradients.append(gradient)\n\n        if has_transformed_shots:\n            shots = shots[0]\n\n            if shots is None:\n                shots = results[0].metadata[\"shots\"]\n        else:\n            for i, (shots_, result) in enumerate(zip(shots, results)):\n                if shots_ is None:\n                    shots[i] = result.metadata[\"shots\"]\n\n        return SamplerGradientResult(gradients=gradients, metadata=metadata, shots=shots)\n",
    "is_binary": false,
    "size": 7816
  },
  {
    "path": "qiskit_algorithms/gradients/finite_diff/__init__.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2023\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n",
    "is_binary": false,
    "size": 492
  },
  {
    "path": "qiskit_algorithms/gradients/lin_comb/lin_comb_estimator_gradient.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\"\"\"\nGradient of probabilities with linear combination of unitaries (LCU)\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import Any\n\nimport numpy as np\nfrom qiskit.circuit import Parameter, QuantumCircuit\nfrom qiskit.primitives import BaseEstimatorV2\nfrom qiskit.quantum_info import SparsePauliOp\nfrom qiskit.quantum_info.operators.base_operator import BaseOperator\n\nfrom ..base.base_estimator_gradient import BaseEstimatorGradient\nfrom ..base.estimator_gradient_result import EstimatorGradientResult\nfrom ..utils import DerivativeType, _make_lin_comb_gradient_circuit, _make_lin_comb_observables\nfrom ...custom_types import Transpiler\nfrom ...run_estimator_job import run_estimator_job\nfrom ...utils.circuit_key import _circuit_key\n\n\nclass LinCombEstimatorGradient(BaseEstimatorGradient):\n    \"\"\"Compute the gradients of the expectation values.\n    This method employs a linear combination of unitaries [1].\n\n    **Reference:**\n    [1] Schuld et al., Evaluating analytic gradients on quantum hardware, 2018\n    `arXiv:1811.11184 <https://arxiv.org/pdf/1811.11184.pdf>`_\n    \"\"\"\n\n    SUPPORTED_GATES = [\n        \"rx\",\n        \"ry\",\n        \"rz\",\n        \"rzx\",\n        \"rzz\",\n        \"ryy\",\n        \"rxx\",\n        \"cx\",\n        \"cy\",\n        \"cz\",\n        \"ccx\",\n        \"swap\",\n        \"iswap\",\n        \"h\",\n        \"t\",\n        \"s\",\n        \"sdg\",\n        \"x\",\n        \"y\",\n        \"z\",\n    ]\n\n    def __init__(\n        self,\n        estimator: BaseEstimatorV2,\n        precision: float | None = None,\n        derivative_type: DerivativeType = DerivativeType.REAL,\n        *,\n        transpiler: Transpiler | None = None,\n        transpiler_options: dict[str, Any] | None = None,\n    ):\n        r\"\"\"\n        Args:\n            estimator: The estimator used to compute the gradients.\n            derivative_type: The type of derivative. Can be either ``DerivativeType.REAL``\n                ``DerivativeType.IMAG``, or ``DerivativeType.COMPLEX``. Defaults to\n                ``DerivativeType.REAL``.\n\n                    - ``DerivativeType.REAL`` computes :math:`2 \\mathrm{Re}[\u27e8\u03c8(\u03c9)|O(\u03b8)|d\u03c9 \u03c8(\u03c9)\u3009]`.\n                    - ``DerivativeType.IMAG`` computes :math:`2 \\mathrm{Im}[\u27e8\u03c8(\u03c9)|O(\u03b8)|d\u03c9 \u03c8(\u03c9)\u3009]`.\n                    - ``DerivativeType.COMPLEX`` computes :math:`2 \u27e8\u03c8(\u03c9)|O(\u03b8)|d\u03c9 \u03c8(\u03c9)\u3009`.\n            precision: Precision to be used by the underlying Estimator. If provided, this number\n                takes precedence over the default precision of the primitive. If None, the default\n                precision of the primitive is used.\n            transpiler: An optional object with a `run` method allowing to transpile the circuits\n                that are produced within this algorithm. If set to `None`, these won't be\n                transpiled.\n            transpiler_options: A dictionary of options to be passed to the transpiler's `run`\n                method as keyword arguments.\n        \"\"\"\n        self._lin_comb_cache: dict[tuple, dict[Parameter, QuantumCircuit]] = {}\n        super().__init__(\n            estimator,\n            precision,\n            derivative_type=derivative_type,\n            transpiler=transpiler,\n            transpiler_options=transpiler_options,\n        )\n\n    @BaseEstimatorGradient.derivative_type.setter  # type: ignore[attr-defined]\n    def derivative_type(self, derivative_type: DerivativeType) -> None:\n        \"\"\"Set the derivative type.\"\"\"\n        self._derivative_type = derivative_type\n\n    def _run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        observables: Sequence[BaseOperator],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        precision: float | Sequence[float] | None,\n    ) -> EstimatorGradientResult:\n        \"\"\"Compute the estimator gradients on the given circuits.\"\"\"\n        g_circuits, g_parameter_values, g_parameters = self._preprocess(\n            circuits, parameter_values, parameters, self.SUPPORTED_GATES\n        )\n        results = self._run_unique(\n            g_circuits, observables, g_parameter_values, g_parameters, precision=precision\n        )\n        return self._postprocess(results, circuits, parameter_values, parameters)\n\n    def _run_unique(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        observables: Sequence[BaseOperator],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        precision: float | Sequence[float] | None,\n    ) -> EstimatorGradientResult:\n        \"\"\"Compute the estimator gradients on the given circuits.\"\"\"\n        has_transformed_precision = False\n\n        if isinstance(precision, float) or precision is None:\n            precision = [precision] * len(circuits)\n            has_transformed_precision = True\n\n        metadata = []\n        all_n = []\n        pubs = []\n\n        if not (len(circuits) == len(observables) == len(parameters) == len(parameter_values)):\n            raise ValueError(\n                f\"circuits, observables, parameters, and parameter_values must have the same length, \"\n                f\"but have respective lengths {len(circuits)},  {len(observables)}, {len(parameters)} \"\n                f\"and {len(parameter_values)}.\"\n            )\n\n        for circuit, observable, parameter_values_, parameters_, precision_ in zip(\n            circuits, observables, parameter_values, parameters, precision\n        ):\n            # Prepare circuits for the gradient of the specified parameters.\n            meta = {\"parameters\": parameters_}\n            circuit_key = _circuit_key(circuit)\n\n            if circuit_key not in self._lin_comb_cache:\n                # Cache the circuits for the linear combination of unitaries.\n                # We only cache the circuits for the specified parameters in the future.\n                self._lin_comb_cache[circuit_key] = _make_lin_comb_gradient_circuit(\n                    circuit, add_measurement=False\n                )\n\n            lin_comb_circuits = self._lin_comb_cache[circuit_key]\n            gradient_circuits = []\n\n            for param in parameters_:\n                gradient_circuits.append(lin_comb_circuits[param])\n\n            n = len(gradient_circuits)\n            # Make the observable as :class:`~qiskit.quantum_info.SparsePauliOp` and\n            # add an ancillary operator to compute the gradient.\n            observable = SparsePauliOp(observable)\n            observable_1, observable_2 = _make_lin_comb_observables(\n                observable, self._derivative_type\n            )\n            # If its derivative type is `DerivativeType.COMPLEX`, calculate the gradient\n            # of the real and imaginary parts separately.\n            meta[\"derivative_type\"] = self.derivative_type\n            metadata.append(meta)\n            # Combine inputs into a single job to reduce overhead.\n            if self._derivative_type == DerivativeType.COMPLEX:\n                all_n.append(2 * n)\n                pubs.extend(\n                    [\n                        (gradient_circuit, observable_1, parameter_values_, precision_)\n                        for gradient_circuit in gradient_circuits\n                    ]\n                )\n                pubs.extend(\n                    [\n                        (gradient_circuit, observable_2, parameter_values_, precision_)\n                        for gradient_circuit in gradient_circuits\n                    ]\n                )\n            else:\n                all_n.append(n)\n                pubs.extend(\n                    [\n                        (gradient_circuit, observable_1, parameter_values_, precision_)\n                        for gradient_circuit in gradient_circuits\n                    ]\n                )\n\n        if self._transpiler is not None:\n            for index, pub in enumerate(pubs):\n                new_circuit = self._transpiler.run(pub[0], **self._transpiler_options)\n                new_observable = pub[1].apply_layout(new_circuit.layout)\n                pubs[index] = (new_circuit, new_observable) + pub[2:]\n\n        # Run the single job with all circuits.\n        results = run_estimator_job(self._estimator, pubs)\n\n        # Compute the gradients.\n        gradients = []\n        partial_sum_n = 0\n\n        for n in all_n:\n            # this disable is needed as Pylint does not understand derivative_type is a property if\n            # it is only defined in the base class and the getter is in the child\n            # pylint: disable=comparison-with-callable\n            if self.derivative_type == DerivativeType.COMPLEX:\n                gradient = np.zeros(n // 2, dtype=\"complex\")\n                gradient.real = np.array(\n                    [result.data.evs for result in results[partial_sum_n : partial_sum_n + n // 2]]\n                )\n                gradient.imag = np.array(\n                    [\n                        result.data.evs\n                        for result in results[partial_sum_n + n // 2 : partial_sum_n + n]\n                    ]\n                )\n\n            else:\n                gradient = np.real(\n                    [result.data.evs for result in results[partial_sum_n : partial_sum_n + n]]\n                )\n\n            partial_sum_n += n\n            gradients.append(gradient)\n\n        if has_transformed_precision:\n            precision = precision[0]\n\n            if precision is None:\n                precision = results[0].metadata[\"target_precision\"]\n        else:\n            for i, (precision_, result) in enumerate(zip(precision, results)):\n                if precision_ is None:\n                    precision[i] = results[i].metadata[\"target_precision\"]\n\n        return EstimatorGradientResult(gradients=gradients, metadata=metadata, precision=precision)\n",
    "is_binary": false,
    "size": 10328
  },
  {
    "path": "qiskit_algorithms/gradients/lin_comb/lin_comb_qgt.py",
    "content": "# This code is part of a Qiskit project.\n#\n# (C) Copyright IBM 2022, 2025.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\"\"\"\nA class for the Linear Combination Quantum Gradient Tensor.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import Any\n\nimport numpy as np\n\nfrom qiskit.circuit import Parameter, QuantumCircuit\nfrom qiskit.primitives import BaseEstimatorV2\nfrom qiskit.quantum_info import SparsePauliOp\n\nfrom ..base.base_qgt import BaseQGT\nfrom .lin_comb_estimator_gradient import LinCombEstimatorGradient\nfrom ..base.qgt_result import QGTResult\nfrom ..utils import DerivativeType, _make_lin_comb_qgt_circuit, _make_lin_comb_observables\nfrom ...custom_types import Transpiler\n\nfrom ...exceptions import AlgorithmError\nfrom ...utils.circuit_key import _circuit_key\n\n\nclass LinCombQGT(BaseQGT):\n    \"\"\"Computes the Quantum Geometric Tensor (QGT) given a pure, parameterized quantum state.\n\n    This method employs a linear combination of unitaries [1].\n\n    **Reference:**\n\n        [1]: Schuld et al., \"Evaluating analytic gradients on quantum hardware\" (2018).\n             `arXiv:1811.11184 <https://arxiv.org/pdf/1811.11184.pdf>`_\n    \"\"\"\n\n    SUPPORTED_GATES = [\n        \"rx\",\n        \"ry\",\n        \"rz\",\n        \"rzx\",\n        \"rzz\",\n        \"ryy\",\n        \"rxx\",\n        \"cx\",\n        \"cy\",\n        \"cz\",\n        \"ccx\",\n        \"swap\",\n        \"iswap\",\n        \"h\",\n        \"t\",\n        \"s\",\n        \"sdg\",\n        \"x\",\n        \"y\",\n        \"z\",\n    ]\n\n    def __init__(\n        self,\n        estimator: BaseEstimatorV2,\n        phase_fix: bool = True,\n        derivative_type: DerivativeType = DerivativeType.COMPLEX,\n        precision: float | None = None,\n        *,\n        transpiler: Transpiler | None = None,\n        transpiler_options: dict[str, Any] | None = None,\n    ):\n        r\"\"\"\n        Args:\n            estimator: The estimator used to compute the QGT.\n            phase_fix: Whether to calculate the second term (phase fix) of the QGT, which is\n                :math:`\\langle\\partial_i \\psi | \\psi \\rangle \\langle\\psi | \\partial_j \\psi \\rangle`.\n                Default to ``True``.\n            derivative_type: The type of derivative. Can be either ``DerivativeType.REAL``\n                ``DerivativeType.IMAG``, or ``DerivativeType.COMPLEX``. Defaults to\n                ``DerivativeType.REAL``.\n\n                - ``DerivativeType.REAL`` computes\n\n                .. math::\n\n                    \\mathrm{Re(QGT)}_{ij}= \\mathrm{Re}[\\langle \\partial_i \\psi | \\partial_j \\psi \\rangle\n                        - \\langle\\partial_i \\psi | \\psi \\rangle \\langle\\psi | \\partial_j \\psi \\rangle].\n\n                - ``DerivativeType.IMAG`` computes\n\n                .. math::\n\n                    \\mathrm{Re(QGT)}_{ij}= \\mathrm{Im}[\\langle \\partial_i \\psi | \\partial_j \\psi \\rangle\n                        - \\langle\\partial_i \\psi | \\psi \\rangle \\langle\\psi | \\partial_j \\psi \\rangle].\n\n                - ``DerivativeType.COMPLEX`` computes\n\n                .. math::\n\n                    \\mathrm{QGT}_{ij}= [\\langle \\partial_i \\psi | \\partial_j \\psi \\rangle\n                        - \\langle\\partial_i \\psi | \\psi \\rangle \\langle\\psi | \\partial_j \\psi \\rangle].\n            precision: Precision to be used by the underlying Estimator. If provided, this number\n                takes precedence over the default precision of the primitive. If None, the default\n                precision of the primitive is used. It will also be used to instantiate the internal\n                gradient.\n            transpiler: An optional object with a `run` method allowing to transpile the circuits\n                that are produced by the internal gradient of this algorithm. If set to `None`,\n                these won't be transpiled.\n            transpiler_options: A dictionary of options to be passed to the transpiler's `run`\n                method as keyword arguments.\n        \"\"\"\n        super().__init__(\n            estimator,\n            phase_fix,\n            derivative_type,\n            precision,\n            transpiler=transpiler,\n            transpiler_options=transpiler_options,\n        )\n        self._gradient = LinCombEstimatorGradient(\n            estimator,\n            derivative_type=DerivativeType.COMPLEX,\n            precision=precision,\n            transpiler=transpiler,\n            transpiler_options=transpiler_options,\n        )\n        self._lin_comb_qgt_circuit_cache: dict[\n            tuple, dict[tuple[Parameter, Parameter], QuantumCircuit]\n        ] = {}\n\n    def _run(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        precision: float | Sequence[float] | None,\n    ) -> QGTResult:\n        \"\"\"Compute the QGT on the given circuits.\"\"\"\n        g_circuits, g_parameter_values, g_parameters = self._preprocess(\n            circuits, parameter_values, parameters, self.SUPPORTED_GATES\n        )\n        results = self._run_unique(\n            g_circuits, g_parameter_values, g_parameters, precision=precision\n        )\n        return self._postprocess(results, circuits, parameter_values, parameters)\n\n    def _run_unique(\n        self,\n        circuits: Sequence[QuantumCircuit],\n        parameter_values: Sequence[Sequence[float]],\n        parameters: Sequence[Sequence[Parameter]],\n        *,\n        precision: float | Sequence[float] | None,\n    ) -> QGTResult:\n        \"\"\"Compute the QGTs on the given circuits.\"\"\"\n        metadata = []\n        all_n, all_m = [], []\n        phase_fixes: list[int | np.ndarray] = []\n\n        has_transformed_precision = False\n\n        if isinstance(precision, float) or precision is None:\n            precision = [precision] * len(circuits)\n            has_transformed_precision = True\n\n        pubs = []\n\n        if not (len(circuits) == len(parameters) == len(parameter_values) == len(precision)):\n            raise ValueError(\n                f\"circuits, parameters, parameter_values and precision must have the same length, but \"\n                f\"have respective lengths {len(circuits)},  {len(parameters)}, {len(parameter_values)} \"\n                f\"and {len(precision)}.\"\n            )\n\n        for circuit, parameter_values_, parameters_, precision_ in zip(\n            circuits, parameter_values, parameters, precision\n        ):\n            # Prepare circuits for the gradient of the specified parameters.\n            parameters_ = [p for p in circuit.parameters if p in parameters_]\n            meta = {\"parameters\": parameters_}\n            metadata.append(meta)\n\n            # Compute the first term in the QGT\n            circuit_key = _circuit_key(circuit)\n            if circuit_key not in self._lin_comb_qgt_circuit_cache:\n                # generate the all of the circuits for the first term in the QGT and cache them.\n                # Only the circuit related to specified parameters will be executed.\n                # In the future, we can generate the specified circuits on demand.\n                self._lin_comb_qgt_circuit_cache[circuit_key] = _make_lin_comb_qgt_circuit(circuit)\n            lin_comb_qgt_circuits = self._lin_comb_qgt_circuit_cache[circuit_key]\n\n            qgt_circuits = []\n            rows, cols = np.triu_indices(len(parameters_))\n            for row, col in zip(rows, cols):\n                param_i = parameters_[row]\n                param_j = parameters_[col]\n                qgt_circuits.append(lin_comb_qgt_circuits[(param_i, param_j)])\n\n            observable = SparsePauliOp.from_list([(\"I\" * circuit.num_qubits, 1)])\n            observable_1, observable_2 = _make_lin_comb_observables(\n                observable, self._derivative_type\n            )\n\n            n = len(qgt_circuits)\n            if self._derivative_type == DerivativeType.COMPLEX:\n                all_m.append(len(parameters_))\n                all_n.append(2 * n)\n                pubs.extend(\n                    [\n                        (qgt_circuit, observable_1, parameter_values_, precision_)\n                        for qgt_circuit in qgt_circuits\n                    ]\n                )\n                pubs.extend(\n                    [\n                        (qgt_circuit, observable_2, parameter_values_, precision_)\n                        for qgt_circuit in qgt_circuits\n                    ]\n                )\n            else:\n                all_m.append(len(parameters_))\n                all_n.append(n)\n                pubs.extend(\n                    [\n                        (qgt_circuit, observable_1, parameter_values_, precision_)\n                        for qgt_circuit in qgt_circuits\n                    ]\n                )\n\n        if self._transpiler is not None:\n            for index, pub in enumerate(pubs):\n                new_circuit = self._transpiler.run(pub[0], **self._transpiler_options)\n                new_observable = pub[1].apply_layout(new_circuit.layout)\n                pubs[index] = (new_circuit, new_observable) + pub[2:]\n\n        # Run the single job with all circuits.\n        job = self._estimator.run(pubs)\n\n        if self._phase_fix:\n            # Compute the second term in the QGT if phase fix is enabled.\n            phase_fix_obs = [\n                SparsePauliOp.from_list([(\"I\" * circuit.num_qubits, 1)]) for circuit in circuits\n            ]\n            phase_fix_job = self._gradient.run(\n                circuits=circuits,\n                observables=phase_fix_obs,\n                parameter_values=parameter_values,\n                parameters=parameters,\n                precision=precision,\n            )\n\n        try:\n            results = job.result()\n            if self._phase_fix:\n                gradient_results = phase_fix_job.result()\n        except AlgorithmError as exc:\n            raise AlgorithmError(\"Estimator job or gradient job failed.\") from exc\n\n        # Compute the phase fix\n        if self._phase_fix:\n            for gradient in gradient_results.gradients:\n                phase_fix = np.outer(np.conjugate(gradient), gradient)\n                # Select the real or imaginary part of the phase fix if needed\n                if self.derivative_type == DerivativeType.REAL:\n                    phase_fix = np.real(phase_fix)\n                elif self.derivative_type == DerivativeType.IMAG:\n                    phase_fix = np.imag(phase_fix)\n                phase_fixes.append(phase_fix)\n        else:\n            phase_fixes = [0 for _ in range(len(circuits))]\n        # Compute the QGT\n        qgts = []\n        partial_sum_n = 0\n        for phase_fix, n, m in zip(phase_fixes, all_n, all_m):\n            qgt = np.zeros((m, m), dtype=\"complex\")\n            # Compute the first term in the QGT\n            if self.derivative_type == DerivativeType.COMPLEX:\n                qgt[np.triu_indices(m)] = np.array(\n                    [result.data.evs for result in results[partial_sum_n : partial_sum_n + n // 2]]\n                )\n                qgt[np.triu_indices(m)] += 1j * np.array(\n                    [\n                        result.data.evs\n                        for result in results[partial_sum_n + n // 2 : partial_sum_n + n]\n                    ]\n                )\n            elif self.derivative_type == DerivativeType.REAL:\n                qgt[np.triu_indices(m)] = np.real(\n                    [result.data.evs for result in results[partial_sum_n : partial_sum_n + n]]\n                )\n            elif self.derivative_type == DerivativeType.IMAG:\n                qgt[np.triu_indices(m)] = 1j * np.real(\n                    [result.data.evs for result in results[partial_sum_n : partial_sum_n + n]]\n                )\n\n            # Add the conjugate of the upper triangle to the lower triangle\n            qgt += np.triu(qgt, k=1).conjugate().T\n            if self.derivative_type == DerivativeType.REAL:\n                qgt = np.real(qgt)\n            elif self.derivative_type == DerivativeType.IMAG:\n                qgt = np.imag(qgt)\n\n            # Subtract the phase fix from the QGT\n            qgt = qgt - phase_fix\n            partial_sum_n += n\n            qgts.append(qgt / 4)\n\n        if has_transformed_precision:\n            precision = precision[0]\n\n            if precision is None:\n                precision = results[0].metadata[\"target_precision\"]\n        else:\n            for i, (precision_, result) in enumerate(zip(precision, results)):\n                if precision_ is None:\n                    precision[i] = results[i].metadata[\"target_precision\"]\n\n        return QGTResult(\n            qgts=qgts, derivative_type=self.derivative_type, metadata=metadata, precision=precision\n        )\n",
    "is_binary": false,
    "size": 13065
  }
]
