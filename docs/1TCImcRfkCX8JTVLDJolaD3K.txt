Source: C:\Users\10\Downloads\lectures\pdf\1TCImcRfkCX8JTVLDJolaD3K.pdf\nConverted: 2025-09-28 02:49:30\nPages: 473\nOCR: Enabled\n================================================================================\n\n\n=== PAGE 1 ===\n\n\n=== OCR PAGE 1 ===\n:: a

WILEY HANDBOOKS IN gr;
at

FINANCIAL ENGINEERING i)

-~ 4)
AND ECONOMETRICS oo *»

HANDBOOK OF

HIGH-FREQUENCY
TRADING AND
MODELING IN

FINANCE

Ionut Florescu
Maria C. Mariani
H. Eugene Stanley
Frederi G. Viens
\n\n=== PAGE 2 ===\nWiley Handbooks in
FINANCIAL ENGINEERING AND ECONOMETRICS
Advisory Editor
Ruey S. Tsay
The University of Chicago Booth School of Business, USA
A complete list of the titles in this series appears at the end of this
volume.
\n\n=== PAGE 3 ===\nHandbook of High-Frequency
Trading and Modeling in
Finance
Edited by
IONUT FLORESCU
MARIA C. MARIANI
H. EUGENE STANLEY
FREDERI G. VIENS
\n\n=== OCR PAGE 3 ===\nHandbook of High-Frequency
Trading and Modeling in
Finance

Edited by

IONUT FLORESCU
MARIA C. MARIANI
H. EUGENE STANLEY
FREDERI G. VIENS

WILEY
\n\n=== PAGE 4 ===\nCopyright © 2016 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted
in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or
otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright
Act, without either the prior written permission of the Publisher, or authorization through
payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222
Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at
www.copyright.com. Requests to the Publisher for permission should be addressed to the
Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030,
(201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their
best efforts in preparing this book, they make no representations or warranties with respect
to the accuracy or completeness of the contents of this book and specifically disclaim any
implied warranties of merchantability or fitness for a particular purpose. No warranty may
be created or extended by sales representatives or written sales materials. The advice and
strategies contained herein may not be suitable for your situation. You should consult with a
professional where appropriate. Neither the publisher nor author shall be liable for any loss
of profit or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside
the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears
in print may not be available in electronic formats. For more information about Wiley
products, visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Names: Florescu, Ionut, 1973- editor.
Title: Handbook of high-frequency trading and modeling in finance / edited by Ionut
Florescu, Maria C. Mariani, H. Eugene Stanley, Frederi G. Viens.
Description: Hoboken, NJ : John Wiley & Sons, Inc., [2016] | Includes index.
Identifiers: LCCN 2015043237 (print) | LCCN 2016000501 (ebook) | ISBN 9781118443989
(cloth) | ISBN 9781118593400 (pdf) | ISBN 9781118593325 (epub)
Subjects: LCSH: Investment analysis--Mathematical models. | Investments--Mathematical
models. | Finance--Mathematical models.
Classification: LCC HG4529 .H35863 2016 (print) | LCC HG4529 (ebook) |
DDC 332.64/20285--dc23
LC record available at http://lccn.loc.gov/2015043237
\n\n=== PAGE 5 ===\nContents
Notes on Contributors
Editors
List of Contributors
Preface
Chapter One Trends and Trades
1.1 Introduction
1.2 A trend-based trading strategy
1.3 CUSUM timing
1.4 Example: Random walk on ticks
1.5 CUSUM strategy Monte Carlo
1.6 The effect of the threshold parameter
1.7 Conclusions and future work
Appendix: Tables
References
Chapter Two Gaussian Inequalities and Tranche Sensitivities
2.1 Introduction
2.2 The tranche loss function
2.3 A sensitivity identity
2.4 Correlation sensitivities
Acknowledgment
References
Chapter Three A Nonlinear Lead Lag Dependence Analysis of
Energy Futures: Oil, Coal, and Natural Gas1
3.1 Introduction
3.2 Data
3.3 Estimation techniques
3.4 Results
\n\n=== PAGE 6 ===\n3.5 Discussion
3.6 Conclusions
Acknowledgments
References
Notes
Chapter Four Portfolio Optimization: Applications in Quantum
Computing
4.1 Introduction
4.2 Background
4.3 The models
4.4 Methods
4.5 Results
4.6 Discussion
4.7 Conclusion
Acknowledgments
Appendix 4.A: WMIS Matlab Code
References
Notes
Chapter Five Estimation Procedure for Regime Switching
Stochastic Volatility Model and Its Applications
5.1 Introduction
5.2 The methodology
5.3 Results obtained applying the model to real data
5.4 Conclusion
Appendix 5.A: Theoretical results and empirical testing
References
Chapter Six Detecting Jumps in High-Frequency Prices Under
Stochastic Volatility: A Review and a Data-Driven Approach
6.1 Introduction
6.2 Review on the intraday jump tests
6.3 A data-driven testing procedure
\n\n=== PAGE 7 ===\n6.4 Simulation study
6.5 Empirical results
6.6 Conclusion
Acknowledgments
Appendix 6.A: Least-square estimation of HAR-MA (2) model
for log(BP) of SPY
Appendix 6.B: Estimation of ARMA (2, 1) model for log(BP) of
SPY
Appendix 6.C: Minimized loss function loss(ρ1, ρ2) for
SV2FJ_2ρ model, SPY
Appendix 6.D.1: Calibration of ξ under SV2FJ_2ρ model at 2-
min frequency, E[Nt] = 0.08
Appendix 6.D.2: Calibration of ξ under SV2FJ_2ρ model at 2-
min frequency, E[Nt] = 0.40
Appendix 6.D.3: Calibration of ξ under SV2FJ_2ρ model at 5-
min frequency, E[Nt] = 0.08
Appendix 6.D.4: Calibration of ξ under SV2FJ_2ρ Model at 5-
min frequency, E[Nt] = 0.40
Appendix 6.D.5: Calibration of ξ under SV2FJ_2ρ model at
10-min frequency, E[Nt] = 0.08
Appendix 6.D.6: Calibration of ξ under SV2FJ_2ρ model at
10-min frequency, E[Nt] = 0.40
References
Notes
Chapter Seven Hawkes Processes and Their Applications to High-
Frequency Data Modeling
7.1 Introduction
7.2 Point processes
7.3 Hawkes processes
7.4 Statistical inference of Hawkes processes
7.5 Applications of Hawkes processes
Appendix 7.A: Point Processes
\n\n=== PAGE 8 ===\nAppendix 7.B: A Brief History of Hawkes processes
References
Notes
Chapter Eight Multifractal Random Walk Driven by a Hermite
Process:
8.1 Introduction
8.2 Preliminaries
8.3 Multifractal random walk driven by a Hermite process
8.4 Financial applications
8.5 Concluding remarks
References
Notes
Chapter Nine Interpolating Techniques and Nonparametric
Regression Methods Applied to Geophysical and Financial Data
Analysis
9.1 Introduction
9.2 Nonparametric regression models
9.3 Interpolation methods
9.4 Conclusion
Acknowledgments
References
Chapter Ten Study of Volatility Structures in Geophysics and
Finance Using Garch Models
10.1 Introduction
10.2 Short memory models
10.3 Long memory models
10.4 Detection and estimation of long memory
10.5 Data collection, analysis, and result
10.6 Discussion and conclusion
References
\n\n=== PAGE 9 ===\nChapter Eleven Scale Invariance and Lévy Models Applied to
Earthquakes and Financial High-Frequency Data
11.1 Introduction
11.2 Governing equations for the deterministic model
11.3 Lévy flights and application to geophysics
11.4 Application to the high-frequency market data
11.5 Brief program code description
11.6 Conclusion
11.A Appendix
References
Chapter Twelve Analysis of Generic Diversity in the Fossil
Record, Earthquake Series, and High-Frequency Financial Data
12.1 Introduction
12.2 Statistical preliminaries and results
12.3 Statistical and numerical analysis
12.4 Analysis with Lévy distribution
12.5 Analysis of the Stock Indices, high-frequency (tick) data,
and explosive series
12.6 Results and discussion
Acknowledgments
12.A Appendix A—Big ‘O’ notation
References
Index
Series
EULA
List of Tables
Chapter 1
Table 1.1
Table 1.2
\n\n=== PAGE 10 ===\nTable 1.3
Table 1.4
Table 1.5
Table 1.6
Table 1.7
Table 1.8
Table 1.A.1
Table 1.A.2
Table 1.A.3
Table 1.A.4
Table 1.A.5
Table 1.A.6
Chapter 3
Table 3.1
Table 3.2
Table 3.3
Table 3.4
Chapter 5
Table 5.1
Chapter 6
Table 6.1
Table 6.2
Table 6.3
Table 6.4
Table 6.5
Table 6.6
Table 6.7
\n\n=== PAGE 11 ===\nTable 6.8
Table 6.9
Table 6.10
Chapter 9
Table 9.1
Table 9.2
Table 9.3
Table 9.4
Table 9.5
Table 9.6
Table 9.7
Table 9.8
Table 9.9
Table 9.10
Table 9.11
Chapter 10
Table 10.1
Table 10.2
Table 10.3
Table 10.4
Table 10.5
Table 10.6
Table 10.7
Table 10.8
Table 10.9
Table 10.10
Table 10.11
\n\n=== PAGE 12 ===\nTable 10.12
Table 10.13
Table 10.14
Table 10.15
Table 10.16
Table 10.17
Table 10.18
Chapter 11
Table 11.1
Table 11.2
Table 11.3
Chapter 12
Table 12.1
Table 12.2
Table 12.3
Table 12.4
Table 12.5
Table 12.6
Table 12.7
Table 12.8
List of Illustrations
Chapter 1
Figure 1.1 Plot of the first subperiods, and cumulative gain,
for the CUSUM strategy, August 2, 2011, US 5-year treasury
note.
Figure 1.2 Lengths of subperiods versus gains, August 2,
2011, US 5-year treasury note.
\n\n=== PAGE 13 ===\nFigure 1.3 Subperiod length versus gain, July 29, 2011, US
30-year treasury note.
Figure 1.4 Subperiod length versus gain, August 1, 2011, US
30-year treasury note.
Figure 1.5 Subperiod length versus gain, August 2, 2011, US
30-year treasury note.
Figure 1.6 Subperiod length versus gain, August 3, 2011, US
30-year treasury note.
Figure 1.7 Figures 1.3, 1.4, 1.5 and 1.6 combined (30-year).
Figure 1.8 The four possible SRW paths for T+
1 = 2(3) = 6.
Figure 1.9 The 12 possible LSRW paths for T+
1 = 6.
Figure 1.10 Total gain versus thresholds for 5-year and 30-
year notes. Top graph: Small thresholds that vary as follows:
0.5*tick, tick, 2*tick, 3*tick, etc. (up to 29*tick). Bottom
graph: Large thresholds that vary as follows: 50*tick,
2*50*tick, 3*50*tick, etc. (up to 20*50*tick).
Figure 1.11 Number of subperiods versus thresholds for 5-
year and 30-year notes. Top graph: Small thresholds that vary
as follows: 0.5*tick, tick, 2*tick, 3*tick, etc. (up to 29*tick).
Bottom graph: Large thresholds that vary as follows 50*tick,
2*50*tick, 3*50*tick, etc. (up to 20*50*tick).
Chapter 3
Figure 3.1 Log prices by product. Horizontal lines represent
structural breaks according to the Bai–Perron test of the
Coal/WTI log prices ratio.
Figure 3.2 ACF and partial ACF of log returns by product.
Chapter 4
Figure 4.1 Example embedding.
Figure 4.2 Summary of input data.
Figure 4.3 Log-return correlations.
Figure 4.4 Market graph for threshold 0.35.
\n\n=== PAGE 14 ===\nFigure 4.5 Market graphs for increasing thresholds.
Figure 4.6 Portfolios for correlation-based MIS model.
Figure 4.7 Variance-covariance of the input data.
Figure 4.8 Portfolios for the restricted MIS model.
Figure 4.9 Classical results with minimum variance model.
Figure 4.10 Portfolios under the WMIS model.
Figure 4.11 Classical results with mean-variance model.
Chapter 5
Figure 5.1 Bear Stearns stock price/volatility March 10 to
March 18, 2008.
Figure 5.2 Bear Stearns stock price/volatility 2 weeks before
collapse.
Figure 5.3 Bear Stearns stock price/volatility 1 week before
the collapse.
Figure 5.4 Bear Stearns stock price/volatility week of the
collapse.
Figure 5.5 Bear Stearns stock price/volatility during the last
2 weeks.
Figure 5.6 Hourly temperature data from 2000 and 2006.
Figure 5.7 Three-hour volatility estimates for 1976–1977.
Figure 5.8 Raw acceleration signal at the two stations.
Figure 5.9 Variability comparison. Beginning of an
earthquake.
Figure 5.10 Parkfield CA, Donna Lee Peak seismograph
readings.
Figure 5.11 Parkfield CA earthquake. Comparison of the
variability signals, Donna Lee variability in green and Red
Hills variability in blue.
Figure 5.12 Test data: 4 Nodes (0.1, 0.3, 0.5, and 0.75).
Chapter 6
\n\n=== PAGE 15 ===\nFigure 6.1 Volatility signature plot of RV, SPY.
Figure 6.2 Two-min SPY returns on October 9 and 10, 2008.
Figure 6.3 The ACFs and XCFs of log(BP) from SPY and
SV2FJ_2ρ Model.
Figure 6.4 Estimated IVP over test period, SPY.
Figure 6.5 Detected jumps during the test period and
intraday distributions, SPY.
Chapter 7
Figure 7.1 Volatility signature plot of hawkes jump model.
Chapter 8
Figure 8.1 Paths of MRW driven by a Rosenblatt process,
from top left to bottom right, H = 0.501; 6; 7; 8; 9, and 0.99.
Figure 8.2 From top left to bottom right: Empirical density
of S&P 500, with (from top to bottom) τ = 15 s, 30 min, 4 h,
and 1 day; empirical density of the MRW driving by a fBm
with the following values of τ from top to bottom: τ = 10− 4,
10− 2, 1, 102, and H = 0.5 (and σ2= 0.02), fBm with H = 0.56
and with a Rosenblatt process with H = 0.52. In any cases, the
black line corresponds to the Gaussian prediction. We used
the logarithmic scale.
Figure 8.3 NFLX from January 2 to March 28, 2013, at 1-
min frequency, 24,832 observations.
Figure 8.4 Autocorrelation function of the returns (left) and
of the squared returns (right) of NFLX. In red, the average of
the autocorrelations of 10 paths of the HMRW driven by an
fBm (H = 0.56), with squared increments (right) and with
standard increments (left). In green for the HRMW driven by
a Rosenblatt process (H = 0.52).
Figure 8.5 Multifractal spectrum of NFLX (black dots) and
of HMRW driven by an mbf in red with H = 0.56 and σG =
0.19. q varies from 0. to 5.
\n\n=== PAGE 16 ===\nFigure 8.6 EA from January 2 to March 28, 2013, at 1 min,
23,139 observations.
Figure 8.7 Down: autocorrelation of the returns (left) and of
the squared returns (right) from EA. In red, the average of 10
paths of the HMRW with squared increments (right) and
standard increments (left). Up: Multifractal spectrum of EA
(black dots) and of HMRW (red) with H = 0.501 and σG =
0.19. q varies from 0 to 5.
Chapter 9
Figure 9.1 Simulation results for the earthquake that
happened in the months of January–April in 1973. For both
simulations, we have used 653 data points that contain the
magnitude of the earthquake collected from different
locations. (a) The local regression estimation surface
generated by Lowess method and (b) the local regression
estimation surface generated by Loess method.
Figure 9.2 Simulation results for the earthquake that
happened in the months of January–May in 1979. For both
simulations, we have used 1139 data points that contain the
magnitude of the earthquake collected from different
locations. (a) The local regression estimation surface
generated by Lowess method and (b) the local regression
estimation surface generated by Loess method.
Figure 9.3 Simulation results for the earthquake that
happened in the months of April–June in 1988. For both
simulations, we have used 700 data points that contain the
magnitude of the earthquake collected from different
locations. (a) The local regression estimation surface
generated by Lowess method and (b) the local regression
estimation surface generated by Loess method.
Figure 9.4 Simulation results for the earthquake that
happened in the months of September–October in 1996. For
both simulations, we have used 560 data points that contain
the magnitude of the earthquake collected from different
locations. (a) The local regression estimation surface
\n\n=== PAGE 17 ===\ngenerated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
Figure 9.5 Simulation results for the earthquake that
happened in the months of November–December in 2008.
For both simulations, we have used 700 data points that
contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface
generated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
Figure 9.6 Exxon Mobile Corporation—The solid red line
represents the best fit curve with four different methodology
using same data.
Figure 9.7 The Cal Group (CAL)–The solid red line
represents the best fit curve with four different methodology
using same data.
Figure 9.8 JP Morgan Chase—The solid red line represents
the best fit curve with four different methodology using same
data.
Figure 9.9 International Business Machines—The solid red
line represents the best fit curve with four different
methodology using same data.
Figure 9.10 MFA Financial, Inc-The solid red line represents
the best fit curve with four different methodologies using
same data.
Figure 9.11 Simulation results for the earthquake that
happened in the months of January–April in 1973. For all the
simulations, we have used 653 data points that contain the
magnitude of the earthquake collected from different
locations. The Interpolant estimation surface is generated by
the following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e)
TPS.
Figure 9.12 Simulation results for the earthquake that
happened in the months of January–May in 1979. For all the
simulations, we have used 1139 data points that contain the
\n\n=== PAGE 18 ===\nmagnitude of the earthquake collected from different
locations. The Interpolant estimation surface generated by the
following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e)
TPS.
Figure 9.13 Simulation results for the earthquake that
happened in the months of April–June in 1988. For all the
simulations, we have used 700 data points that contain the
magnitude of the earthquake collected from different
locations. The Interpolant estimation surface generated by the
following: (a) nearest neighborhood method; (b) linear
method; (c) cubic method; (d) biharmonic method; and (e)
TPS.
Figure 9.14 Simulation results for the earthquake that
happened in the months of September–October in 1996. For
all the simulations, we have used 560 data points that contain
the magnitude of the earthquake collected from different
locations. The Interpolant estimation surface generated by the
following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e)
TPS.
Figure 9.15 Simulation results for the earthquake that
happened in the months of November–December in 2008.
The Interpolant estimation surface generated by the
following: (a) nearest neighborhood method; (b) blinear
method; (c) bicubic method; and (d) biharmonic method.
Chapter 10
Figure 10.1 Log daily price, distribution, and normal Q–Q
plot of the DJIA prices from March 14, 2003, to October 10,
2011.
Figure 10.2 First difference of the log prices (returns series).
Figure 10.3 ACF and PACF of returns.
Figure 10.4 Time plot, ACF, histogram, and normal q–q plot
of standardized residuals.
\n\n=== PAGE 19 ===\nFigure 10.5 ACF and PACF of the squared residuals and
squared returns.
Figure 10.6 Conditional standard superimposed on the
returns.
Figure 10.7 Time plot of standardized residual.
Figure 10.8 ACF and distribution of the squared residuals.
Figure 10.9 The conditional standard deviation and
standard residuals of MA(1)+FIGARCH(1,d,1).
Figure 10.10 MA(1)+FIGARCH(1,d,1) standardized residuals
distribution.
Figure 10.11 S&P 500 returns and squared and absolute
returns ACF and PACF.
Figure 10.12 S&P 500 volatility model diagnostics: Top are
the ACF of the standardized and standardized squared
residual. Bottom are the distribution and GED Q–Q plot for
the fitted model.
Figure 10.13 BAC high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 10.14 BAC conditional volatility model diagnostics:
Top are the ACF of the standardized and standardized
squared residual. Bottom are the distribution and GED Q–Q
plot for the fitted model.
Figure 10.15 JPM high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 10.16 JPM conditional volatility model diagnostics:
Top are the ACF of the standardized and standardized
squared residual. Bottom are the distribution and GED Q–Q
plot for the fitted model.
Figure 10.17 IBM high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 10.18 IBM conditional volatility model diagnostics:
Top are the ACF of the standardized and standardized
\n\n=== PAGE 20 ===\nsquared residual. Bottom are the distribution and GED Q–Q
plot for the fitted model.
Figure 10.19 WMT high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 10.20 WMT high-frequency conditional volatility
model diagnostics.
Figure 10.21 Explosive series and squared and absolute
series ACF and PACF.
Figure 10.22 Explosive series conditional volatility model
diagnostics.
Chapter 11
Figure 11.1 Times t1, …, tk need to correspond to the real
earthquake events.
Figure 11.2 Earthquake data and fitting with function (11.2).
Figure 11.3 Earthquake data and fitting with function (11.2).
Figure 11.4 Earthquake data and fitting with function (11.2).
Figure 11.5 Earthquake data and fitting with function (11.2).
Figure 11.6 Earthquake data and fitting with function (11.2).
Figure 11.7 Earthquake data and fitting with function (11.2).
Figure 11.8 Earthquake data and fitting with function (11.2).
Figure 11.9 Earthquake data fitting by TLF, corresponding to
11.3.
Figure 11.10 Earthquake data fitting by TLF, corresponding
to 11.4.
Figure 11.11 Earthquake data fitting by TLF, corresponding
to 11.5.
Figure 11.12 Earthquake data fitting by TLF, corresponding
to 11.6.
Figure 11.13 Earthquake data fitting by TLF, corresponding
to 11.7.
\n\n=== PAGE 21 ===\nFigure 11.14 Earthquake data fitting by TLF, corresponding
to 11.8.
Figure 11.15 JP Morgan Chase—The solid line represents the
best fit with (11.5).
Figure 11.16 Citi—The solid line represents the best fit with
(11.5).
Figure 11.17 IAG—The solid line represents the best fit with
(11.5).
Figure 11.18 LBC—The solid line represents the best fit with
(11.5).
Chapter 12
Figure 12.1 Plot of the frequency of the number of genera
compared with different distribution (with p-values computed
from AD test indicating goodness of fit).
Figure 12.2 Histogram of (a) JT for diversity data and (b)
two applications of JT for diversity data.
Figure 12.3 Plot of two applications of Johnson’s
transformation to the diversity data.
Figure 12.4 Histogram of extinction magnitude fitting with
(a) the largest extreme value distribution; (b) the Gamma
distribution; and (c) the 3-parameter Gamma distribution.
Figure 12.5 Probability plot of extinction magnitude data.
Figure 12.6 Probability plot of extinction magnitude data.
Figure 12.7 JT for extinction magnitude data.
Figure 12.8 Diversity (red dots) and its linear interpolation
(blue line).
Figure 12.9 Data fitted with distribution (12.12), empirical
distribution for GST, and standard normal distribution. Time
lag T = 1, 2, 4, 8, and 16.
Figure 12.10 Distribution of log change in diversity for time
lag T plotted against various distributions. (a) T=1; and (b)
T=4.
\n\n=== PAGE 22 ===\nFigure 12.11 The figure shows the estimates of the Lévy
Flight parameter for the solution of the SDE.
Figure 12.12 The figure shows the estimates of the Lévy
Flight parameter for JP Morgan.
Figure 12.13 The figure shows the estimates of the Lévy
Flight parameter for The Walt Disney Company.
Figure 12.14 The figure shows the estimates of the Lévy
Flight parameter for the DJIA index.
Figure 12.15 The figure shows the estimates of the Lévy
Flight parameter for IBM. These are high-frequency (tick)
data.
Figure 12.16 The figure shows the estimates of the Lévy
Flight parameter for Google. These are high-frequency (tick)
data.
Figure 12.17 The figure shows the estimating of the Levy
Flight parameter for Walmart. These are high frequency (tick)
data.
Figure 12.18 The figure shows the estimates of the Lévy
flight parameter for The Walt Disney Company. These are
high-frequency (tick) data.
Figure 12.19 The figure shows the estimates of the Lévy
flight parameter for Intel Corporation. These are high-
frequency (tick) data.
Figure 12.20 S&P 500 return and squared and absolute
returns ACF and PACF.
Figure 12.21 S&P 500 volatility model diagnostics: On top
are the ACF of the standardized and standardized squared
residual and on bottom are the distribution and GED Q–Q
plot for the fitted model.
Figure 12.22 BAC high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 12.23 BAC conditional volatility model diagnostics:
On top are the ACF of the standardized and standardized
\n\n=== PAGE 23 ===\nsquared residual, and on bottom are the distribution and GED
Q–Q plot for the fitted model.
Figure 12.24 JPM high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 12.25 JPM conditional volatility model diagnostics:
On top are the ACF of the standardized and standardized
squared residual, and on bottom are the distribution and GED
Q–Q plot for the fitted model.
Figure 12.26 IBM high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 12.27 IBM conditional volatility Model diagnostics:
Top are the ACF of the standardized and standardized
squared residual. Bottom are the distribution and GED Q–Q
plot for the fitted model.
Figure 12.28 WMT high-frequency returns and squared and
absolute returns ACF and PACF.
Figure 12.29 WMT high-frequency conditional volatility
model diagnostics.
\n\n=== PAGE 24 ===\nNotes on Contributors
Editors
Ionut Florescu
Financial Engineering Division, Stevens Institute of Technology,
Hoboken, NJ, USA
Maria C. Mariani
Department of Mathematical Sciences, University of Texas at El
Paso, El Paso, TX, USA
H. Eugene Stanley
Boston University, Boston, MA, USA
Frederi G. Viens
Purdue University, West Lafayette, IN, USA
List of Contributors
K. Basu
Department of Mathematics, Occidental College, Los Angeles,
CA, USA
M. P. Beccar-Varela
Department of Mathematical Sciences, University of Texas at El
Paso, El Paso, TX, USA
Claas Becker
Studiengang Angewandte Mathematik, Hochschule RheinMain,
65197 Wiesbaden, Germany
P. Bezdek
Department of Mathematics, The University of Utah, Salt Lake
City, UT, USA
F. Biney
\n\n=== PAGE 25 ===\nDepartment of Mathematical Sciences, The University of Texas at
El Paso, El Paso, TX, USA
Michael Carlisle
Department of Mathematics, Baruch College, City University of
New York, New York, NY, USA
Bernardo Creamer
Universidad de las Américas, Quito, Ecuador
Germán G. Creamer
School of Business, Stevens Institute of Technology, Hoboken,
NJ, USA
Alexis Fauth
SAMM, Université de Paris 1 Panthéon-Sorbonne, 90, rue de
Tolbiac, 75634, Paris, France and Invivoo, 13 rue de l'Abreuvoir,
92400 Courbevoie, France
Olympia Hadjiliadis
Department of Mathematics and Statistics, Hunter College, City
University of New York, and Departments of Computer Science
and Mathematics, Graduate Center, City University of New York,
New York, NY, USA
Baron Law
Purdue University, West Lafayette, IN, USA
Forrest Levin
Financial Engineering Division, Stevens Institute of Technology,
Hoboken, NJ, USA
Michael Marzec
Stevens Institute of Technology, Hoboken, NJ, USA
Ambar N. Sengupta
Department of Mathematics, Louisiana State University, Baton
Rouge, LA, USA
I. SenGupta
\n\n=== PAGE 26 ===\nDepartment of Mathematics, North Dakota State University,
Fargo, ND, USA
Mark B. Shackleton
Department of Accounting and Finance, Lancaster University
Management School, Lancaster, England, United Kingdom
M. Shpak
NeuroTexas Institute, St. David's Medical Center, Austin, TX,
USA
Ioannis Stamos
Department of Computer Science, Hunter College, City University
of New York, and Department of Computer Science, Graduate
Center, City University of New York, New York, NY, USA
Ping-Chen Tsai
Department of Finance, Southern Taiwan University of Science
and Technology, Yongkang, Tainan City, Taiwan
Ciprian A. Tudor
Laboratoire Paul Painlevé, Université de Lille 1, F-59655
Villeneuve d'Ascq, France, and Department of Mathematics,
Academy of Economical Studies, Bucharest, Romania
\n\n=== PAGE 27 ===\nPreface
This Handbook is a collection of chapters that describe a range of
current empirical and analytical work on financial industry data
sampled at high frequency (HF).
Our contemporary Age of Information is a world dominated by ever-
increasing quantitative elements that decision makers are expected
to take into account. Many fields are confronted with large amounts
of data. The phenomenon is particularly challenging in the finance
industry, in that evidently relevant data can be sampled with
increasingly HF, a trend that started in earnest more than a decade
ago and does not seem to be letting down. Some of the special
challenges posed by these now staggering amounts of data stem from
the uncomfortable evidence that traditional models and information
technology tools can be poorly suited to grapple with their size and
complexity.
Probabilistic modeling and statistical data analysis attempt to
uncover order from apparent disorder. By illustrating this
methodological framework in the context of HF finance, the current
volume may serve as a guide to various new systematic approaches
concerning how to implement these quantitative activities with HF
financial data. The chapters herein cover a wide range of topics
related to the analysis and modeling of data sampled with HF,
principally in finance, as well as in other fields where new ideas may
prove helpful to HF finance applications. The first chapters cover the
dynamics and complexity of futures and derivatives markets as well
as a novel take on the portfolio optimization problem using quantum
computers. The following chapters are dedicated to estimating
complex model parameters using HF data. The final chapters create
links between models used in financial markets and models used in
other research areas such as geophysics, fossil records, and
earthquake studies.
The editors express their deepest gratitude to all the contributors for
their talent and labor in bringing together this Handbook, to the
\n\n=== PAGE 28 ===\nmany anonymous referees who helped the contributors perfect their
work, and to Wiley for making the publication a reality.
Ionut Florescu
Hoboken, NJ
Maria C. Mariani
El Paso, TX
H. Eugene Stanley
Boston, MA
Frederi G. Viens
Washington, DC
August 9, 2015
\n\n=== PAGE 29 ===\nChapter One
Trends and Trades
Michael Carlisle1, Olympia Hadjiliadis2, and Ioannis Stamos3
1Department of Mathematics, Baruch College, City University of New York, New
York, NY, USA
2Department of Mathematics and Statistics, Hunter College, City University of New
York, and Departments of Computer Science and Mathematics, Graduate Center,
City University of New York, New York, NY, USA
3Department of Computer Science, Hunter College, City University of New York,
and Department of Computer Science, Graduate Center, City University of New
York, New York, NY, USA
1.1 Introduction
High-frequency data in finance is often characterized by fast fluctuations and noise (see,
e.g., [7]), a trait that is known to make the volatility of the data very hard to estimate
(see, e.g., [13]). Although this characteristic creates many challenges in modeling, it
offers itself to the study of distinguishing “signal” from “noise,” a topic of interest in the
area of quickest detection (see [25], [5]). One of the most popular algorithms used in
quickest detection is known as the cumulative sum (CUSUM) stopping rule first
introduced by Page [24]. In this work, we employ a sequence of CUSUM stopping rules
to construct an online trading strategy. This strategy takes advantage of the relatively
frequent number of alarms CUSUM stopping times may provide when applied to high-
frequency data as a result of the fast fluctuations present therein. The trading strategy
implemented settles frequently and thus eliminates the risk of large positions. This
makes the strategy implementable in practice. Prior work has been done by Lam and
Yam [20] on drawing connections between CUSUM techniques and the filter trading
strategy, yet both the filter trading strategy (see [2, 3]), or its equivalent, the buy and
hold strategy (see [12]), run high risks of great losses mainly due to the randomness
associated with settling. The well-known trailing stops strategy whose properties have
been thoroughly studied in the literature (see, e.g., [15] or [1]) is also related to the filter
strategy and thus suffers similar risks.
Although our proposed rule presents clear merits in terms of minimizing the risk of large
positions by taking advantage of the high volatility frequently present in high-frequency
data, the main purpose of this chapter is to present and illustrate the use of detection
techniques (in this case the CUSUM) in high-frequency finance. In particular, the
strategy proposed is based on running in parallel two CUSUM stopping rules: one
detects an upward (+) change and the other a downward (−) change in the mean of the
observations. Once an upward/downward CUSUM alarm (called a “signal”) goes off,
there is a buy/short sale of one unit of the underlying asset. At that moment, we repeat a
CUSUM stopping rule, and for every alarm of the same sign, we continue buying or short
selling one unit of the underlying asset until a CUSUM alarm of the opposite sign is set
off, at which time we sell off all of what we bought or buy up all of what we short sold.
The high frequency of CUSUM alarms in high-frequency tick data permits the
\n\n=== PAGE 30 ===\nimplementation of this rule in practice since large exposures on one side, whether on the
buy or on the sell side, are settled relatively quickly.
The algorithmic strategy proposed is applied on real tick data of a 30-year asset and a 5-
year note sold at auction on various individual days. It is seen that the algorithm is most
profitable in the presence of upward or downward trends (which we call “subperiods”),
even in the presence of noise, and is less profitable on periods of price stability. The
proposed strategy is, in fact, a trend-following algorithm.
To quantify the performance of the proposed algorithmic strategy, we calculate its
expected reward in a simple random walk model. Our diagnostic plots indicate that the
more biased the random walk is, the more profitable the proposed strategy becomes,
which is consistent with the actual findings when the strategy is applied to real data. This
is because in the presence of a bias, trends are more likely to form than in the absence of
a bias.
We take the analytical approach of discrete data and a linear random walk model, rather
than taking the continuous approach via, for example, the geometric Brownian motion
model, because we are analyzing the movement of individual ticks of a price, quantized
in a linear fashion (e.g., at the level of 1 cent, 
 cent, or 
 cent). Our models focus on
tracking the motion of an asset price via these ticks, and so a linear approach is a more
realistic setting, when short interest rate effects would be minimal.
We begin our analysis in Section 1.2 by describing a general trading strategy based on
following upward or downward trends in a data stream, without specifying the timing
mechanism behind such a strategy. We then develop the notion of gain over the time
period of an individual trend. In Section 1.3, we build a timing scheme stemming from
quickest detection considerations and give a preliminary performance evaluation of the
overall strategy on real tick data. Next, in Section 1.4, we analyze the specific case of
random walk-based data and calculate the expected value of the gain over a trend in this
case. We give an explicit formula for this gain in the special case of simple asymmetric
random walk on asset tick changes. Then, in Section 1.5, we give results of Monte Carlo
simulations for the asymmetric lazy simple random walk and symmetric lazy random
walk on tick changes. In Section 1.6, we discuss the effect of the CUSUM threshold
parameter on the trading strategy. We conclude in Section 1.7 by a discussion of ways in
which the proposed strategy may be improved with suggestions for further work.
1.2 A trend-based trading strategy
Let {Sn}n = 0, 1, 2... be a sequence of data points; for our purposes, they will be samples of
the price of an asset. We assume that S0 = s is a constant, and Sk = 0 for some k implies
that Sn = 0 for all n > k. Let T0 = 0, and define Tk, k = 1, 2, ... as an increasing sequence of
(stopping) times, called signals, noting some trend in the sequence. We call Tk the k-th
signal.
1.2.1 SIGNALING AND TRENDS
In this subsection, we construct a trading strategy in the case that there are two types of
signals: “+ signals” (declaring the detection of an upward trend in the data) and “−
signals” (declaring the detection of a downward trend in the data). Let “Property + (k)”
be the property that causes a + signal to occur as the kth signal, and denote this event by
\n\n=== OCR PAGE 30 ===\nimplementation of this rule in practice since large exposures on one side, whether on the
buy or on the sell side, are settled relatively quickly.

The algorithmic strategy proposed is applied on real tick data of a 30-year asset and a 5-
year note sold at auction on various individual days. It is seen that the algorithm is most
profitable in the presence of upward or downward trends (which we call “subperiods”),
even in the presence of noise, and is less profitable on periods of price stability. The
proposed strategy is, in fact, a trend-following algorithm.

To quantify the performance of the proposed algorithmic strategy, we calculate its
expected reward in a simple random walk model. Our diagnostic plots indicate that the
more biased the random walk is, the more profitable the proposed strategy becomes,
which is consistent with the actual findings when the strategy is applied to real data. This
is because in the presence of a bias, trends are more likely to form than in the absence of
a bias.

We take the analytical approach of discrete data and a linear random walk model, rather
than taking the continuous approach via, for example, the geometric Brownian motion
model, because we are analyzing the movement of individual ticks of a price, quantized

ina linear fashion (e.g., at the level of 1 cent, 32 cent, or 64 cent). Our models focus on
tracking the motion of an asset price via these ticks, and so a linear approach is a more
realistic setting, when short interest rate effects would be minimal.

We begin our analysis in Section 1.2 by describing a general trading strategy based on
following upward or downward trends in a data stream, without specifying the timing
mechanism behind such a strategy. We then develop the notion of gain over the time
period of an individual trend. In Section 1.3, we build a timing scheme stemming from
quickest detection considerations and give a preliminary performance evaluation of the
overall strategy on real tick data. Next, in Section 1.4, we analyze the specific case of
random walk-based data and calculate the expected value of the gain over a trend in this
case. We give an explicit formula for this gain in the special case of simple asymmetric
random walk on asset tick changes. Then, in Section 1.5, we give results of Monte Carlo
simulations for the asymmetric lazy simple random walk and symmetric lazy random
walk on tick changes. In Section 1.6, we discuss the effect of the CUSUM threshold
parameter on the trading strategy. We conclude in Section 1.7 by a discussion of ways in
which the proposed strategy may be improved with suggestions for further work.

1.2 A trend-based trading strategy

Let {S,}n = 0,1, 2.., be a sequence of data points; for our purposes, they will be samples of
the price of an asset. We assume that S, = s is a constant, and S; = 0 for some k implies
that S,, = 0 for all n > k. Let T, = 0, and define T;, k = 1, 2, ... as an increasing sequence of
(stopping) times, called signals, noting some trend in the sequence. We call T, the k-th
signal.

1.2.1 SIGNALING AND TRENDS

In this subsection, we construct a trading strategy in the case that there are two types of
signals: “+ signals” (declaring the detection of an upward trend in the data) and “-
signals” (declaring the detection of a downward trend in the data). Let “Property + (k)”
be the property that causes a + signal to occur as the kth signal, and denote this event by
\n\n=== PAGE 31 ===\n(1.1)
(1.2)
(1.3)
(1.4)
{Tk = T+
k}. Likewise, let “Property − (k)” be the property that causes a − signal to occur
as the k-th signal, and denote this by {Tk = T−
k}. Only one type of trend can be detected
at a time, so we formally define T+
k and T−
k by
Thus, Tk = T+
k∧T−
k for every k = 1, 2, ....
Next, we state what it means for the data to stay in a trend. We define the sequence of
signal indices α(l) as follows: let α(0) = 0, so Tα(0) = 0, and for l ≥ 1, with k ≥ 2, define the
properties
Then, we define the lth shift point as, for l = 1, 2, ...,
Note that Tα(l) is at least two signals after Tα(l − 1). Definition (1.3) is equivalent to
A sequence of the same type of signal will be called a subperiod of the sample points. A
shift point denotes the end of a subperiod of the same type of signal.
Let Δn be the number of shares of the asset S held at time n. Set Δ0 = 0. Note that, for
every n ∈ (Tα(l), Tα(l + 1)), the sign of Δn is invariant, that is, either Δn > 0 holds for every
n ∈ (Tα(l), Tα(l + 1)) or Δn < 0 holds for every n ∈ (Tα(l), Tα(l + 1)).
Our trading strategy is as follows:
\n\n=== OCR PAGE 31 ===\n{T,, = T*;,}. Likewise, let “Property — (k)” be the property that causes a — signal to occur
as the k-th signal, and denote this by {T;, = T-;}. Only one type of trend can be detected
at a time, so we formally define T*, and T-;, by

[t= T, if Property + (k) occurs (1.1),
ke oo if Property — (k) occurs

[--= T,, if Property — (k) occurs (1.2)
ke co if Property + (k) occurs

Thus, Ty = T*, A Tx for every k = 1, 2, ....

Next, we state what it means for the data to stay in a trend. We define the sequence of
signal indices a(J) as follows: let a(0) = 0, so Taio) = 0, and for / 2 1, with k > 2, define the

properties

“Property +(/,k)": Tj; = T, for every a(/— 1) <j <kandT, = Tr
“Property —(/,k)": Tj = T; for every a(l — 1) <j <kand T, = T;.
Then, we define the lth shift point as, for | = 1, 2, ...,

a(/) := inf {k > a(/ — 1) +2: Property + (/, k) or Property — (/, k) holds}. (4.3),

Note that T,,p is at least two signals after Tay _ ,. Definition (1.3) is equivalent to

a(l) := inf {k> a(/— 1) + 2:7, has different sign than T;. a(l—1)<j<k}. (1.4)
A sequence of the same type of signal will be called a subperiod of the sample points. A
shift point denotes the end of a subperiod of the same type of signal.

Let A,, be the number of shares of the asset S held at time n. Set Ay = 0. Note that, for
everyn € (Tap, Tat + 1), the sign of A,, is invariant, that is, either A,, > 0 holds for every
n € (Tan, Tag +9) OF An < 0 holds for every n € (Tan, Taw + 1)

Our trading strategy is as follows:
\n\n=== PAGE 32 ===\n(1.5)
(1.6)
(1.7)
We assume a market in which all market orders are instantly fulfilled. The intent of this
strategy is to profit from following subperiods of + or − signals by the old adage “buy
low, sell high.” The success of this strategy relies mainly on the length of such
subperiods.
1.2.2 GAIN OVER A SUBPERIOD
We wish to analyze the gain Gl, l = 1, 2, ..., for this trading strategy over the time period
(Tα(l − 1), Tα(l)], called subperiod l; this is the amount of cash earned or lost by liquidating
the transactions made from signals Tα(l − 1) + 1, ..., Tα(l) − 1 at Tα(l).
Note that a subperiod is determined by the first signal on that run: if T1 = T+
1, then the
run from signal 1 to signal α(1) − 1 is a “bull run” subperiod of individual buy orders
followed by a sell-off at time Tα(1) = T−
α(1); if T1 = T−
1, then this run is a “bear run”
subperiod of individual short sales followed by a buy-up at Tα(1) = T+
α(1). Define Gl to be
the gain on subperiod l; thus, G1 is the gain on the first subperiod, starting at signal Tα(0)
+ 1 = T1 and ending at signal Tα(1). We require, as a condition, the sign of the first signal of
the subperiod. Let c ≥ 0 be the percentage cost per transaction, and define
The gain on a subperiod is calculated as follows:
For example, if c = 0.01, T1 = T+
1, and α(1) = 4, then Tα(1) = T4 = T−
4. Say the prices at the
buy-signal times are 
, 
, 
, and we sell everything off at 
. Then
\n\n=== OCR PAGE 32 ===\nA if no signal at time n, i.e. n # T; Vj (no change) (5),

A, +1 ifn =T,; =T; for some j, a(l) <j < a(l+ 1)
for some I dbuy one during a + subperiod)

Ant =4An— 1 ifn= T; =T; for some j, a(l) <j < a(l+ 1)

for some / (sell one during a — subperiod)
0 ifn = Ty) for some / > 1

(buy-up if TY; sell-off if TT).

We assume a market in which all market orders are instantly fulfilled. The intent of this
strategy is to profit from following subperiods of + or — signals by the old adage “buy
low, sell high.” The success of this strategy relies mainly on the length of such
subperiods.

1.2.2 GAIN OVER A SUBPERIOD

We wish to analyze the gain G;, | = 1, 2, ..., for this trading strategy over the time period
(Taq —1)> Tal, called subperiod I; this is s the amount of cash earned or lost by liquidating
the transactions made from signals Taq - 1) 4.15 +» Tay -1 At Tac

Note that a subperiod is determined by the first signal on that run: if T, = T*,, then the
run from signal 1 to signal a(1) - 1 is a “bull run” subperiod of individual buy orders
followed by a sell-off at time Tq) = T” qq); if T, = T7,, then this run is a “bear run”
subperiod of individual short sales followed by a buy-up at T,(.) = T*aqy- Define G; to be
the gain on subperiod J; thus, G, is the gain on the first subperiod, starting at signal T,(o)
+1 = T, and ending at signal T,,). We require, as a condition, the sign of the first signal of
the subperiod. Let c 2 0 be the percentage cost per transaction, and define

Y, := a(t) — a(l—1)-1. (1.6)

AL Vt =Paya?

The gain on a subperiod is calculated as follows:

= 0) Derr Sp — UF al) — ad = 1) = Sp, a7)
Goe= if Taya = Tey. 4? nt
1) d= a) = a= 1) = Sp 0) Te Sr
if Toast = Teaayt

y _
_ja ~ OY Sr, 7 FIST, Taye = Ty
(= c(¥)Sy,, — (+e) Du Sr. if Ty eae =Ty

a(i-1)+1°

}+atl—1)

For example, if c = 0.01, T, = T*,, and a(1) = 4, then Ty) = T, = T"4. Say the prices at the

Sp, = 5 Sp, =7, Sp, =9

buy-signal times are , and we sell everything off at Sr, =8 Then
\n\n=== PAGE 33 ===\n(1.8)
(1.9)
(1.10)
(1.11)
, 
, 
, 
, and we liquidate at time T4 to 
. The gain on the
first subperiod would then be G1 = (0.99)(3)(8) − (1.01)(5 + 7 + 9) = 2.55.
Combining the 1 − c terms and adding on the random variable 
, we have after
some algebra a sum of price increments:
We can rewrite each difference in the sum as a telescoping sum: setting
as the incremental price change between signals k and k + 1, we have
Substituting this back into (1.8) yields
Therefore, by (1.11), the gain over subperiod l is
Note that, in the absence of transaction costs (i.e., c = 0), the expected gain Gl is entirely
determined by price increments and the sign of the first signal of the subperiod.
1.3 CUSUM timing
Next, we describe a version of the CUSUM statistic process and its associated CUSUM
stopping rule, which we will use to devise a timing scheme based on the quickest
detection of trends, and incorporate this scheme to our trading strategy.
1.3.1 CUSUM PROCESS AND STOPPING TIME
In this section, we begin by introducing the measurable space 
, where 
,
 and 
. The law of the sequence Yi, i = 1, …, is
described by the family of probability measures {Pν}, 
. In other words, the
probability measure Pν for a given ν > 0, playing the role of the change point, is the
\n\n=== OCR PAGE 33 ===\nAr, = 0 Ar, = 1 Ar, = 2 Ar, = 3 and we liquidate at time T, to 47, =9 The gain on the
first subperiod would then be G, = (0.99)(3)(8) — (1.01)(5 + 7 + 9) = 2.55.

2cY,S

Combining the 1 - c terms and adding on the random variable ‘«(T,), we have after

some algebra a sum of price increments:

(1.8)
G, + 2c¥ Sr = (e+ (- D4) ee -¥s Tot |
= (c+(-1*) Yon, ~ Styieu))
j=!
We can rewrite each difference in the sum as a telescoping sum: setting
Zc 2= Sp, — Spy k= 1,25 (1.9),

as the incremental price change between signals k and k + 1, we have

a()-1 a(i)-1

k=jt+a(l-1) k=j+a(l-1)

Substituting this back into (1.8) yields

y, a()-1 y, (1.10)
G+ 2e¥Syry = (e+ (I) | YX a)= c+ (-1" ra
k: j=l

j=! Lk=j+al-1)
Therefore, by (1.11), the gain over subperiod | is
Y,

G,= (c +(-1 y) Y Za - 2c¥ Sar,

j=l

(ua),

Note that, in the absence of transaction costs (i.e., c = 0), the expected gain G; is entirely
determined by price increments and the sign of the first signal of the subperiod.

1.3 CUSUM timing

Next, we describe a version of the CUSUM statistic process and its associated CUSUM
stopping rule, which we will use to devise a timing scheme based on the quickest
detection of trends, and incorporate this scheme to our trading strategy.

1.3.1 CUSUM PROCESS AND STOPPING TIME

In this section, we begin by introducing the measurable space (@, F), where Q = R®,
PF =U,Fy and Fn = O{Y¥i,t € {0, 1, ....7}}, The law of the sequence Y;, i = 1, ..., is
described by the ‘family of probability measures {P,}, v € N*. In other words, the
probability measure P,, for a given v > 0, playing the role of the change point, is the
\n\n=== PAGE 34 ===\n(1.12)
(1.13)
(1.14)
measure generated on Ω by the sequence Yi, i = 1, …, when the distribution of the Yi’s
changes at time ν. The probability measures P0 and P∞ are the measures generated on Ω
by the random variables Yi when they have an identical distribution. In other words, the
system defined by the sequence Yi undergoes a “regime change” from the distribution P0
to the distribution P∞ at the change point time ν.
The CUSUM statistic is defined as the maximum of the log-likelihood ratio of the
measure Pν to the measure P∞ on the σ-algebra 
. That is,
is the CUSUM statistic on the σ-algebra 
. The CUSUM statistic process is then the
collection of the CUSUM statistics {Cn} of (1.12) for n = 1, …. The CUSUM stopping rule
is then
for some threshold h > 0. In the CUSUM stopping rule (1.13), the CUSUM statistic
process of (1.12) is initialized at
The CUSUM statistic process was first introduced by Page [24] in the form that it takes
when the sequence of random variables Yi is independent and Gaussian; that is, Yi ~ N(μ,
1), i = 1, 2, …, with μ = μ0 for i < ν and μ = μ1 for i ≥ ν. Since its introduction by Page [24],
the CUSUM statistic process of (1.12) and its associated CUSUM stopping time of (1.13)
have been used in a plethora of applications where it is of interest to perform detection of
abrupt changes in the statistical behavior of observations in real time. Examples of such
applications are signal processing (see [10]), monitoring the outbreak of an epidemic
(see [29]), financial surveillance (see [14] and [9]), and more recently computer vision
(see [19] or [30]). The popularity of the CUSUM stopping time (1.13) is mainly due to its
low complexity and optimality properties (see, for instance, [21], [22, 23], [6] and [27] or
[26]), in both discrete and continuous time models.
As a specific example, we now derive the form in which Page [24] introduced the
CUSUM. To this effect, let Yi ~ N(μ0, σ2) that change to Yi ~ N(μ1, σ2) at the change point
time ν. We now proceed to derive the form of the CUSUM statistic process (1.12) and its
associated CUSUM stopping time (1.13) in the example set forth in this section. To this
effect, let us now denote by 
 the Gaussian kernel. For the sequence of
random variables Yi given earlier, we can now compute (see also [28] or [25]):
\n\n=== OCR PAGE 34 ===\nmeasure generated on Q by the sequence Y;, i = 1, ..., when the distribution of the Y;’s
changes at time v. The probability measures P, and P,, are the measures generated on Q
by the random variables Y; when they have an identical distribution. In other words, the
system defined by the sequence Y; undergoes a “regime change” from the distribution P,
to the distribution P,, at the change point time v.

The CUSUM statistic is defined as the maximum of the log-likelihood ratio of the
measure P,, to the measure P,, on the o-algebra F.,. That is,

dP. 112
C, := max log — (12)
O<v<n

F,

oo IF,

is the CUSUM statistic on the o-algebra ¥n. The CUSUM statistic process is then the
collection of the CUSUM statistics {C,,} of (1.12) for n = 1, .... The CUSUM stopping rule

is then

v

OSv<n

d .
T(h) := inf {* > 0: max log P > w ; (113)

for some threshold h > 0. In the CUSUM stopping rule (1.13), the CUSUM statistic
process of (1.12) is initialized at

2

Co = 0. (4.14),

The CUSUM statistic process was first introduced by Page [24] in the form that it takes
when the sequence of random variables Y; is independent and Gaussian; that is, Y; ~ N(u,
1), i= 1, 2,..., with p = uy fori < v and pt = p1, fori = v. Since its introduction by Page [24],
the CUSUM statistic process of (1.12) and its associated CUSUM stopping time of (1.13)
have been used in a plethora of applications where it is of interest to perform detection of
abrupt changes in the statistical behavior of observations in real time. Examples of such
applications are signal processing (see [10]), monitoring the outbreak of an epidemic
(see [29]), financial surveillance (see [14] and [9]), and more recently computer vision
(see [19] or [30]). The popularity of the CUSUM stopping time (1.13) is mainly due to its
low complexity and optimality properties (see, for instance, [21], [22, 23], [6] and [27] or
[26]), in both discrete and continuous time models.

As a specific example, we now derive the form in which Page [24] introduced the
CUSUM. To this effect, let ¥; ~ N(ug, 07) that change to Y; ~ N(,, 0”) at the change point
time v. We now proceed to derive the form of the CUSUM statistic process (1.12) and its
associated CUSUM stopping time (1.13) in the example set forth in this section. To this
effect, let us now denote by #0) Vin the Gaussian kernel. For the sequence of
random variables Y; given earlier, we can now compute (see also [28] or [25]):

\n\n=== PAGE 35 ===\n(1.15)
(1.16)
(1.17)
(1.18)
(1.19)
(1.20)
(1.21)
In view of (1.14), we initialize the sequence (1.15) at 
 and proceed to distinguish
the following two cases:
Case 1: μ1 > μ0: divide out μ1 − μ0, multiply by the constant σ2 in (1.15), and use (1.13)
to obtain the CUSUM stopping rule T+ :
for an appropriately scaled threshold h+ > 0.
Case 2: μ1 < μ0: divide out μ1 − μ0, multiply by the constant σ2 in (1.15), and use (1.13)
to obtain the CUSUM stopping rule T−:
for an appropriately scaled threshold h− > 0.
As shown in the study [24] or [11], we can reexpress the stopping times (1.16) and (1.17)
in terms of the recurrence relations
which lead to
The sequences un and dn of (1.18) and (1.19), respectively, form a CUSUM according to
the deviation of the monitored sequential observations Yn from the average of their pre-
and postchange means. The first time that one of these sequences reaches its threshold
(in (1.20) or (1.21)), the respective alarm T+ or T− fires.
\n\n=== OCR PAGE 35 ===\ndP, (= ) (4.15),

C, = max log

= max log
<v<n

; g
O<v<n n ( ¥— Ho
F, Ho

1 . mi tK
=> max (p, =H) > [»- mot) :

o2 0<v<n

eS

= MitHo
In view of (1.14), we initialize the sequence (1.15) at Y= 2 and proceed to distinguish
the following two cases:

« Case 1: LL, > Hy: divide out p1, — 19, multiply by the constant o? in (1.15), and use (1.13)
to obtain the CUSUM stopping rule T* :

” + (1.16),
rey aint {20 max [:-" =| su}

O<v<n
for an appropriately scaled threshold h* > 0.

« Case 2: , < Ho: divide out 11, - 9, multiply by the constant o? in (1.15), and use (1.13)
to obtain the CUSUM stopping rule T-:

nm + (4.17),
T-(h-) = inf {' >0: max [A - | > }

Osv<n 2
p

for an appropriately scaled threshold h- > 0.

As shown in the study [24] or [11], we can reexpress the stopping times (1.16) and (1.17)
in terms of the recurrence relations

+ 1.18
Uy = 0; u, 2= max {0.4.1 + (1, - AH sta) } (1.18),

d, = 0; d, := max {o d,_\— (». - a) } : fat)

which lead to
Tt(ht) =inf{n >O:u, >h*}, (1.20),
T-(h-) =inf{n>0:d,>h-}. (1.21)

The sequences u,, and d,, of (1.18) and (1.19), respectively, form a CUSUM according to
the deviation of the monitored sequential observations Y,, from the average of their pre-
and postchange means. The first time that one of these sequences reaches its threshold
(in (4.20) or (1.21)), the respective alarm T* or T™ fires.
\n\n=== PAGE 36 ===\n(1.22)
(1.23)
Although the stopping times (1.16) and (1.17) and their respective equivalents (1.20) and
(1.21) can be derived by formal CUSUM regime change considerations using the example
set forth in this section, they may also be used as general nonparametric stopping rules
directly applied to sequential observations as seen in the study by Brodsky and
Darkhovsky [8] or Devore [11]. The former can be used as a general stopping rule to
detect an upward change in the mean while the latter a downward one. In many
applications, it is of interest to monitor an upward or downward change in the mean of
sequential observations simultaneously. This gives rise to the two-sided CUSUM (2-
CUSUM), which was first introduced by Barnard [4], and whose optimality properties
have been established in Hadjiliadis [17], Hadjiliadis and Moustakides [16], and
Hadjiliadis et al. [18]. In the context presented in this section, the 2-CUSUM stopping
time takes the form
where T+(h+) appears in (1.20) and T−(h−) in (1.21). The symmetric version of the 2-
CUSUM stopping time is that of (1.22) when h+ = h− = h.
1.3.2 A CUSUM TIMING SCHEME
We now apply the aforementioned CUSUM stopping rule of (1.22) to a stream of data
representing the value of the underlying asset without any model assumptions. In other
words, the underlying asset is not necessarily assumed to be independent or normally
distributed. That is, we apply the forms (1.16) and (1.17) in a nonparametric fashion. Let
M > 0 denote the “tick size” of the asset being monitored (presuming that S changes in
increments of M; we do not know the probability distribution of these changes), and h >
0 be a given threshold. Given that S0 = s, recall that T0 = 0. We monitor the progress of
upward or downward adjustments in the price Sn of the underlying, by individual ticks.
In view of the previous subsection at time Tk, μ0 is set to the value of the underlying at
time Tk, namely 
 and μu
1 = STk + M and μd
1 = STk − M are the two “new” mean
levels to be monitored against. Thus, as in equations (1.18) and (1.19), which cumulate
the deviations of the monitored sequence from the average of their pre- and postchange
means, we now monitor the deviations of the underlying sequence Sn, n = 1, 2…, from the
quantities
where k ≥ 0. To this effect, set uk
0 = d0
k = 0, and for n ≥ 1, define the CUSUM statistics
\n\n=== OCR PAGE 36 ===\nAlthough the stopping times (1.16) and (1.17) and their respective equivalents (1.20) and
(1.21) can be derived by formal CUSUM regime change considerations using the example
set forth in this section, they may also be used as general nonparametric stopping rules
directly applied to sequential observations as seen in the study by Brodsky and
Darkhovsky [8] or Devore [11]. The former can be used as a general stopping rule to
detect an upward change in the mean while the latter a downward one. In many
applications, it is of interest to monitor an upward or downward change in the mean of
sequential observations simultaneously. This gives rise to the two-sided CUSUM (2-
CUSUM), which was first introduced by Barnard [4], and whose optimality properties
have been established in Hadjiliadis [17], Hadjiliadis and Moustakides [16], and
Hadjiliadis et al. [18]. In the context presented in this section, the 2-CUSUM stopping
time takes the form

Tht) AT Uh), (1.22),

where T*(h*) appears in (1.20) and T-(h") in (4.21). The symmetric version of the 2-
CUSUM stopping time is that of (122) when h* = hv =h.

1.3.2 ACUSUM TIMING SCHEME

We now apply the aforementioned CUSUM stopping rule of (1.22) to a stream of data
representing the value of the underlying asset without any model assumptions. In other
words, the underlying asset is not necessarily assumed to be independent or normally
distributed. That is, we apply the forms (1.16) and (1.17) in a nonparametric fashion. Let
M > o denote the “tick size” of the asset being monitored (presuming that S changes in
increments of M; we do not know the probability distribution of these changes), and h >
0 bea given threshold. Given that S, = s, recall that T, = 0. We monitor the progress of

upward or downward adjustments in the price S,, of the underlying, by individual ticks.
In view of the previous subsection at time Tj, [Up is set to the value of the underlying at
time T;, namely “0 = 57, and p, = Si, + Mand 4, = Sy, — Mare the two “new” mean

levels to be monitored against. Thus, as in equations (1.18) and (1.19), which cumulate
the deviations of the monitored sequence from the average of their pre- and postchange
means, we now monitor the deviations of the underlying sequence S,,, n = 1, 2..., from the

quantities

av Sn +M)+ Sy, | M (2.23)
m: => SF

d. (Sp, — M) + Sy, M
m, = rr = T, 7 a:

where k = 0. To this effect, set u“, = do* = 0, and for n > 1, define the CUSUM statistics
\n\n=== PAGE 37 ===\n(1.24)
(1.25)
Thus, for k ≥ 0, the CUSUM timing scheme for our trend-following trading strategy is
defined by using (1.20) and (1.21) (and coming from (1.1) and (1.2)),
In other words, each Tk is the symmetric 2-CUSUM stopping time of (1.22) for cycle k.
Finally, at the “end of day,” that is, on the final tick, we close out our position, inducing a
final shift point to end trading, for algorithmic purposes.
1.3.3 US TREASURY NOTES, CUSUM TIMING
The following figures and chart describe the CUSUM timing scheme (1.25) applied to the
trading strategy (1.5) for US Treasury notes sold at auction in 2011. Gains quoted are in
increments of $1000. In Figure 1.1, we show the asset price, along with the number of
shares held, per-subperiod gain, and running total gain. Figures 1.2, 1.3, 1.4, 1.5 and 1.6
show the individual subperiod gains, plotted by the number of signals during a
subperiod, of the gain for 5-year and 30-year treasury notes, and Figure 1.7 aggregates
the data from Figures 1.3, 1.4, 1.5 and 1.6 for 30-year notes.
\n\n=== OCR PAGE 37 ===\nuk := max{0, uy + (Sner, _ m"')}, (1.24)
dé := max{0, ad - (Siar, - mé)}.
Thus, for k = 0, the CUSUM timing scheme for our trend-following trading strategy is
defined by using (1.20) and (1.21) (and coming from (1.1) and (1.2),

Property + (k +1): uk >h; Property —(k+1): d‘ >h (2.25),
J :=min{n >0: Property +(k +1) or —(k+ 1) occurs}
Ty 2 =T, +5,

In other words, each T;, is the symmetric 2-CUSUM stopping time of (1.22) for cycle k.

Finally, at the “end of day,” that is, on the final tick, we close out our position, inducing a
final shift point to end trading, for algorithmic purposes.

1.3.3 US TREASURY NOTES, CUSUM TIMING

The following figures and chart describe the CUSUM timing scheme (1.25) applied to the
trading strategy (1.5) for US Treasury notes sold at auction in 2011. Gains quoted are in
increments of $1000. In Figure 1.1, we show the asset price, along with the number of
shares held, per-subperiod gain, and running total gain. Figures 1.2, 1.3, 1.4, 1.5 and 1.6
show the individual subperiod gains, plotted by the number of signals during a
subperiod, of the gain for 5-year and 30-year treasury notes, and Figure 1.7 aggregates
the data from Figures 1.3, 1.4, 1.5 and 1.6 for 30-year notes.

\n\n=== PAGE 38 ===\nFIGURE 1.1 Plot of the first subperiods, and cumulative gain, for the CUSUM strategy,
August 2, 2011, US 5-year treasury note.
FIGURE 1.2 Lengths of subperiods versus gains, August 2, 2011, US 5-year treasury
note.
\n\n=== OCR PAGE 38 ===\n+1.007e2

08 —
Cumulative gain G, + Gp Cumulative
0.7 gain G, + G+ Gg
0.6 Gain Go, run 2
0.5
pare |
0.4 Asset price Sn i
Gain G,, run 1 (dots are signals) jg..." 1
0.3 pz nom wf EE
el e | i eee _p—A~n
0.2 cs | =a— ber")
Negative gain G3, run 3

) 50 100 150 200 250
FIGURE 1.1 Plot of the first subperiods, and cumulative gain, for the CUSUM strategy,
August 2, 2011, US 5-year treasury note.

0.5
0.4
0.3 °
a7
0.2
0.1 :
*
°
3
° rir;
0 j 4 6 7 8 9 10

FIGURE 1.2 Lengths of subperiods versus gains, August 2, 2011, US 5-year treasury
note.
\n\n=== PAGE 39 ===\nFIGURE 1.3 Subperiod length versus gain, July 29, 2011, US 30-year treasury note.
FIGURE 1.4 Subperiod length versus gain, August 1, 2011, US 30-year treasury note.
\n\n=== OCR PAGE 39 ===\n0.6
0.4 ‘

0.2

0
0

0.2

6m cums

ee
oe
Wacieme

FIGURE 1.3 Subperiod length versus gain, July 29, 2011, US 30-year treasury note.
a

0.8
0.6 ; =
0.4 :
0.2 ; : :
: ry
: H
0 : 3 |
0 j 3 4 5 6 7
-0.2 s .

FIGURE 1.4 Subperiod length versus gain, August 1, 2011, US 30-year treasury note.
\n\n=== PAGE 40 ===\nFIGURE 1.5 Subperiod length versus gain, August 2, 2011, US 30-year treasury note.
FIGURE 1.6 Subperiod length versus gain, August 3, 2011, US 30-year treasury note.
\n\n=== OCR PAGE 40 ===\n0.8 .
0.6 ,
0.4 —e
. e bd
0.2 3 ;
|
0 > 3 . 7
o0 i j | 4 5 6 7 8

°

FIGURE 1.5 Subperiod length versus gain, August 2, 2011, US 30-year treasury note.

2.5

15

0.5 °

j |
otha
0 > 4 6 8 10 12
-0.5
FIGURE 1.6 Subperiod length versus gain, August 3, 2011, US 30-year treasury note.

14
\n\n=== PAGE 41 ===\n(1.26)
FIGURE 1.7 Figures 1.3, 1.4, 1.5 and 1.6 combined (30-year).
1.4 Example: Random walk on ticks
We now describe a simple example to model the asset price motions. Assume that ∃N > 0
such that the sequence 
 are the steps of a random walk taking integer values
bounded between − N and N, that is, |Xj| ≤ N for all 
, and that Xj ∈ { − N, −N + 1,
..., N − 1, N} for every j, with pk = P(Xj = k) ≥ 0 and ∑N
k = −Npk = 1. Let S0 = s, and for n ≥
1, set Sn = s + ∑n
j = 1Xj. We will consider Sn to be a random walk on ticks, rather than
price itself, and so normalize tick size to M = 1.
Note that, since Δn = 0⇔n = α(l) for some l ∈ {0, 1, 2, ...}, the expected gain over a
subperiod is the expected gain over an excursion to zero on Δn, and so we can simply
consider the first excursion (independent of other excursions) on the time interval (Tα(0)
= 0, Tα(1)]. Also, note that in this case, if the transaction cost c = 0, the Gl of (1.11) are IID
random variables.
Set
and note that signal timing increments are independent. Conditioned on the sign of
signal α(l − 1) + 1 at time Tα(l − 1) + 1, Yl is a geometric random variable (starting at 1)
which gives the number of signals of the same sign in subperiod l. The distribution of Yl,
conditioned on 
, is
\n\n=== OCR PAGE 41 ===\n25

2 .
15
> = 8
0.5 44
jit
0
ehaes 6 8 10 12 14

°

FIGURE 1.7 Figures 1.3, 1.4, 1.5 and 1.6 combined (30-year).

1.4 Example: Random walk on ticks

We now describe a simple example to model the asset price motions. Assume that 4N > 0

such that the sequence {Xj lien are the steps of a random walk taking integer values
bounded between ~ N and N,, that is, |X;| < N for all / € N, and that X; « {- N,-N +1,
.., N - 1, N} for every j, with p;, = PO = k) = oand Y®,_ yp, = 1. Let Sy = s, and for n =
1, set S,, = 5 +") ,X;. We will consider S,, to be a random walk on ticks, rather than
price itself, and so normalize tick size to M = 1.

Note that, since A, = oon = a(/) for some! € {0, 1, 2, ...}, the expected gain over a
subperiod is the expected gain over an excursion to zero on A, and so we can simply
consider the first excursion (independent of other excursions) on the time interval (Tyo
= 0, Tyy]- Also, note that in this case, if the transaction cost c = 0, the G; of (1.11) are IID
random variables.

Set
pt:=P(T,=T}), p= 1—-p* = PT, =T,), (1.26),

and note that signal timing increments are independent. Conditioned on the sign of
signal a(/ — 1) + 1 at time Ty _ 1) , , Y; is a geometric random variable (starting at 1)

which gives the number of signals of the same sign in subperiod 1. The distribution of Y;,

conditioned on * T-»+1, is
\n\n=== PAGE 42 ===\n(1.27)
(1.28)
(1.29)
(1.30)
To explain this, consider the case Tα(l − 1) + 1 = T+
α(l − 1) + 1 (the first + signal of a bull run
subperiod): a subperiod of + has “failure” probability p+ (a + signal continues the
subperiod with another buy) and “success” probability p− (a − signal causes a sell-off and
ends the subperiod).
Al is an 
-measurable random variable, and every increment in the sum in (1.11) is
independent of time Tα(l − 1) + 1. Finally, note that the Yl are independent of the walk up to
time Tα(l − 1), and if c = 0, so are the Gl.
1.4.1 RANDOM WALK EXPECTED GAIN OVER A SUBPERIOD
We wish to examine the expected gain E(Gl) over subperiod l. For simplicity in our initial
analysis, set c = 0. Since the Gl are IID, we will calculate E(G1). This is, since α(0) = 0
and Y1 = α(1) − α(0) − 1 = α(1) − 1, by (1.11),
We condition over the possible values of Y1 and A1. Note that the sign of T1 also
determines the possibilities of Zj for j = 1, 2, ..., Y1 − 1. Zj depends on the type of
subperiod it resides on, so by the fact that the event 
, and by setting, for j
= α(l − 1) + 1, ..., α(l),
then, for n = 1, 2, ..., we have
Since the conditioning on B+
j, 1, n (and, likewise, B−
j, 1, n) is based only on the walk during
the time increments (T0, T1] and (Tα(1) − 1, Tα(1)], for n > 1, B+
j, 1, n and B−
j, 1, n are
numbers for j = 1, 2, ..., n − 1. Also, for these j, B+
j, 1, n are the same by the strong Markov
property at Tj − 1 since the signs on the Tj are all +. However, since the signal Tα(1) = Tn + 1
has different sign than Tn, B+
n, 1, n has a different distribution. In fact, since this
\n\n=== OCR PAGE 42 ===\ngeom(p-) if Tyan = Tyg (4.27),
i“ geom(p*) if Tyy_ Dat = Treas

To explain this, consider the case Tay _ 1) 4. = T* aq —1) +1 (the first + signal of a bull run

subperiod): a subperiod of + has “failure” probability p* (a + signal continues the
subperiod with another buy) and “success” probability p~ (a — signal causes a sell-off and
ends the subperiod).

A;is an Frg-sy-measurable random variable, and every increment in the sum in (1.11) is
independent of time Tyq _ 1) , ,. Finally, note that the Y; are independent of the walk up to
time Ty _ , and if ¢ = 0, so are the Gj.

1.4.1 RANDOM WALK EXPECTED GAIN OVER A SUBPERIOD

We wish to examine the expected gain E(G)) over subperiod 1. For simplicity in our initial
analysis, set c = 0. Since the G, are IID, we will calculate E(G,). This is, since a(o) = 0
and Y, = a(1) - a(o) - 1 = a(1) - 1, by (1.14),

E(G,)=E Joo da)

We condition over the possible values of Yan and A,. Note that the sign of T, also
determines the possibilities of Z; for j = 1, 2, ..., Y, — 1. Z; depends on the type of
(Y= n} € Fy,

(1.28)

subperiod it resides on, so by the fact that the event
=a(l-1) +1,.., a(D,

and by setting, forj

Bi, = EZ | Teaeyar = Tyg Y=) (2.29)
Bin = EZ | Teas = = Type Y=;

then, for n = 1, 2, ..., we have

(1.30)
Jicrn-e]-e [de|ni= nr r=]
= Del |7- T} =n) = Li.

Since the conditioning on B*, , , (and, likewise, B", , ,) is based only on the walk during

the time increments (To, 7.) and (T. ata) - » Tay] forn >1,B*,,,andB; , , are

numbers for j = 1, 2, — 1. Also, for these j, B*; , , are the same by the strong Markov
property at T;_ , since Sthe signs on the T; are all +. However, since the signal Ty) = Ty 41

has different sign than T,,, B*, ,,, has a different distribution. In fact, since this
\n\n=== PAGE 43 ===\n(1.31)
(1.32)
(1.33)
condition implies that Tn + 1 = Tα(1) = T−
α(1), B+
n, 1, n can be written by the strong Markov
property at Tn = Tα(1) − 1 as
To simplify notation, we rewrite B+
1, 1, n = B+ and B−
1, 1, n = B−, since they do not depend
on n. In the case n = 1, we simply have B+
1, 1, 1 = B− and B−
1, 1, 1 = B+, and note that B+ ≥ 0
and B− ≤ 0. Thus, our sum (1.30) becomes
The only thing that needs to change for the analogous argument for B−
j, 1, n are the signs;
thus, we also have
Next, we give the probability that Y1 = n, conditioned on the sign of T1. This is easy, since
we know that, conditioned on the sign of T1, Y1 is a geometric random variable. By (1.27),
for n = 1, 2, ...,
By (1.31), (1.32), and (1.33), and recalling that p− = 1 − p+, the expected gain on a
subperiod, given that the subperiod consists of n signals before a liquidation, is
\n\n=== OCR PAGE 43 ===\ncondition implies that T,, , , = Taq) = Taq BY n, 1, n Can be written by the strong Markov
property at T,, = Ty - 1 aS

Bt, =E Z | T,=T}.Y, = 4 =E Z | T, =T}.T, T;

a(=ntl = fay

=E [2 Tyjantt = Tra =B,

‘n

To simplify notation, we rewrite B*, , , = B* and B™, , , = B™, since they do not depend
on n. In the case n = 1, we simply have B*, , , = B” and B”, , , = B*, and note that B* > 0
and B- < o. Thus, our sum (1.30) becomes

n nol
1=T).Y,= 5 = Ye, = vie, +nBr
j=l j=l
= a D B+ + mB.

The only thing that needs to change for the analogous argument for B’, , ,, are the signs;
thus, we also have

2

y,

-1 (1.32).
E Pau =T,.¥,= ‘| = MO 4 Bt.
jal

Next, we give the probability that Y, = n, conditioned on the sign of T,. This is easy, since
we know that, conditioned on the sign of T,, Y, is a geometric random variable. By (1.27),
for n = 1, 2,...,
PY, =n|T, =T}) = (ty) (1:33)
PY =n|T, = Ty) = )"(p*).

By (1.31), (2.32), and (1.33), and recalling that p~ = 1 - p*, the expected gain ona
subperiod, given that the subperiod consists of n signals before a liquidation, is
\n\n=== PAGE 44 ===\n(1.34)
(1.35)
(1.36)
(1.37)
The probability that a subperiod lasts n signals, regardless of its sign, is, by (1.27) and
(1.33),
which also gives the expected number of same-sign signals in a subperiod
Note that this necessarily matches the calculation via conditioning on T1’s sign; that is,
by (1.33),
We can sum over all possible values n in (1.34), and use (1.35) to get the expected gain of
a subperiod in terms of p+, p−, B+, and B−:
\n\n=== OCR PAGE 44 ===\nuf (1.34),
E(G,|Y, =n) =ptE pa

J=|

T= TT. Y= 7

iZ,

jal

=pr (“2 Dp + nt) —p- (“> Dp +nB*)

T,=T;.¥, -,|

_ antl) (
a)
The probability that a subperiod lasts n signals, regardless of its sign, is, by (1.27) and
(2.33),

PY, =n) = PY, =n |T, = TPT, = TY) + PY, =n IT, = TPT, =T)) (4.35).

Btpt — Bp-) +n(B- — Bt).

= (pty"(p-) + (P)"(P*),

which also gives the expected number of same-sign signals in a subperiod

. pe (1.36)
E(Y,) = nP(Y, =n) =—+—.
' x ' p- pt
Note that this necessarily matches the calculation via conditioning on T,’s sign; that is,
by (2.33),

+p

E(Y,) = EY |T, = T)p* + EY, |T, = T)p- = Pp +—.

We can sum over all possible values n in (1.34), and use (1.35) to get the expected gain of
a subperiod in terms of p*, p", B*, and B°:

< L
EG,) = Y) EGY, =n)PM, =n) (137),

n=1

-> oem D (Btp* — B-p-) + n(B- — #|

n=1

x ((pt)"(p7) + @)"P"))
\n\n=== PAGE 45 ===\n(1.38)
Note that, if 
 (which holds for any symmetric random walk), then E(G1) = 0, and
as p+↓0 or p+↑1, E(G1) → ∞.
1.4.2 SIMPLE RANDOM WALK, CUSUM TIMING
We now calculate the expected return of the first subperiod for a simple random walk
asset price, applying CUSUM timing. Set our CUSUM threshold to h = 1, and our
probability measure to the simple asymmetric random walk on ticks, that is, N = 1, with
p1 = p, p− 1 = 1 − p for some 0 < p < 1. With M = 1, we have by (1.23), for every k ≥ 0,
Since Xj ∈ { − 1, 1} for every 0 ≤ j < T1, the possible values of u0
j and d0
j, by (1.24), are
, where a 2 occurs only with two consecutive ticks of the same type (ending on an
even step). T1 is the first time 2j such that u0
2j ≥ 1 or d0
2j ≥ 1. Hence,
Given S0 = s > 0, 
 can take only two possible values, from the paths described earlier.
For j = 1, 2, ..., each possibility takes the form of a geometric random variable
conditioned on the final two steps 
, where a “failure” is a sequence of two steps
of opposite direction; that is, + 1 then − 1, or − 1 then + 1.
An illustration of the paths leading to a “+” signal T+
1 is shown in Figure 1.8. The
probabilities of each value of 
 occurring are
Since there is only one possible outcome per signal type, these match the probabilities of
each type of signal occurring:
\n\n=== OCR PAGE 45 ===\n_1
Note that, if? ns 2 (which holds for any symmetric random walk), then E(G,) = 0, and
as p* |o or p*t1, E(G,) > ».

1.4.2 SIMPLE RANDOM WALK, CUSUM TIMING

We now calculate the expected return of the first subperiod for a simple random walk
asset price, applying CUSUM timing. Set our CUSUM threshold to h = 1, and our
probability measure to the simple asymmetric random walk on ticks, that is, N = 1, with
P; =P, p-,=1-p for some 0 < p < 1. With M =1, we have by (1.23), for every k = 0,

1 1
mi, = Sy, + oy mi = Sp - 2
Since X; € {- 1, 1} for every 0 <j < T,, the possible values of u°, and d®;, by (1.24), are
a)
{0, 2° ) where a 2 occurs only with two consecutive ticks of the same type (ending on an
even step). T, is the first time 2j such that u°,; > 1 or d°,; > 1. Hence,

T, = TP = SX) =X, VL Sk < 2-1, X=
T,=T, =Y SX, =-X, VAL Sk < 2-1 X=

Given S, = s > 0, Sr, can take only two possible values, from the paths described earlier.
Forj = 1, 2, ..., each possibility takes the form of a geometric random variable

conditioned on the final two steps Xp-1 Xr, where a “failure” is a sequence of two steps
of opposite direction; that is, + 1 then - 1, or - 1 then + 1.

{T,=Tf} = {Sp =s+2}: (1.38).
P(Sp, = 5 +2, Th = 2) = [2p(1 — p)¥-!p?

{T) =Ty} <— {Sp =s-2};
P(Sp, = 8-2, T, = 2/) = [2p — p)P' — py.

«wy

An illustration of the paths leading to a “+” signal T*, is shown in Figure 1.8. The

probabilities of each value of Sr, occurring are

eS

PS;, = 5 +2) = Y2pU - pF |p? =

j=l

PS, = 5-2) = Y' Pp - pF = pyY =

j=l

a

1 —2p(1 —p)
(—py

1 —2p(1—p)

Since there is only one possible outcome per signal type, these match the probabilities of
each type of signal occurring:
\n\n=== PAGE 46 ===\n(1.39)
(1.40)
(1.41)
(1.42)
The increment 
 then takes values in { − 2, 2} and depends on the sign of
the signal of Tk + 1. Conditioned on this signal sign, and by the strong Markov property at
Tk, we get the conditional expectations
Thus, by (1.34), (1.40), (1.41), and (1.37), we have the expected gain
which can be shown to be symmetric about its minimum 
 (at E(G1) = 0), with
limp↓0E(G1) = limp↑1E(G1) = ∞.
\n\n=== OCR PAGE 46 ===\nPp (4.39)
+ := P(T, = T+) = P(Sy, =s +2) =" —_.;
P (T; 1) = P(S7, = 5 +2) 1—2pd -p)
_ _ (l=py
= P(T, = T-) = P(S, =s—2)= :
P (T, ry) (Sp, =s—2) 1 - 2p —p)

The increment q= Sr... ~ Sr, then takes values in { - 2, 2} and depends on the sign of
the signal of T;, ,, ,. Conditioned on this signal sign, and by the strong Markov property at

T;,, we get the conditional expectations
BY = E(Z,| Ty) = Th.) = 2P(Sp, — 5 = 217, = TY) = 2, (1.40),
Bo = E(Z,| Thy = Ty) = —2P (Sp, — 8 = -2|T, = Ty) = -2. (1.44),
Thus, by (1.34), (1.40), (1.41), and (1.37), we have the expected gain
2p? — (1 — p)*) 2 (=p)? 1.42
EG,) = 22 P P -- P (1.42),
1-2p1—p) |(—p) pt

1
which can be shown to be symmetric about its minimum P= 35 (at E(G,) = 0), with

limpoB(G,) = lim, ,,E(G,) = ©.
\n\n=== PAGE 47 ===\n(1.43)
(1.44)
FIGURE 1.8 The four possible SRW paths for T+
1 = 2(3) = 6.
We also have the expected time until a signal occurs: by (1.38) and (1.39),
Finally, the expected number of same-sign signals in a subperiod is, by (1.36) and (1.39),
\n\n=== OCR PAGE 47 ===\nSg=s+2
0

ug =2,dg =0
Sj=st+1
O_1 Q_og je
uj = a =0,j/=1,3,5
Sj=s
ie) 0 ;
\ 7N\ / uj = dj =0,j=0, 2,4
\ 4 oN /
\ if \ é
/ /
\ \
Nv Nas
t ) Sj=s-1 td
07s O.. 1%:
uj =0,d7 = pi=i.8
FIGURE 1.8 The four possible SRW paths for T*, = 2(3) = 6.
We also have the expected time until a signal occurs: by (1.38) and (1.39),
— L.
E(T,|T, = TH) = Yea, =2j|T, =TH) (43)
j=l
& PT, =2.T,=TS) we -
=2 § j———+_ = = YS jp - p>
A! pa, =) p dA pa pil
= 2p? = 2 :
pt(1—[2p(1—p))? 1 -2pA py’
2
E(T, | T, = T>) = ———————::
(MINS T= Tp pi
E(T,) = ET, | T, = Tf)p* + ET, |T, = Ty )p~
2
~ 1=[2p0 =p)

Finally, the expected number of same-sign signals in a subperiod is, by (1.36) and (1.39),

+ — 2

l-p)  p*+(1—-p)t .
BY) =" a _ +! P _P +( P ; (144),
po pt = (1—py p> p?(1—py

\n\n=== PAGE 48 ===\n(1.45)
(1.46)
(1.47)
(1.48)
1.4.3 LAZY SIMPLE RANDOM WALK, CUSUM TIMING
Introducing a more complicated random walk distribution, such as a lazy simple random
walk, with step distribution
where p− 1 + p0 + p1 = 1 increases the complexity of the analysis of the CUSUM timing
strategy probabilities, and therefore of calculating the expected gain analytically. We will
retain h = 1 and M = 1.
By introducing a zero tick, we expand the possible cases of “failure” to set off a CUSUM
signal. We decompose the lazy random walk path into seven distinct possible
components. First, there are three possible patterns that fail to set off a signal, being “up-
down” (with probability p1p− 1), “down-up” (with probability p− 1p1), and “zero” (a one-
step pattern with probability p0). Note that the first two of these are the two possible
failure patterns of (1.38). There are, consequently, four “success” patterns:
the two from (1.38): “up-up” (with probability p2
1) and “down-down” (with
probability p2
− 1);
and two patterns with zero ticks: “up-zero” (with probability p1p0) and “down-zero”
(with probability p− 1p0).
The number of such patterns that occurs up to a signal time is geometric. Define
as the respective signal-pattern success and failure probabilities. Then, we define Vj as
the number of failure patterns until signal j = 1, 2, .... Vj ~ geom(S*) (starting at 0), and,
conditioned on Vj, we define Wj as the number of zero-tick patterns that occur during
this time frame. Since the Wj zero-ticks can take place at any pattern position of the Vj
patterns, 
 bin(Vj, 
). Note that if p0 = 0, this reduces to the case in the previous
section.
We can calculate the expected time before a signal: if there are Vj failure patterns (of
length 1 or 2 ticks) before signal j, Wj of these are the 1-tick zero-tick failures, and,
finally, we have a 2-tick success pattern, then the number of ticks before the first signal is
The expected time until a signal is, then, by (1.48) and (1.47),
\n\n=== OCR PAGE 48 ===\n1.4.3 LAZY SIMPLE RANDOM WALK, CUSUM TIMING

Introducing a more complicated random walk distribution, such as a lazy simple random
walk, with step distribution

+1 with probability p, (2.45)
X;=40 with probability po
—1 with probability p_,,

where p_, + Pg + P; = 1 increases the complexity of the analysis of the CUSUM timing
strategy probabilities, and therefore of calculating the expected gain analytically. We will
retain h = 1 and M =1.

By introducing a zero tick, we expand the possible cases of “failure” to set off a CUSUM
signal. We decompose the lazy random walk path into seven distinct possible
components. First, there are three possible patterns that fail to set off a signal, being “up-
down” (with probability p,p_ ,), “down-up” (with probability p_ ,p,), and “zero” (a one-
step pattern with probability p,). Note that the first two of these are the two possible
failure patterns of (1.38). There are, consequently, four “success” patterns:

« the two from (1.38): “up-up” (with probability p?,) and “down-down” (with
probability p?_ ,);
¢ and two patterns with zero ticks: “up-zero” (with probability p,p,) and “down-zero”
(with probability p_ ,p,).
The number of such patterns that occurs up to a signal time is geometric. Define

S*:= pi +p? +PiPo + P-iPo (1.46)

F* :=1-S* =2p\p_,+po (2.47),
as the respective signal-pattern success and failure probabilities. Then, we define V; as
the number of failure patterns until signal j = 1, 2, .... Vj ~ geom(S*) (starting at 0), and,
conditioned on V;, we define W; as the number of zero-tick patterns that occur during
this time frame. Since the W; zero-ticks can take place at any pattern position of the V;

daw Po
patterns, Wily, bin(V;, F*). Note that if py = 0, this reduces to the case in the previous
section.
We can calculate the expected time before a signal: if there are V; failure patterns (of
length 1 or 2 ticks) before signal j, W; of these are the 1-tick zero-tick failures, and,
finally, we have a 2-tick success pattern, then the number of ticks before the first signal is

T, := W, + 2(V, —W,) +2 =2V, -W, +2. (148)
The expected time until a signal is, then, by (1.48) and (1.47),
\n\n=== PAGE 49 ===\n(1.49)
(1.50)
(1.51)
At p0 = 0, (1.49) reduces to (1.43).
The zero-tick success patterns increase the possible asset values at a signal. In (1.40) and
(1.41), the only possible values for the price change increment Zk of (1.9) are { − 2, 2}.
Here, the possible values of Zk are { − 2, −1, 1, 2}, and so, by the Markov property at the
times j − 2, and defining PT
j ≔ P(T1 = j)/S* for j ≥ 2, we have the probabilities
which all sum to 1, by the fact that 
. The equations in (1.50) also yield the
conditional probabilities
An illustration of possible paths leading to a “+” signal can be found in Figure 1.9.
\n\n=== OCR PAGE 49 ===\n= Le
E(T,)=2E(V,)- BW.) +2. = 26V,)— YEW |V,=9PV=y42

v=0

a» &
= 2E(V,) - e vP(V, =v) +2

v=0

At Do = 0, (1.49) reduces to (1.43).

The zero-tick success patterns increase the possible asset values at a signal. In (1.40) and
(1.41), the only possible values for the price change increment Z; of (1.9) are { — 2, 2}.
Here, the possible values of Z;, are { — 2, -1, 1, 2}, and so, by the Markov property at the
times j - 2, and defining Pt, ‘= P(T, = j)/S* for j = 2, we have the probabilities

= 1.50
P(Sp, =s+2)= Y PSy, =8 +2. Tf =i) (2.50)

=

=D PSaas.n>i-D= LP

=)

co
PCS, =st 1) =piro PF P(Sr, =s- 1) =p-1Po ), Ps

je

oo
P(S;, = 5-2) =p, DPI,

i=

j=2

riot
=2" j ~ s*. The equations in (1.50) also yield the

which all sum to 1, by the fact that »»
conditional probabilities

2

P; (u5D).
P(Sp, = 8 +2|T, = Tf) = ———,
Pi +P1Po
PSp,=s+1|T, =TH)= PPO
P\ +PiPo
_ PP
P(Sp, =s—1|T, =T;)==——_,,
P_, +P-1P0
Py

P(Sp, =s-2|T, =T)=

>

P_, +P_1Po

An illustration of possible paths leading to a “+” signal can be found in Figure 1.9.
\n\n=== PAGE 50 ===\n(1.52)
(1.53)
(1.54)
FIGURE 1.9 The 12 possible LSRW paths for T+
1 = 6.
Retaining the definitions of p+ and p− from (1.26), we get
which allows us to calculate the expected number of signals on a subperiod. By (1.52) and
(1.36),
which reduces, if p0 = 0, to (1.44). Also, if the walk is symmetric, that is, p1 = p− 1, then
E(Y1) = 2.
Next, we find B+ and B−, the expected size of the incremental changes Zk, conditioned on
the type of subperiod. Generalizing (1.40) and (1.41) (where p0 = 0), we have by (1.51)
\n\n=== OCR PAGE 50 ===\nSg=s+2

ug =2,de =0

Sg=s+1

ug =1,dg =0

Sj=s

\ 7N\ / u; =d?=0,j=0, 1,2,3,4

/ \/
t ) Sj=s-1 T )
oe

uj =0, dj =p /=1,3

FIGURE 1.9 The 12 possible LSRW paths for T*, = 6.

Retaining the definitions of p* and p~ from (1.26), we get

+

_ PiPo +P; __ P-1Po0 +P, (1.52),
=e P= s
which allows us to calculate the expected number of signals on a subperiod. By (1.52) and

(2.36),

+p PiPo+P, — P-1Po +P (453),
eyy=e+2 = ae,
PU PX PyPot+P2, — PiPot PY

which reduces, if py = 0, to (1.44). Also, if the walk is symmetric, that is, p, = p_ ,, then
E(Y,) = 2.

Next, we find B* and B-, the expected size of the incremental changes Z,, conditioned on
the type of subperiod. Generalizing (1.40) and (1.41) (where py = 0), we have by (1.51)

BY = 2P(S;, —s =2|T, = Tt) + PS, —s = 1/7, = TF) (1.54)
Py

=14+—_,
P\Po t+ Py
\n\n=== PAGE 51 ===\n(1.55)
(1.56)
(1.57)
Finally, the expected gain E(G1) at the end of a subperiod can be found by combining
(1.37) with (1.52), (1.54), and (1.55), generalizing the p0 = 0 case (1.42).
1.5 CUSUM strategy Monte Carlo
Here we provide Monte Carlo simulations of the collection of random walks on ticks
given in the previous section to numerically analyze the behavior of our strategy against
such walks as asset prices.
The two classes of random walks for our simulations are special subclasses of (1.45): they
are the lazy symmetric simple random walk
and the lazy asymmetric simple random walk with upward drift
where j allows p− 1 > 0. Each class of walks was run for 200 simulated trading days, with
N = 5000 ticks for 1 day’s trading, and starting price s = 10, 000 ticks each day (to
guarantee that 1 day’s trading does not bottom out the asset).
Define the idle time of a trading strategy during a day as the (random) set of tick times
between subperiods, that is, when our algorithm declares that our portfolio be empty. If
the day consists of N ticks, then the idle time for the day is defined as
The % idle time in a day is simply 
. If there are R subperiods in a day, this is
where Tα(R) = N if the final subperiod’s end is induced by the end-of-day settling the
algorithm requires. We can estimate the average number of subperiods per day by
, and so, since there is the length of one signal between each subperiod, we can
\n\n=== OCR PAGE 51 ===\nB- = ~2P(S;, —s =-2|T, =T;) — PS;, —s =-1|T, =T;) (155)
pe

=-1-——__.

P_\Po +P-,

Finally, the expected gain E(G,) at the end of a subperiod can be found by combining
(2.37) with (2.52), (1.54), and (1.55), generalizing the py = 0 case (1.42).

1.5 CUSUM strategy Monte Carlo

Here we provide Monte Carlo simulations of the collection of random walks on ticks
given in the previous section to numerically analyze the behavior of our strategy against
such walks as asset prices.

The two classes of random walks for our simulations are special subclasses of (1.45): they
are the lazy symmetric simple random walk

+1 with probability p, = 2 (1.50),
X;=40 with probability pp € {0, 0.05, 0.1, ...,0.35}

—1 with probability p_, =

and the lazy asymmetric simple random walk with upward drift
+1 with probability p, = 0.5 — 2 + 0.05), € (0, 1,....6) (2.57)
X;= 40 with probability p9 € {0,0.1,0.2,0.3,0.4}
—1 with probability p_; = 1 — p, — po,

where j allows p_, > 0. Each class of walks was run for 200 simulated trading days, with
N= 5000 ticks for 1 day’s trading, and starting price s = 10, 000 ticks each day (to
guarantee that 1 day’s trading does not bottom out the asset).

Define the idle time of a trading strategy during a day as the (random) set of tick times
between subperiods, that is, when our algorithm declares that our portfolio be empty. If
the day consists of N ticks, then the idle time for the day is defined as

idle time := {n € {1,2,....N}: A, =0}.

[idle time|
The % idle time inadayis simply __. If there are R subperiods in a day, this is

R-1
lidle time] = }’ (Taner = Ta) + N = Tar)
i=0

where T,g) = N if the final subperiod’s end is induced by the end-of-day settling the
algorithm requires. We can estimate the average number of subperiods per day by

E(T)[E(Y)+1], and so, since there is the length of one signal between each subperiod, we can
\n\n=== PAGE 52 ===\nnaively estimate the average amount of idle time in a day as the average number of
subperiods per day multiplied by the average time to a signal, that is,
. Then, the % idle time in a day is naively estimated by this
value divided by the number of ticks per day, or, simply, 
.
Tables containing the results of simulations can be found in the Appendix, Section 1.7.
Table 1.A.1 contains experimental averages of the following values for the lazy symmetric
random walks represented by (1.56):
average gain per subperiod (1.37), which can be seen to be close to E(G1) = 0 in all
cases due to symmetry;
average subperiod length, which approximates (1.49) and (1.53)’s E(T1)E(Y1): for
example, p0 = 0.1 has 
;
average number of signals per subperiod, which approximates (1.53)’s E(Y1) + 1: for
example, p0 = 0.1 has 2.998 ≈ 2 + 1 = 3;
average number of subperiods per day, which approximates R above (which is itself
approximated above by 
); for p0 = 0.1, this is 
;
and the average % idle time; for p0 = 0.1, this is 
.
The remaining tables contain similar experimental data for various lazy simple random
walks from Section 1.4.3. Results of simulations using frequencies derived from the real
data from the 5-year and 30-year bonds are shown in Tables 1.6, 1.7, and 1.8.
Table 1.A.2 contains detail on the subperiods of these walks:
the average number of subperiods with a specific number of signals; for example, p0
= 0.1, subperiod length n = 4 has 27.31, which, when divided by the average total
number of subperiods 435.185 from Table 1.A.1, gives
 from (1.35) using (1.52);
and the average gain on such a subperiod of length n = 4, which is 3.64 ≈ E(G1 | Y1 =
4) = 3.478 from (1.34) using (1.52), (1.54), and (1.55).
Tables 1.A.3 and 1.A.4 contain the same experimental values as Tables 1.A.1 and 1.A.2,
this time from the simple random walk of Section 1.4.2. For example, in Table 1.A.3,
examining p1 = 0.65, we have
average gain per subperiod 16.378 ≈ E(G1) = 16.481 from (1.42);
average subperiod length 13.749 ≈ E(Y1) · E(T1) ≈ 3.7389 · 3.6697 = 13.7208 from
(1.44) and (1.43);
average number of signals per subperiod 4.746 ≈ E(Y1) + 1 = 4.7389 from (1.44);
\n\n=== OCR PAGE 52 ===\nnaively estimate the average amount of idle time in a day as the average number of
subperiods per day multiplied by the average time to a signal, that is,

EQEQ)+1] E(T)) = EQ)+1, Then, the % idle time in a day is naively estimated by this

value divided by the number of ticks per day, or, simply, £01.

Tables containing the results of simulations can be found in the Appendix, Section 1.7.
Table 1.A.1 contains experimental averages of the following values for the lazy symmetric
random walks represented by (1.56):

* average gain per subperiod (1.37), which can be seen to be close to E(G,) = 0 in all
cases due to symmetry;
* average subperiod length, which approximates (1.49) and (1.53)’s E(T,)E(Y,): for

~ 2-0.1 —~ 223 _~I0-7
7.610 » (Rt; ) 2) = 3.832) = 7.67,

example, p, = 0.1 has
¢ average number of signals per subperiod, which approximates (1.53)’s E(Y,) + 1: for
example, py = 0.1 has 2.998 = 2+ 1= 33

* average number of subperiods per day, which approximates R above e (which i is itself

——— 435.185 = > —— w= 434.21
approximated above by RE); for Py = 0.1, this is .83(3) 3

le ti «133.2% &% —L_ =}
¢ and the average % idle time; for p, = 0.1, this is EY)+1 3,

The remaining tables contain similar experimental data for various lazy simple random
walks from Section 1.4.3. Results of simulations using frequencies derived from the real
data from the 5-year and 30-year bonds are shown in Tables 1.6, 1.7, and 1.8.

Table 1.A.2 contains detail on the subperiods of these walks:

¢ the average number of subperiods with a specific number of signals; for example, p,
= 0.1, subperiod length n = 4 has 27.31, which, when divided by the average total
number of subperiods 435.185 from Table 1.A.1, gives

2751 x; 0.06275 = P(Y, = 4) = 0.0625

435.185

from (1.35) using (1.52);
¢ and the average gain on such a subperiod of length n = 4, which is 3.64 ~ E(G, | Y, =
4) = 3.478 from (1.34) using (1.52), (1.54), and (1.55).
Tables 1.A.3 and 1.4.4 contain the same experimental values as Tables 1.A.1 and 1.A.2,

this time from the simple random walk of Section 1.4.2. For example, in Table 1.4.3,
examining p, = 0.65, we have

¢ average gain per subperiod 16.378 ~ E(G,) = 16.481 from (1.42);

¢ average subperiod length 13.749 ~ E(Y,) - E(T,) = 3.7389 - 3.6697 = 13.7208 from
(1.44) and (1.43);
* average number of signals per subperiod 4.746 ~ E(Y,) + 1 = 4.7389 from (1.44);
\n\n=== PAGE 53 ===\n(1.58)
average number of subperiods 
; and
average % idle time 20.8% 
.
Note that, for the simple random walk without a “lazy” probability p0, the average
amount of idle time per simulation (the percentage of ticks between subperiods) drops as
the walk becomes more asymmetric, as the expected amount of time to get a signal (1.43)
(and so be in a subperiod) drops. In Table 1.A.4, the first row of each block approximates
(1.35) multiplied by the average number of subperiods from Table 1.A.3 for that p = p1,
and the second row approximates (1.34), which is n2 − 3n for n same-sign signals.
1.6 The effect of the threshold parameter
In this section, we discuss the effect of varying the threshold parameter h on the
proposed trading strategy. We first examine this effect on the real data in Section 1.3.3.
In particular, Figure 1.10 summarizes the effect of varying thresholds on the gain in all 5
US Treasury bonds of Section 1.3.3. In this figure, it is shown that varying the threshold
does not change the sign of the gain. In fact, varying the threshold in the 5-year note
leaves the daily gain almost unchanged, while in the 30-year bonds, although a more
random variation is observed, no apparent pattern of an increasing or decreasing effect
on the gain is observed. This demonstrates a level of robustness of the proposed
strategy’s gain as a function of the threshold. A closer examination shows that the
number of signals per subperiod is almost constant, regardless of the threshold size, as
shown in the column “average # of signals per subperiod” in Tables 1.1–1.5. Yet, the
number of subperiods per trading day decreases as the threshold increases. This is
shown in Figure 1.11, where we note that the number of signals per trading day decreases
at the rate of the square root of the threshold. The decrease in the number of subperiods
on a given trading day as a result of an increase in the threshold is to be expected since
the quantity that varies when the threshold varies is the number of ticks, or equivalently,
the amount of time as measured by ticks, required before the completion of a given
subperiod. This is true because a smaller threshold gives rise to a more sensitive CUSUM
stopping time. In fact, the expected time to a signal (CUSUM alarm) increases as the
threshold increases in the order of the square root of the threshold, that is,
To justify (1.58), note that, on a trend, one of the CUSUM statistics from (1.24) increases
quadratically, regardless of the threshold h. For example, on an upward trend, uk
n =
O(n2), and so the amount of time n it takes to break the threshold, that is, the minimum
n to achieve uk
n ≥ h, is found by observing O(n2) ≈ h ⇒ 
. The coefficient can
then be found by checking the baseline threshold h = 1. We also offer empirical evidence
for this from bond data and Monte Carlo simulations.
\n\n=== OCR PAGE 53 ===\n288.120 ~ ——N—— = — 50 __ = 9755
* average number of subperiods 288.120 E(D)[EQ)+1]  3.6697(4.7389) 287.5 16.

~— =.= 6
* average % idle time 20.8% E()+1 4.7389 21.10%

and

Note that, for the simple random walk without a “lazy” probability p,, the average
amount of idle time per simulation (the percentage of ticks between subperiods) drops as
the walk becomes more asymmetric, as the expected amount of time to get a signal (1.43)
(and so be in a subperiod) drops. In Table 1.A.4, the first row of each block approximates
(1.35) multiplied by the average number of subperiods from Table 1.A.3 for that p = p,,

and the second row approximates (1.34), which is n? — 3n for n same-sign signals.

1.6 The effect of the threshold parameter

In this section, we discuss the effect of varying the threshold parameter h on the
proposed trading strategy. We first examine this effect on the real data in Section 1.3.3.
In particular, Figure 1.10 summarizes the effect of varying thresholds on the gain in all 5
US Treasury bonds of Section 1.3.3. In this figure, it is shown that varying the threshold
does not change the sign of the gain. In fact, varying the threshold in the 5-year note
leaves the daily gain almost unchanged, while in the 30-year bonds, although a more
random variation is observed, no apparent pattern of an increasing or decreasing effect
on the gain is observed. This demonstrates a level of robustness of the proposed
strategy’s gain as a function of the threshold. A closer examination shows that the
number of signals per subperiod is almost constant, regardless of the threshold size, as
shown in the column “average # of signals per subperiod” in Tables 1.1-1.5. Yet, the
number of subperiods per trading day decreases as the threshold increases. This is
shown in Figure 1.11, where we note that the number of signals per trading day decreases
at the rate of the square root of the threshold. The decrease in the number of subperiods
on a given trading day as a result of an increase in the threshold is to be expected since
the quantity that varies when the threshold varies is the number of ticks, or equivalently,
the amount of time as measured by ticks, required before the completion of a given
subperiod. This is true because a smaller threshold gives rise to a more sensitive CUSUM
stopping time. In fact, the expected time to a signal (CUSUM alarm) increases as the
threshold increases in the order of the square root of the threshold, that is,

E(T,(h)] © E[T\()|Vh. (258)
To justify (1.58), note that, on a trend, one of the CUSUM statistics from (1.24) increases
quadratically, regardless of the threshold h. For example, on an upward trend, u“,, =
O(n?), and so the amount of time 7 it takes to break the threshold, that is, the minimum
n to achieve us, > h, is found by observing O(n?) = h=>" © oh), The coefficient can

then be found by checking the baseline threshold h = 1. We also offer empirical evidence
for this from bond data and Monte Carlo simulations.
\n\n=== PAGE 54 ===\nFIGURE 1.10 Total gain versus thresholds for 5-year and 30-year notes. Top graph:
Small thresholds that vary as follows: 0.5*tick, tick, 2*tick, 3*tick, etc. (up to 29*tick).
Bottom graph: Large thresholds that vary as follows: 50*tick, 2*50*tick, 3*50*tick, etc.
(up to 20*50*tick).
\n\n=== OCR PAGE 54 ===\nTotal gain versus thresholds for 5-year and 30-year notes. Top graph:
Small thresholds that vary as follows: 0.5*tick, tick, 2*tick, 3*tick, etc. (up to 29*tick).
Bottom graph: Large thresholds that vary as follows: 50*tick, 2*50*tick, 3*50*tick, etc.
(up to 20*50*tick).
\n\n=== PAGE 55 ===\nFIGURE 1.11 Number of subperiods versus thresholds for 5-year and 30-year notes.
Top graph: Small thresholds that vary as follows: 0.5*tick, tick, 2*tick, 3*tick, etc. (up to
29*tick). Bottom graph: Large thresholds that vary as follows 50*tick, 2*50*tick,
3*50*tick, etc. (up to 20*50*tick).
Table 1.1 5-year 08/02/2011 note. Tick size is M = 0.0078125, the number of ticks is N
= 17,074. Each threshold h in column 1 is actually hM.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
0.5
488
1468
0.001
23.340
3.008 33.290
0.385
1
412
1259
0.003
28.488
3.056 31.258
1.074
2
331
1030
0.002
36.332
3.112 29.565
0.637
3
278
864
0.001
43.209
3.108 29.647
0.279
4
241
744
0.001
50.440
3.087 28.804
0.147
5
210
662
0.002
58.895
3.152 27.562
0.389
6
197
609
0.001
60.675
3.091 29.993
0.217
7
187
575
0.001
63.588
3.075 30.356
0.256
\n\n=== OCR PAGE 55 ===\n‘Num. of subperiods vs. threshold

‘Subpetiods

° 5 10 18 20

FIGURE 1.11 Number of subperiods versus thresholds for 5-year and 30-year notes.
Top graph: Small thresholds that vary as follows: 0.5*tick, tick, 2*tick, 3*tick, etc. (up to
29*tick). Bottom graph: Large thresholds that vary as follows 50*tick, 2*50*tick,
3*50*tick, etc. (up to 20*50*tick).

Table 1.1 5-year 08/02/2011 note. Tick size is M = 0.0078125, the number of ticks is N
= 17,074. Each threshold h in column 1 is actually hM.

# # Average Average Average # %of Total
gain _subperiod of signals
Threshold subperiods signals per length per idle gain
subperiod subperiod time

0.5 488 1468 0.001 23.340 3.008 33.290 0.385
1 412 1259 0.003 28.488 3.056 31.258 1.074
2 331 1030 0.002 36.332 3.112 29.565 0.637
3 278 864 0.001 43.209 3.108 29.647 0.279
4 241 744 0.001 50.440 3.087 28.804 0.147
5 210 662 0.002 58.895 3.152 27.562 0.389
6 197 609 0.001 60.675 3.091 29.993 0.217
7 187 575 0.001 63.588 3.075 30.356 0.256
\n\n=== PAGE 56 ===\n#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
8
178
540
0.000
65.157
3.034 32.072
0.054
9
158
500
0.002
74.911
3.165 30.678
0.304
10
155
484
0.001
76.729
3.123 30.344
0.093
50
70
197
−0.004
154.271
2.814 36.752 −0.249
100
44
131
−0.013
245.205
2.977 36.810 −0.577
150
38
111
−0.019
287.921
2.921 35.920 −0.718
200
29
79
−0.010
356.862
2.724 39.387 −0.297
250
27
76
−0.006
397.074
2.815 37.209 −0.157
Table 1.2 30-year 07/29/2011 note. Tick size is M = 0.015625, the number of ticks is N
= 4588. Each threshold h in column 1 is actually hM.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
0.5
467
1334
−0.004
6.503
2.857 33.806 −1.865
1
335
988
−0.007
8.931
2.949 34.786 −2.191
2
233
681
−0.005
13.309
2.923 32.411 −1.142
3
192
565
0.002
15.901
2.943 33.457
0.311
4
174
504
0.002
17.632
2.897 33.130
0.282
5
146
430
0.001
20.651
2.945 34.285
0.218
6
127
375
0.005
24.055
2.953 33.413
0.624
7
123
360
0.002
24.894
2.927 33.261
0.204
8
113
331
−0.009
26.186
2.929 35.506 −1.062
9
104
306
−0.014
28.471
2.942 35.462 −1.436
10
103
302
−0.015
28.573
2.932 35.854 −1.530
50
34
115
0.057
97.676
3.382 27.616
1.937
100
23
75
0.060
145.391
3.261 27.114
1.375
150
18
59
0.070
180.778
3.278 29.076
1.266
200
13
50
0.242
278.077
3.846 21.207
3.140
250
14
47
0.242
229.714
3.357 29.904
3.390
\n\n=== PAGE 57 ===\nTable 1.3 30-year 08/01/2011 bond. Tick size is M = 0.015625, the number of ticks is N
= 3244. Each threshold h in column 1 is actually hM.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
0.5
358
1036
−0.003
6.095
2.894 32.737 −1.100
1
262
792
0.007
8.531
3.023 31.104
1.839
2
195
582
−0.002
11.031
2.985 33.693 −0.392
3
136
457
0.038
17.669
3.360 25.925
5.140
4
115
393
0.041
19.826
3.417 29.716
4.750
5
103
356
0.045
22.136
3.456 29.716
4.656
6
95
324
0.038
23.579
3.411 30.949
3.655
7
81
291
0.055
28.815
3.593 28.052
4.437
8
76
268
0.057
30.500
3.526 28.545
4.344
9
76
254
0.051
30.039
3.342 29.624
3.844
10
71
251
0.074
32.338
3.535 29.223
5.280
50
33
111
0.042
76.818
3.364 21.856
1.375
100
21
70
0.352
112.333
3.333 27.281
7.390
150
18
57
0.065
114.500
3.167 36.467
1.172
200
16
52
0.294
142.312
3.250 29.809
4.703
250
16
46
0.149
139.750
2.875 31.073
2.391
\n\n=== PAGE 58 ===\nTable 1.4 30-year 08/02/2011 bond. Tick size is M = 0.015625, the number of ticks is N
= 4349. Each threshold h in column 1 is actually hM.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
0.5
430
1291
0.009
6.802
3.002 32.743 3.901
1
316
982
0.022
9.497
3.108 30.996 6.888
2
218
704
0.011
13.298
3.229 33.341 2.421
3
185
604
0.012
16.411
3.265 30.191 2.264
4
146
509
0.016
20.856
3.486 29.984 2.404
5
126
444
0.025
24.397
3.524 29.317 3.186
6
110
393
0.032
28.391
3.573 28.190 3.531
7
112
382
0.031
26.804
3.411 30.973 3.468
8
104
354
0.046
28.577
3.404 31.662 4.829
9
93
321
0.028
33.269
3.452 28.857 2.609
10
89
298
0.022
32.921
3.348 32.628 1.967
50
38
129
0.138
80.763
3.395 29.432 5.233
100
29
90
0.176
105.207
3.103 29.846 5.110
150
24
75
0.105
128.375
3.125 29.156 2.516
200
19
61
0.133
179.895
3.211 21.407 2.531
250
17
54
0.072
178.706
3.176 30.145 1.219
\n\n=== PAGE 59 ===\nTable 1.5 30-year 08/03/2011 note. Tick size is M = 0.015625, the number of ticks is N
= 5153. Each threshold h in column 1 is actually hM.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
0.5
549
1674
0.010
6.342
3.049 32.428 5.556
1
425
1311
0.014
8.169
3.085 32.622 6.058
2
276
900
0.028
12.641
3.261 32.292 7.827
3
233
741
0.020
14.670
3.180 33.670 4.654
4
198
634
0.021
17.662
3.202 32.137 4.232
5
180
576
0.021
19.444
3.200 32.078 3.810
6
169
532
0.020
20.604
3.148 32.428 3.325
7
160
507
0.033
22.331
3.169 30.662 5.311
8
139
457
0.030
25.612
3.288 30.914 4.141
9
134
438
0.037
26.560
3.269 30.933 4.953
10
127
414
0.041
28.307
3.260 30.235 5.187
50
52
172
0.038
68.404
3.308 30.972 1.953
100
39
125
0.041
92.282
3.205 30.157 1.609
150
26
92
0.192
150.077
3.538 24.277 5.001
200
21
73
0.161
185.571
3.476 24.374 3.391
250
20
68
0.253
194.600
3.400 24.471 5.063
\n\n=== PAGE 60 ===\n(1.59)
Table 1.6 Average gains and lengths of subperiods, asymmetric random walk. P(−3) =
0.00012, P(−2) = 0.00141, P(−1) = 0.05348, P(0) = 0.88619, P(1) = 0.05670, P(2) =
0.00182, and P(3) = 0.00029, for various thresholds. These probabilities were computed
from the 5-year 08/02/2011 bond. Tick size is 1, the number of ticks is 5000 and starting
price 10,000. The number of simulations is 1000.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
1
162.401 488.191
0.030
20.648
3.006 32.936
4.9
2
125.799 378.175
0.036
26.665
3.006 32.913
4.5
3
103.358 310.196
0.041
32.508
3.001 32.801
4.3
4
88.207 264.402
0.034
38.121
2.998 32.749
3.0
5
77.574 232.413
0.037
43.390
2.996 32.681
2.9
6
69.419 208.576
0.052
48.530
3.005 32.623
3.6
7
63.469 190.459
0.066
53.123
3.001 32.566
4.2
8
58.775 176.465
0.077
57.408
3.002 32.517
4.5
9
54.911 164.795
0.072
61.455
3.001 32.509
4.0
10
51.568 154.781
0.062
65.533
3.001 32.411
3.2
11
48.860 146.595
0.077
69.171
3.000 32.406
3.8
12
46.511 139.607
0.078
72.754
3.002 32.323
3.6
13
44.562 133.657
0.085
75.995
2.999 32.270
3.8
14
42.796 128.334
0.108
79.217
2.999 32.197
4.6
15
41.267 123.685
0.084
82.236
2.997 32.127
3.5
100
14.414
43.619
0.588
245.445
3.026 29.243
8.5
200
9.628
29.632
1.451
381.445
3.078 26.549
14.0
300
7.636
23.615
2.119
489.755
3.093 25.205
16.2
400
6.477
20.074
2.487
588.740
3.099 23.735
16.1
500
5.672
17.819
3.057
687.449
3.142 22.016
17.3
The fifth columns of Tables 1.1–1.5 represent the average subperiod length E[Tα(1)(h)];
the sixth columns represent the average # of signals per subperiod E[Y1] + 1. Therefore,
the expected time to a signal can be found as
To see the square-root effect, let us examine the following rows:
In rows 2 and 5 of Table 1.2, we can calculate 
 and
 respectively. We now notice that 
.
\n\n=== OCR PAGE 60 ===\nTable 1.6 Average gains and lengths of subperiods, asymmetric random walk. P(-3) =
0.00012, P(—2) = 0.00141, P(—1) = 0.05348, P(0) = 0.88619, P(1) = 0.05670, P(2) =
0.00182, and P(3) = 0.00029, for various thresholds. These probabilities were computed
from the 5-year 08/02/2011 bond. Tick size is 1, the number of ticks is 5000 and starting
price 10,000. The number of simulations is 1000.

# # Average Average Average# %of Total
gain _subperiod of signals
Threshold subperiods signals per length per idle gain
subperiod subperiod time

1 162.401 488.191 0.030 20.648 3.006 32.936 4.9
2 125.799 378.175 0.036 26.665 3.006 32.913 4.5
3 103.358 310.196 0.041 32.508 3.001 32.801 4.3
4 88.207 264.402 0.034 38.121 2.998 32.749 3.0
5 77-574 232.413 0.037 43.390 2.996 32.681 2.9
6 69.419 208.576 0.052 48.530 3.005 32.623 3.6
7 63.469 190.459 0.066 53-123, 3.001 32.566 4.2
8 58.775 176.465 0.077 57.408 3.002 32.517 4.5
9 54.911 164.795 0.072 61.455 3.001 32.509 4.0
10 51.568 154.781 0.062 65.533 3.001 32.411 3.2
u 48.860 146.595 0.077 69.171 3.000 32.406 3.8
12 46.511 139.607 0.078 72.754 3.002 32.323 3.6
13 44.562 133.657 0.085 75-995 2.999 32.270 3.8
14 42.796 128.334 0.108 79.217 2.999 32.197 4.6
15 41.267 123.685 0.084 82.236 2.9907 32.127 3.5
100 14.414 43.619 0.588 245.445 3.026 29.243 8.5
200 9.628 29.632 1.451 381.445 3.078 26.549 14.0
300 7.636 23.615 2.119 489.755 3.093 25.205 16.2
400 6.477. 20.074 2.487 588.740 3.099 23.735 16.1
500 5.672 17.819 3.057 687.449 3.142 22.016 17.3

The fifth columns of Tables 1.1-1,5 represent the average subperiod length E[T,4(,)(h)];

the sixth columns represent the average # of signals per subperiod E[Y,] + 1. Therefore,

the expected time to a signal can be found as

ElT (A) (4.59)
E[Y

To see the square-root effect, let us examine the following rows:

E|T\| =

_ 8.931 _
¢ Inrows 2 and 5 of Table 1.2, we can calculate E(T\(D)) = To99 = 4-582 ond
E(T,(4)] = 1897 9.295, respectively. We now notice that £[7)(4)] © EIT, 4,
\n\n=== PAGE 61 ===\nIn row 4 of Table 1.2, we can calculate 
 and
.
In rows 2 and 8 of Table 1.3, we can calculate 
 and
 respectively. This leads to 
.
We have also generated simulated data for each of the bonds from which once again we
can easily decipher the same square-root effect. To be more specific, we have fitted a lazy
random walk model to the 30-year bond series data for 07/29/2011 and 08/02/2011
with the appropriate parameters as designated in the caption of Table 1.7. A simple
goodness-of-fit test demonstrates the validity of the model selected. The same process is
followed in the remaining 30-year bond data. The results of the simulations are
summarized in Tables 1.7 and 1.8, respectively. We again demonstrate the square-root
effect once again for the same thresholds used in the observed data:
In rows 1, 4, and 3 of Table 1.7, we can calculate 
,
 and 
. Once again we observe the
approximations 
 and 
 respectively.
In rows 1 and 7 of Table 1.8, we can calculate 
 and
, from which we can extract the approximation
.
\n\n=== OCR PAGE 61 ===\n15.901

¢ Inrow 4 of Table 1.2, we can calculate E(T,(3)] = 3 = 8.184 and
E(T\) = EIT).
« Inrows 2 and BOF Table 1.3, we can calculate EIT\(D] = 2023 — 4.217 and
EIT) = Fa5y = 13, respectively. This leads to E[7\(7)] * EIT, 7,
We have also generated simulated data for each of the bonds from which once again we

can easily decipher the same square-root effect. To be more specific, we have fitted a lazy

random wal

k model to the 30-year bond series data for 07/29/2011 and 08/02/2011

with the appropriate parameters as designated in the caption of Table 1.7. A simple

goodness-o!

-fit test demonstrates the validity of the model selected. The same process is

followed in the remaining 30-year bond data. The results of the simulations are

summarize:

in Tables 1.7 and 1.8, respectively. We again demonstrate the square-root

effect once again for the same thresholds used in the observed data:

¢ Inrows1, 4, and 3 of Table 1.7, we can calculate 2.01

E[T\(4)] =

= 3.749

E{T\(1)] =
13.879
2024 = 6.857 . Once again we observe the

oS = 8.12 ng EIT) =

approximations £[7(4)] * E[T( V4 ana E[T\(3)] © ET T\(1)1 V3. respectively.

« Inrows
E[T(7)] =

E[T\(7)]

— 686M _
1 rand Z, of Table 1.8, we can calculate EIT\(D] = To57 = 3.422 and

~ 1994 = 9.847 , from which we can extract the approximation

w E[T (V7,
\n\n=== PAGE 62 ===\nTable 1.7 Average gains and lengths of subperiods, asymmetric random walk. P(−4) =
0.00494, P(−3) = 0.00997, P(−2) = 0.03732, P(−1) = 0.12561, P(0) = 0.62919, P(1) =
0.12747, P(2) = 0.04575, and P(3) = 0.01279, P(4) = 0.00697, for various thresholds.
These probabilities were computed from the 30-year 07/29/2011 and 08/02/2011 bonds
after a best fit that was verified by chi-square test. The p-value for 07/29/2011 is 0.87
and for 08/02/2011, 0.815. Tick size is 1, the number of ticks is 5000, and the starting
price 10,000. The number of simulations is 1000.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
1
443.934 1336.262
0.083
7.535
3.010 33.099
37.0
2
299.194
903.318
0.161
11.208
3.019 32.935
48.3
3
241.959
731.714
0.240
13.879
3.024 32.838
58.0
4
204.308
618.324
0.289
16.452
3.026 32.776
59.1
5
181.312
549.742
0.352
18.563
3.032 32.684
63.8
6
164.086 498.442
0.421
20.548
3.038 32.569
69.1
7
150.539
457.415
0.453
22.410
3.039 32.529
68.2
8
139.256 423.863
0.507
24.255
3.044 32.446
70.6
9
130.517
397.679
0.549
25.909
3.047 32.369
71.6
10
122.775
374.570
0.617
27.547
3.051 32.358
75.8
11
115.983
354.361
0.683
29.171
3.055 32.333
79.2
12
110.395
337.981
0.775
30.676
3.062 32.269
85.6
13
105.205 322.336
0.848
32.226
3.064 32.194
89.2
14
100.997 309.365
0.902
33.602
3.063 32.127
91.1
15
96.980
297.191
0.927
34.964
3.064 32.183
89.9
100
30.065
96.746
5.522
117.792
3.218 29.172 166.0
200
19.133
63.974
10.603
190.220
3.344 27.210 202.9
300
14.566
50.358
16.365
258.656
3.457 24.648 238.4
400
11.936
42.703
22.771
323.669
3.578 22.734 271.8
500
10.217
37.682
28.222
388.293
3.688 20.656 288.3
\n\n=== PAGE 63 ===\nTable 1.8 Average gains and lengths of subperiods, asymmetric random walk. P(−4) =
0.01023, P(−3) = 0.01460, P(−2) = 0.04601, P(−1) = 0.13666, P(0) = 0.58916, P(1) =
0.12914, P(2) = 0.04548, P(3) = 0.01772, and P(4) = 0.01100, for various thresholds.
These probabilities were computed from the 30-year 08/01/2011 and 08/03/2011 bonds
after a best fit that was verified by chi-square test. The p-value for 08/01/2011 is 0.80
and for 08/03/2011, 0.48. Tick size is 1, the number of ticks is 5000 and starting price
10,000. The number of simulations is 1000.
#
#
Average
gain
Average
subperiod
Average #
of signals
% of
Total
Threshold subperiods signals
per
subperiod
length
per
subperiod
idle
time
gain
1
488.364 1463.672
−0.014
6.834
2.997 33.249
−7.0
2
333.138 998.589
−0.017
10.031
2.998 33.169
−5.6
3
271.557
813.925
−0.015
12.306
2.997 33.162
−4.0
4
229.944 688.463
−0.043
14.532
2.994 33.170
−9.9
5
204.761 613.066
−0.039
16.340
2.994 33.085
−7.9
6
185.382
555.241
−0.034
18.050
2.995 33.077
−6.4
7
170.343 509.950
−0.055
19.636
2.994 33.102
−9.4
8
157.385
471.073
−0.049
21.245
2.993 33.127
−7.7
9
147.463
441.897
−0.020
22.710
2.997 33.022
−3.0
10
138.931
416.129
−0.012
24.127
2.995 32.960
−1.6
11
131.472 393.089
−0.049
25.474
2.990 33.018
−6.5
12
125.161 374.424
−0.053
26.768
2.992 32.993
−6.6
13
119.276 356.992
−0.026
28.096
2.993 32.976
−3.0
14
114.317
341.780
−0.075
29.313
2.990 32.980
−8.5
15
109.788
328.314
−0.068
30.509
2.990 33.009
−7.5
100
35.056 104.634
0.031
96.983
2.985 32.003
1.1
200
22.908
68.150
−0.009
149.200
2.975 31.642
−0.2
300
17.932
53.188
−0.089
192.092
2.966 31.108
−1.6
400
14.932
44.283
−0.198
232.382
2.966 30.602
−3.0
500
13.070
38.673
−0.084
267.170
2.959 30.162
−1.1
The square-root effect suggests that increasing the threshold reduces the number of
complete subperiods R on any given trading day, and thus the number of transactions
completed therein. Thus, although varying the threshold does not have a systematic
effect on the gain in the absence of transaction costs, increasing the threshold would
decrease the number of transactions but increase the “riskiness” of the trading strategy.
A good measure of performance of the strategy over the course of an entire day of trading
is the total gain ∑R
l = 1Gl, which, under the zero transaction cost model with IID Gl, has
expected value of E(R)E(G1) by Wald’s equation. Examining this product as a function of
h under different probabilistic models of the asset price is of interest, especially in terms
of maximizing the day’s total gain based on the value of h. However, in the presence of
\n\n=== PAGE 64 ===\n(1.60)
transaction costs, Wald’s equation fails and analytical derivations are extremely
challenging. Besides, transaction costs often vary from firm to firm and thus the
appropriate choice of threshold will depend not only on the selection of a measure of
“riskiness” but also on the transaction costs related to the specific product or firm.
1.7 Conclusions and future work
In Figures 1.2, 1.3, 1.4, 1.5, and 1.6 of Section 1.3.3, it is shown that the proposed CUSUM
trading strategy performs well in subperiods of many signals of one sign before a signal
of the opposite sign occurs. This is also evident in Tables 1.3, 1.5, 1.7, 1.A.1, and 1.A.3
related to the results of the simulation in the random walk model of Section 1.4. Such
subperiods are characterized by consistent upward or downward trends in prices. On the
contrary, the proposed strategy is at a loss in the case of few signals of one sign followed
by a signal of the opposite sign. Such subperiods are characterized by stability in prices.
This observation suggests that the CUSUM trading strategy can be further improved by
an online detection of “regimes of stability” (as contrasted to “regimes of trends”). This
suggests the construction of new online algorithm possibly inspired by computer vision
(see, for instance, Hadjiliadis and Stamos [19] or Stamos et al. (30)).
Another statistic that is indicative of the contrast between times of stability versus times
of instability is known as the speed of reaction of the CUSUM, which measures the time
between the last reset to 0 of the CUSUM statistic process and the time of the CUSUM
alarm (see, for instance, [31]). We intend to examine both of these directions of research
in order to improve the performance of the proposed algorithm by limiting trading in
times of stability.
A parameter that should be investigated in depth is the transaction cost c. The form of
the gain over a subperiod given in (1.11) can be written as
This second term should be analyzed as a fixed percentage, and on a sliding scale
(considering, e.g., high-volume rebates). In addition, transaction costs should be
investigated via Monte Carlo simulation, as it requires knowledge of the liquidation price
of the asset for that subperiod.
A parameter closely related to the transaction cost is the threshold parameter h used in
the CUSUM timing. A smaller threshold implies more frequent transactions but
decreases the “riskiness” of the strategy on any given trading day. The optimal choice of
the threshold should thus be based on the trade-off between an appropriately chosen
measure of “risk” of the proposed strategy and the transaction costs in the market where
it is applied.
Moreover, it should be noted that the random walk examples included here are not
intended as actual asset price models (we do not intend to commit a Bachelierian
fallacy); these models are merely used to illustrate the strategy and allow for basic
calculations. In future work, it would be of interest to examine the best fit random walk
model to actual high-frequency asset data (taking into account such real-world
considerations as the bid-ask spread). Furthermore, open problems on this topic include
extending analysis of this strategy to other models of asset price motion—primarily,
\n\n=== OCR PAGE 64 ===\ntransaction costs, Wald’s equation fails and analytical derivations are extremely
challenging. Besides, transaction costs often vary from firm to firm and thus the
appropriate choice of threshold will depend not only on the selection of a measure of
“riskiness” but also on the transaction costs related to the specific product or firm.

1.7 Conclusions and future work

In Figures 1.2, 1.3, 1.4, 1.5, and 1.6 of Section 1.3.3, it is shown that the proposed CUSUM
trading strategy performs well in subperiods of many signals of one sign before a signal
of the opposite sign occurs. This is also evident in Tables 1.3, 1.5, 1.7, 1.A.1, and 1.A.3
related to the results of the simulation in the random walk model of Section 1.4. Such
subperiods are characterized by consistent upward or downward trends in prices. On the
contrary, the proposed strategy is at a loss in the case of few signals of one sign followed
by a signal of the opposite sign. Such subperiods are characterized by stability in prices.
This observation suggests that the CUSUM trading strategy can be further improved by
an online detection of “regimes of stability” (as contrasted to “regimes of trends”). This
suggests the construction of new online algorithm possibly inspired by computer vision
(see, for instance, Hadjiliadis and Stamos [19] or Stamos et al. (30)).

Another statistic that is indicative of the contrast between times of stability versus times
of instability is known as the speed of reaction of the CUSUM, which measures the time
between the last reset to o of the CUSUM statistic process and the time of the CUSUM
alarm (see, for instance, [31]). We intend to examine both of these directions of research
in order to improve the performance of the proposed algorithm by limiting trading in
times of stability.

A parameter that should be investigated in depth is the transaction cost c. The form of
the gain over a subperiod given in (1.11) can be written as

¥, ¥
G,=(-1) YZ rai te (De w.n - 25) :
a A

This second term should be analyzed as a fixed percentage, and on a sliding scale
(considering, e.g., high-volume rebates). In addition, transaction costs should be
investigated via Monte Carlo simulation, as it requires knowledge of the liquidation price
of the asset for that subperiod.

(1.60)

A parameter closely related to the transaction cost is the threshold parameter h used in
the CUSUM timing. A smaller threshold implies more frequent transactions but
decreases the “riskiness” of the strategy on any given trading day. The optimal choice of
the threshold should thus be based on the trade-off between an appropriately chosen
measure of “risk” of the proposed strategy and the transaction costs in the market where
it is applied.

Moreover, it should be noted that the random walk examples included here are not
intended as actual asset price models (we do not intend to commit a Bachelierian
fallacy); these models are merely used to illustrate the strategy and allow for basic
calculations. In future work, it would be of interest to examine the best fit random walk
model to actual high-frequency asset data (taking into account such real-world
considerations as the bid-ask spread). Furthermore, open problems on this topic include
extending analysis of this strategy to other models of asset price motion—primarily,
\n\n=== PAGE 65 ===\nbuilding a binomial model (of which our random walks are the simplest case) and
limiting to a continuous geometric Brownian motion. Note that our two sets of random
walks investigate different types of “time”: the p0 = 0 case investigates “tick time,” where
the clock moves only when the price moves, and the lazy walk, that is, p0 > 0, considers
clock time (since there may be samples where the price does not move). This simple
discrepancy induces extra possible paths into the CUSUM timing process. The general
binomial model, which may move a price multiple ticks per sample, and still retain the
probability of standing still, is certainly, then, of interest.
Finally, we wish to examine the CUSUM strategy with mu
k and md
k set to wait for
multiple ticks instead of one (e.g., 
 for some b > 1).
Appendix: Tables
In this section are the tables described in Section 1.5.
Table 1.A.1 Average gains and lengths of subperiods, lazy simple symmetric random
walk.
Average
total
Average
gain
Average
subperiod
Average #
signals
Average #
Average
%
p1
gain
per
subperiod
length
per
subperiod
subperiods
idle
time
0.000
−0.864
−0.002
7.995
2.998
417.472
33.2
0.050
−5.379
−0.013
7.824
2.999
426.918
33.2
0.100
−3.204
−0.007
7.670
2.998
435.185
33.2
0.150
1.720
0.004
7.579
3.001
440.642
33.2
0.200
4.417
0.010
7.505
3.002
445.020
33.2
0.250
7.205
0.016
7.492
3.003
446.076
33.2
0.300
−3.728
−0.008
7.465
2.996
446.920
33.3
0.350
−0.144
−0.000
7.527
3.000
443.581
33.2
\n\n=== OCR PAGE 65 ===\nbuilding a binomial model (of which our random walks are the simplest case) and
limiting to a continuous geometric Brownian motion. Note that our two sets of random
walks investigate different types of “time”: the py = 0 case investigates “tick time,” where
the clock moves only when the price moves, and the lazy walk, that is, py > 0, considers
clock time (since there may be samples where the price does not move). This simple
discrepancy induces extra possible paths into the CUSUM timing process. The general
binomial model, which may move a price multiple ticks per sample, and still retain the
probability of standing still, is certainly, then, of interest.

Finally, we wish to examine the CUSUM strategy with m", and m%, set to wait for

m= Sp, 4+ mM

multiple ticks instead of one (e.g., 2 for some b > 1).

Appendix: Tables
In this section are the tables described in Section 1.5.

Table 1.A.1 Average gains and lengths of subperiods, lazy simple symmetric random
walk.

Average Average Average Average # Average # Average
total gain subperiod signals %

Pi gain per length per subperiods idle

subperiod subperiod time
0.000 -0.864 -0.002 7.995 2.998 417.472 33.2
0.050 -5.379 -0.013 7.824 2.999 426.918 33.2
0.100 -3.204 -0.007 7.670 2.998 435.185 33.2
0.150 1.720 0.004 7.579 3.001 440.642 33.2
0.200 4.417 0.010 7.505 3.002 445.020 33.2
0.250 7.205 0.016 7.492 3.003 446.076 33.2
0.300 -3.728 -0.008 7.465 2.996 446.920 33.3

0.350 -0.144 -0.000 7.527 3.000 443.581 33.2
\n\n=== PAGE 66 ===\nTable 1.A.2 Number of subperiods of a specified length, and average gain over those
subperiods, lazy simple symmetric random walk.
p0
0.000 0.050 0.100
0.150
0.200 0.250 0.300 0.350
# 1–subperiods
209.05
213.37
218.10
220.11
222.32 223.14 223.83 222.24
average gain, 1–
subperiod
−1.99
−1.90
−1.81
−1.73
−1.66
−1.60
−1.53
−1.48
# 2–subperiods
104.61 106.94 108.50
110.43
111.67
111.18
111.86
110.48
average gain, 2–
subperiod
−1.99
−1.90
−1.81
−1.74
−1.66
−1.59
−1.53
−1.48
# 3–subperiods
51.87
53.50
54.30
54.94
55.27
55.98
56.05
55.32
average gain, 3–
subperiod
0.01
0.01
0.01
0.01
−0.01
0.01
0.01
0.02
# 4–subperiods
26.12
26.60
27.31
27.88
27.95
27.92
27.73
27.80
average gain, 4–
subperiod
4.01
3.82
3.64
3.50
3.36
3.23
3.09
2.97
# 5–subperiods
12.81
13.33
13.50
13.64
13.91
13.80
13.62
13.99
average gain, 5–
subperiod
10.02
9.55
9.11
8.80
8.36
7.99
7.74
7.44
# 6–subperiods
6.49
6.66
6.79
6.86
6.97
6.88
6.94
6.87
average gain, 6–
subperiod
18.01
17.26
16.47
15.71
14.99
14.44
13.87
13.26
# 7–subperiods
3.30
3.33
3.36
3.38
3.40
3.67
3.48
3.45
average gain, 7–
subperiod
28.03
26.55
25.50
24.37
23.35
22.48
21.71
20.72
# 8–subperiods
1.58
1.60
1.67
1.65
1.83
1.77
1.68
1.74
average gain, 8–
subperiod
40.04
38.00
36.13
34.79
33.05
31.75
30.97
29.57
# 9–subperiods
0.81
0.76
0.86
0.88
0.85
0.86
0.84
0.83
average gain, 9–
subperiod
54.03
51.29
48.82
47.12
45.01
43.09
41.67
39.80
# 10 + -subperiods
0.83
0.82
0.81
0.88
0.87
0.88
0.89
0.85
average gain, 10 + -
subperiods
2091.33 1393.27 1852.19 1524.21 1230.49 1715.07 1220.75 1318.50
\n\n=== PAGE 67 ===\nTable 1.A.3 Average gains and lengths of subperiods, asymmetric random walk. p0 =
0.0.
Average
total
Average
gain
Average
subperiod
Average #
signals
Average #
Average
%
p1
gain
per
subperiod
length
per
subperiod
subperiods
idle
time
0.500
−1.005
−0.002
8.014
3.001
417.140
33.1
0.550
414.950
1.039
8.572
3.165
399.230
31.6
0.600
1,818.090
5.170
10.413
3.705
351.630
26.8
0.650
4,718.815
16.378
13.749
4.746
288.120
20.8
0.700
10,597.510
48.681
19.612
6.667
217.695
14.6
0.750
22,358.635
144.829
29.404
10.181
154.380
9.2
0.800
47,017.940
468.563
47.264
17.080
100.345
5.1
\n\n=== PAGE 68 ===\nTable 1.A.4 Number of subperiods of a specified length, and average gain over those
runs, asymmetric random walk. p0 = 0.0.
p1
0.500
0.550
0.600
0.650
0.700
0.750
0.800
# 1–
subperiods
209.12
192.44
149.00
100.33
56.83
27.99
11.00
average gain,
1–subperiod
−1.99
−1.99
−1.99
−1.99
−1.99
−1.99
−1.98
# 2–
subperiods
103.48
95.11
75.41
50.27
29.31
13.47
5.46
average gain,
2–subperiod
−1.99
−2.00
−1.99
−1.99
−1.98
−1.96
−1.94
# 3–
subperiods
51.97
49.91
43.08
32.50
20.64
11.08
5.09
average gain,
3–subperiod
0.01
0.01
0.01
0.01
0.03
0.03
0.06
# 4–
subperiods
26.20
26.66
27.11
24.35
17.00
10.82
4.56
average gain,
4–run
4.02
4.02
4.02
4.03
4.03
4.08
4.13
# 5–runs
13.76
14.95
17.92
17.76
14.63
9.12
4.30
average gain,
5–run
10.02
10.02
10.02
10.02
10.03
10.08
10.13
# 6–runs
6.18
8.39
11.95
14.38
12.02
8.35
4.28
average gain,
6–run
18.02
18.03
18.04
18.05
18.07
18.13
18.22
# 7–runs
3.29
4.87
8.50
10.80
10.24
7.50
3.98
average gain,
7–run
28.01
28.03
28.00
28.05
28.12
28.10
28.21
# 8–runs
1.70
2.85
5.97
8.47
8.98
6.49
3.79
average gain,
8–run
40.02
40.00
40.03
40.01
40.08
40.08
40.17
# 9–runs
0.69
1.59
3.98
6.70
7.41
5.95
3.31
average gain,
9–run
54.00
54.00
54.05
54.11
54.07
54.14
54.19
# 10 + -runs
0.75
2.44
8.73
22.56
40.63
53.60
54.59
average gain,
10 + -runs
1356.00 2688.00 5096.28 18,888.69 68,970.76 198,829.18 63,3281.20
\n\n=== PAGE 69 ===\nTable 1.A.5 Average gains and lengths of runs, asymmetric random walk. p0 = 0.1.
Average
total
Average
gain
Average
subperiod
Average #
signals
Average #
Average
%
p1
gain
per
subperiod
length
per
subperiod
subperiods
idle
time
0.450
7.485
0.017
7.683
3.005
434.700
33.2
0.500
428.825
1.032
8.251
3.173
415.625
31.4
0.550
1,825.045
4.979
9.972
3.705
366.535
26.9
0.600
4,729.605
15.804
13.254
4.767
299.270
20.7
0.650
10,741.020
47.569
18.924
6.727
225.800
14.5
0.700
22,858.345
144.381
28.694
10.364
158.320
9.1
0.750
50,243.475
504.478
47.822
17.994
99.595
4.7
Table 1.A.6 Number of subperiods of a specified length, and average gain over those
runs, asymmetric random walk. p0 = 0.1.
p1
0.450 0.500
0.550
0.600
0.650
0.700
0.750
# 1-runs
217.16
199.44
156.22
104.08
58.37
27.67
10.54
avg gain, 1-run
−1.81
−1.81
−1.80
−1.80
−1.78
−1.77
−1.74
# 2-runs
108.08
99.44
78.25
51.39
29.41
14.06
5.47
avg gain, 2-run
−1.81
−1.79
−1.76
−1.69
−1.63
−1.51
−1.34
# 3-runs
54.92
51.84
44.15
34.02
21.74
11.57
4.70
avg gain, 3-run
0.01
0.04
0.14
0.31
0.47
0.64
0.94
# 4-runs
27.02
28.05
28.40
25.00
18.00
10.38
4.34
avg gain, 4-run
3.61
3.73
3.88
4.12
4.29
4.58
4.96
# 5-runs
14.05
15.66
18.58
19.49
15.38
9.06
4.29
avg gain, 5-run
9.02
9.27
9.48
9.82
10.07
10.51
11.06
# 6-runs
6.57
8.90
12.41
14.54
13.19
8.81
4.07
avg gain, 6-run
16.28
16.64
16.96
17.45
17.66
18.25
18.87
# 7-runs
3.58
4.81
8.80
11.25
10.62
7.71
3.69
avg gain, 7-run
25.49
25.62
26.25
26.72
27.38
27.79
28.72
# 8-runs
1.64
2.96
6.07
8.87
8.64
6.85
3.26
avg gain, 8-run
36.23
36.95
37.57
37.91
38.84
39.49
40.15
# 9-runs
0.85
1.78
4.13
6.91
7.64
6.20
3.40
avg gain, 9-run
49.42
50.21
50.84
50.91
51.87
52.39
53.87
# 10 + -runs
0.83
2.73
9.52
23.73
42.81
56.00
55.85
avg gain, 10 + -
runs
1119.42 2305.77 6948.10 15,781.54 61,048.80 178,378.58 711,759.83
\n\n=== PAGE 70 ===\nReferences
1. Abramov, V., Khan, M. K., and Khan, R. A., A probabilistic analysis of the trading the
line strategy, Quantitative Finance, Vol. 8, No. 5, pp. 499–512, 2008.
2. Alexander, S. S., Price movements in speculative markets: trends or random walk,
Industrial Management Review, Vol. 2, No. 2, pp. 7–26, 1961.
3. Alexander, S. S., Price movements in speculative markets: trends or random walk no.
2, Industrial Management Review, Vol. 5, No. 2, pp. 338–372, 1964.
4. Barnard, G., Control charts and stochastic processes, Journal of the Royal Statistical
Society B, Vol. 11, No. 1, pp. 239–271, 1959.
5. Basseville, M., and Nikiforov, I., Detection of abrupt changes: Theory and
applications, 1993 (Prentice Hall, Englewood Cliffs, NJ).
6. Beibel, M., A note on Ritov’s Bayes approach to the minimax property of the CUSUM
procedure, Annals of Statistics, Vol. 24, No. 2, pp. 1804–1812, 1996.
7. Bozdog, B., Florescu, I., Khashanah K., and Wang, J., A study of persistence in price
movement using high-frequency financial data. In Handbook of modeling high-
frequency data in finance, edited by F. Viens, M. Mariani, and I. Florescu, pp. 27–46,
2012 (John Wiley & Sons, Hoboken, NJ).
8. Brodsky, B. E., and Darkhovsky, B. K., Nonparametric Methods in Change-Point
Problems (Kluwer, Dordrecht), 1993.
9. Broemling, L. D., and Tsurumi, H., Econometrics and Structural Change, 1987
(Marcel Dekker, New York).
10. Cohen, A., Biomedical Signal Processing, 1987 (CRC Press, Boca Raton, FL).
11. Devore, J., Probability and Statistics for Engineering and the Science, 8th edition,
2012 (Brooks/Cole, Boston, MA).
12. Fama, B., and Blum, M., Filter rules and stock market trading, Journal of Business,
Vol. 40, pp. 226–241, 1966.
13. Figueroa-Lopez, J. E., Lancette, S., Lee, K., and Yanhun, M., Estimation of NIG and
VG models for high-frequency financial data. In Handbook of modeling high-
frequency data in finance, edited by F. Viens, M. Mariani, and I. Florescu, pp. 3–26,
2012 (John Wiley & Sons, Hoboken, NJ).
14. Frisen, M., (Ed.), Financial surveillance, 2008 (John Wiley and Sons, Hoboken, NJ).
15. Glynn, P. W., and Iglehart, D. L., Trading securities using trailing stops, Management
Science, Vol. 41, No. 6, pp. 1096–1106, 1995.
16. Hadjiliadis, O., and Moustakides, G. V., Optimal and asymptotically optimal CUSUM
rules for change point detection in the Brownian motion model with multiple
alternatives, Theory of Probability and Its Applications, Vol. 50, No. 1, pp. 131–144,
2006.
17. Hadjiliadis, O., Optimality of the 2-CUSUM drift equalizer rules for detecting two-
sided alternatives in the Brownian motion model, Journal of Applied Probability,
Vol. 42, No. 4, pp. 1183–1193, 2005.
18. Hadjiliadis, O., Hernandez-del-Valle, G., and Stamos, I., A comparison of 2-CUSUM
stopping rules for quickest detection of two-sided alternatives in a Brownian motion
model, Journal of Sequential Analysis, Vol. 28, No. 1, pp. 92–114, 2009.
19. Hadjiliadis, O., and Stamos, I., Sequential classification in point clouds of urban
scenes, In Proceedings of 5th International Symposium on 3D Data Processing
Visualization and Transmission, Paris, France, May 17–20, 2010.
20. Lam, K., and Yam, H. C. CUSUM techniques for technical trading in financial
markets, Financial Engineering and the Japanese Markets, Vol. 4, pp. 257–274,
\n\n=== PAGE 71 ===\n1997.
21. Lorden, G., Procedures for reacting to a change in distribution, Annals of
Mathematical Statistics, Vol. 42, No. 6, pp. 1897–1908, 1971.
22. Moustakides, G. V., Optimal stopping times for detecting changes in distributions,
Annals of Statistics, Vol. 14, No. 4, pp. 1379–1387, 1986.
23. Moustakides, G. V., Optimality of the CUSUM procedure in continuous time, Annals
of Statistics, Vol. 32, No. 1, pp. 302–315, 2004.
24. Page, E. S., Continuous inspection schemes, Biometrika, Vol. 41, Nos. 12, pp. 100–
115, 1954.
25. Poor, H. V., and Hadjiliadis, O., Quickest Detection, 2008 (Cambridge University
Press, Cambridge, UK).
26. Shiryaev, A. N., On optimum methods in quickest detection problems, Theory of
Probability and Its Applications, Vol. 8, No. 1, pp. 22–46, 1963.
27. Shiryaev, A. N., Minimax optimality of the method of cumulative sums (CUSUM) in
the continuous case, Russian Mathematical Surveys, Vol. 51, No. 4, pp. 750–751,
1996.
28. Siegmund, D., Sequential Analysis, 1985 (Springer-Verlag, New York).
29. Sonesson, C., and Bock, D., A review and discussion of prospective statistical
surveillance in public health, Journal of the Royal Statistical Society: Series A, Vol.
166, No. 1, pp. 5–21, 2003.
30. Stamos, I., Hadjiliadis, O., Zhang, H., and Flynn, T., Online algorithms in the
classification of urban objects in 3D point clouds, Proceedings of the 2nd 3D Imaging
Processing Visualization and Transmission Conference, Zurich, Switzerland,
October 13–15, 2012.
31. Zhang, H., and Hadjiliadis, O., Drawdowns and the speed of a market crash,
Methodology and Computing in Applied Probability, Vol. 14, No. 3, pp. 739–752,
2012.
\n\n=== PAGE 72 ===\nChapter Two
Gaussian Inequalities and Tranche
Sensitivities
Claas Becker1 and Ambar N. Sengupta2
1Studiengang Angewandte Mathematik, Hochschule RheinMain,
65197 Wiesbaden, Germany
2Department of Mathematics, Louisiana State University, Baton
Rouge, LA, USA
2.1 Introduction
With the CDO (collateralized debt obligation) market picking up
[16], it is important to build a stronger understanding of pricing and
risk management models. The role of the Gaussian copula model,
whose importance grew after the work of Li [11], has well-known
deficiencies and has been criticized in the technical literature (e.g.,
Hull and White [8]) as well as in the popular press, but it continues
to be fundamental as a starter model. In this chapter, we draw
attention to the applicability of Gaussian inequalities in analyzing
tranche loss sensitivity to correlation parameters for the Gaussian
copula model. Some of these methods are also applicable to other
models. Detailed proofs and more general results, applicable to non-
Gaussian copulas, are available in our longer paper [2].
We work with an 
-valued Gaussian random variable X = (X1, …,
XN), where each Xj is normalized to mean 0 and variance 1, and
study the equity tranche loss
where l1, …, lN > 0, a > 0, and 
 are parameters. Our
results establish an identity between the sensitivity of 
 to the
\n\n=== OCR PAGE 72 ===\nChapter Two
Gaussian Inequalities and Tranche
Sensitivities

Claas Becker! and Ambar N. Sengupta?

1Studiengang Angewandte Mathematik, Hochschule RheinMain,
65197 Wiesbaden, Germany

2Department of Mathematics, Louisiana State University, Baton
Rouge, LA, USA

2.1 Introduction

With the CDO (collateralized debt obligation) market picking up
[16], it is important to build a stronger understanding of pricing and
risk management models. The role of the Gaussian copula model,
whose importance grew after the work of Li [11], has well-known
deficiencies and has been criticized in the technical literature (e.g.,
Hull and White [8]) as well as in the popular press, but it continues
to be fundamental as a starter model. In this chapter, we draw
attention to the applicability of Gaussian inequalities in analyzing
tranche loss sensitivity to correlation parameters for the Gaussian
copula model. Some of these methods are also applicable to other
models. Detailed proofs and more general results, applicable to non-
Gaussian copulas, are available in our longer paper [2].

We work with an R-valued Gaussian random variable X = (X, ...,
Xy), where each X; is normalized to mean 0 and variance 1, and
study the equity tranche loss

N N
Lio) = ¥ Llixy<eu] _ { Li Ky <n] _ at ,
1 +

m=1 m=

where L,, ..., ly > 0, @ > 0, and 1; +++ >¢n © R are parameters. Our
results establish an identity between the sensitivity of ElLo.01] to the
\n\n=== PAGE 73 ===\n(2.1)
(2.2)
correlations 
 and the parameters cj and ck. From this, we
also prove (in [2]) the inequality
Applying this inequality to a CDO containing N names whose default
behavior is governed by the Gaussian variables Xj shows that an
increase in name-to-name correlation decreases expected loss in an
equity tranche. This is a generalization of the well-known result for
Gaussian copulas with uniform correlation.
Our approach is in the spirit of Slepian inequalities (Slepian [15])
and our goal here is to bring attention to such Gaussian inequalities
in the context of CDO loss behavior. Narrower forms of the
inequality (2.1) were the focus of the papers by Hillebrand et al. [6, 7]
and Meng and Sengupta [12]. Other related works include Cousin
and Laurent [3], who use a general theory of stochastic orders (see
Müller and Stoyan [13]). Jarrow and van Deventer [9] explore
dependence of the risk of the equity tranche on correlation under
various scenarios and different models. Ağça and Islam [1] explain
how an increase in correlation can impact market-implied default
probabilities, potentially increasing tranche losses.
2.2 The tranche loss function
Consider a CDO consisting of N names, with τj denoting the
(random) default time of the j-th name. Let
where Fj is the distribution function of τj (relative to the market
pricing measure), assumed to be continuous and strictly increasing,
and Φj is the standard Gaussian distribution function. Then for any
 we have
\n\n=== OCR PAGE 73 ===\ncorrelations “i = E[X)X«] and the parameters c; and c,. From this, we
also prove (in [2]) the inequality

a E[Lio.«q]

(2.1),

Applying this inequality to a CDO containing N names whose default
behavior is governed by the Gaussian variables X; shows that an
increase in name-to-name correlation decreases expected loss in an
equity tranche. This is a generalization of the well-known result for
Gaussian copulas with uniform correlation.

Our approach is in the spirit of Slepian inequalities (Slepian [15])
and our goal here is to bring attention to such Gaussian inequalities
in the context of CDO loss behavior. Narrower forms of the
inequality (2.1) were the focus of the papers by Hillebrand et al. [6, 7]
and Meng and Sengupta [12]. Other related works include Cousin
and Laurent [3], who use a general theory of stochastic orders (see
Miiller and Stoyan [13]). Jarrow and van Deventer [9] explore
dependence of the risk of the equity tranche on correlation under
various scenarios and different models. Agca and Islam [1] explain
how an increase in correlation can impact market-implied default
probabilities, potentially increasing tranche losses.

2.2 The tranche loss function

Consider a CDO consisting of N names, with 1; denoting the
(random) default time of the j-th name. Let

_o-!
Xx; = ® (Fj(z;)), (2.2)

where F; is the distribution function of 1; (relative to the market

pricing measure), assumed to be continuous and strictly increasing,
and 9; is the standard Gaussian distribution function. Then for any
x € Rwe have
\n\n=== PAGE 74 ===\n(2.3)
(2.4)
(2.5)
(2.6)
(2.7)
(2.8)
which means that Xj has standard Gaussian distribution. The
Gaussian copula model posits that the joint distribution of the Xj is
Gaussian; thus,
is an 
-valued Gaussian variable whose marginals are all standard
Gaussian. The correlation
reflects the default correlation between the names j and k. Now let
be the probability that the j-th name defaults within a time horizon T
(which we hold fixed through the entire analysis), and
is the default threshold of the j-th name.
In the schematic model, we use to explore the essential phenomena,
the default of name j, which happens if the default time τj is within
the time horizon T, results in a loss of amount lj > 0 in the CDO
portfolio. Thus, the total loss during the time period [0, T] is
In this simplified model, we are essentially working with a one-
period CDO and are ignoring discounting from the random time of
actual default; these are fairly standard assumptions for a first
analysis.
A tranche is simply a range of loss for the portfolio; it is specified by
a closed interval [a, b] with 0 ≤ a ≤ b. If the loss x is less than a, then
\n\n=== OCR PAGE 74 ===\nP[X,<x]=P [« < Fr '(@)| = F(R; '(@ 0) =x), (3)

which means that X; has standard Gaussian distribution. The
Gaussian copula model posits that the joint distribution of the X; is
Gaussian; thus,

= (X,,...,Xy) (2.4)

is an RY-valued Gaussian variable whose marginals are all standard
Gaussian. The correlation

Tik = E[X}X;] (2.5)

reflects the default correlation between the names j and k. Now let
P= E[t; <T|= P[X; < cl. (2.6)

be the probability that the j-th name defaults within a time horizon T
(which we hold fixed through the entire analysis), and

= ©7'(F(T)) (2.7)
is the default threshold of the j-th name.

In the schematic model, we use to explore the essential phenomena,
the default of name j, which happens if the default time 1; is within
the time horizon T, results in a loss of amount J; > o in the CDO

portfolio. Thus, the total loss during the time period [0, T] is

m=

In this simplified model, we are essentially working with a one-
period CDO and are ignoring discounting from the random time of
actual default; these are fairly standard assumptions for a first
analysis.

(2.8)

A tranche is simply a range of loss for the portfolio; it is specified by
a closed interval [a, b] with 0 < a < b. If the loss x is less than a, then
\n\n=== PAGE 75 ===\n(2.9)
(2.10)
(2.11)
(2.12)
this tranche is unaffected, whereas if x ≥ b then the entire tranche
value b − a is eaten up by loss; in between, if a ≤ x ≤ b, the loss to the
tranche is x − a. Thus, the tranche loss function t[a, b]is given by
We can rewrite this more compactly as
From this, it is clear that t[a, b](x) is continuous in (a, b, x), and from
(2.9) we see that it is a non-decreasing function of x. Thus, the loss in
an equity tranche [0, a] is given by
where a > 0.
2.3 A sensitivity identity
Gaussian inequalities were pioneered by Slepian [15] in the context
of extrema of Gaussian processes. Such inequalities have been
developed further by many researchers, such as Joag-Dev et al. [10].
Some of these inequalities are based on the following remarkable
identity, first discovered by Plackett [14, equation (3)], for the
density of a correlated centered multi dimensional Gaussian
variable:
Proposition 2.1 Let Q(R, x) be the Gaussian density
where R is a d × d strictly positive-definite matrix R = [rjk] and
. Then
\n\n=== OCR PAGE 75 ===\nthis tranche is unaffected, whereas if x = b then the entire tranche
value b - a is eaten up by loss; in between, if a < x < b, the loss to the
tranche is x — a. Thus, the tranche loss function tq, pis given by

0 ifx <a; (2.9),
tap) = yx-a_ ifx € [a,b];
b-a ifx>b.
We can rewrite this more compactly as
thap\(X) = (X— a), — (Xb), (2.10)

From this, it is clear that tq, ,)(x) is continuous in (a, b, x), and from
(2.9) we see that it is a non-decreasing function of x. Thus, the loss in
an equity tranche [0, a] is given by

toa(L) =L—-(L—a),. (2.11),

where a > 0.

2.3 A sensitivity identity

Gaussian inequalities were pioneered by Slepian [15] in the context
of extrema of Gaussian processes. Such inequalities have been
developed further by many researchers, such as Joag-Dev et al. [10].
Some of these inequalities are based on the following remarkable
identity, first discovered by Plackett [14, equation (3)], for the
density of a correlated centered multi dimensional Gaussian
variable:

Proposition 2.1 Let Q(R, x) be the Gaussian density

O(R, x) = (det(22R)yV/2e7 3), (2.12)

where R is ad x d strictly positive-definite matrix R = [rj] and
x €R*, Then
\n\n=== PAGE 76 ===\n(2.13)
(2.14)
(2.15)
For a proof of this identity, see, for example, our longer paper [2],
where we use the following basic and very convenient identity:
for any Gaussian variable X and complex number a.
Using (2.13), we prove in [2] the following identity that is applicable
to understanding expected tranche loss sensitivities:
Theorem 2.1 Let (X1, …, XN) be an 
-valued Gaussian variable,
with each Xm having mean 0 and variance 1, and covariance matrix
R = [rjk] that is strictly positive definite. Let
where 
, and l1, …, lN > 0. Then
for any a ≥ 0, and any distinct j, k ∈ {1, …, N}.
What we see here is a remarkable relation between the sensitivity of
the expected loss
in the equity tranche [0, a] (see (2.11)) with respect to the correlation
parameter rjk and the default thresholds cj and ck.
\n\n=== OCR PAGE 76 ===\n(2.13),

1 2° Q(R, x) wpe .
> _ ifvg=k,
OQ(R,x) _ J 2 Ox;
Ory d-Q(R, x) ifj#k.
Ox ;OX;,

For a proof of this identity, see, for example, our longer paper [2],
where we use the following basic and very convenient identity:

E[e**] = AEX S vartX) (2.14)

for any Gaussian variable X and complex number a.

Using (2.13), we prove in [2] the following identity that is applicable
to understanding expected tranche loss sensitivities:

Theorem 2.1 Let (X,, ..., Xx) be an R‘-valued Gaussian variable,
with each X,, having mean 0 and variance 1, and covariance matrix
R = [ry] that is strictly positive definite. Let

N
L= Y Lin Xp Sen)?
m=1
where C1: +++ +¢n © R, and l,, ..., ly > 0. Then
0, EU(L—a),] = 0,,0,,E((L— a), ] (2.45)

for any a = 0, and any distinct j,k € {1, ..., N}.

What we see here is a remarkable relation between the sensitivity of
the expected loss

E[L -(L-a),]

in the equity tranche [0, a] (see (2.11)) with respect to the correlation
parameter rj and the default thresholds c; and c;.
\n\n=== PAGE 77 ===\n(2.16)
(2.17)
(2.18)
(2.19)
2.4 Correlation sensitivities
We turn now to the nature of correlation sensitivities for the tranche
loss. Our result from [2] is:
Theorem 2.2 Let (X1, …, XN) be an 
-valued Gaussian variable,
with each Xm having mean 0 and variance 1, and covariance matrix
R = [rjk] that is strictly positive definite. Let
where 
, and l1, …, lN > 0. Then, for any a ≥ 0,
for any distinct values of j, k ∈ {1, …, N}.
We sketch here the essential ideas of the proof; a complete proof is
available in [2].
Sketch Proof. The expected tranche loss 
 is given by
where Q(R, x) is the Gaussian density
and
Taking the derivative with respect to the correlation rjk and using the
second Gaussian identity (2.13), we have
\n\n=== OCR PAGE 77 ===\n2.4 Correlation sensitivities

We turn now to the nature of correlation sensitivities for the tranche
loss. Our result from [2] is:

Theorem 2.2 Let (X,, ..., Xy) be anR-valued Gaussian variable,
with each X,, having mean o and variance 1, and covariance matrix
R = [rj] that is strictly positive definite. Let

N
L= PV ilyycor
a

where ‘1: ---»¢nv © R, and L, ..., ly > 0. Then, for any a = 0,

0, EL — a),] = 0,,0,,E[(L — a),.] (2.16)

for any distinct values of j,k € {1, ..., N}.

We sketch here the essential ideas of the proof; a complete proof is
available in [2].

Sketch Proof. The expected tranche loss Fl(L — 4),1is given by

E[(L—a),] -[ (ly(x) — a), O(R, x) de, (2.17)
where Q(R, x) is the Gaussian density

O(R,x) = (det(2aR))7/2e7 38"), (2.18)

and

N
(2.19)
Ie) = Yr Noose im)

m=1

Taking the derivative with respect to the correlation rj, and using the
second Gaussian identity (2.13), we have
\n\n=== PAGE 78 ===\n(2.20)
(2.21)
(2.22)
Now using integration by parts, we move the partial derivatives over
to the first term and obtain, formally,
This is formal because we have not given a precise meaning to the
derivatives involved here. A precise meaning can be given either in
the sense of distributions or, more simply, by working out the full
integration-by-parts process in terms of limits of difference quotients
instead of partial derivatives. Writing the partial derivative 
 as the
limit of a difference quotient and using dominated convergence, and
repeating this for xk, we have
where
We split the integration on the right in (2.21) into four integrals
corresponding to the terms in (2.22) and replace xj by xj − εj and xk
by xk − εk, and then we use
This relation allows us to transform the differences involving the xj
and xk into differences involving cj and ck. Putting these ideas
\n\n=== OCR PAGE 78 ===\n9,,EUL — a), 1 = | (ly) — a), 0, O(R,.x) de (2.20)

° OCR, x)
= Ly(x) — a), ——— dx.
[ (xX) — a), axdx, x

R

Now using integration by parts, we move the partial derivatives over
to the first term and obtain, formally,

A (Ly(x) — a)
/ PUNO Os e + O(R, x) dx.
RN OXjOX;,

This is formal because we have not given a precise meaning to the
derivatives involved here. A precise meaning can be given either in
the sense of distributions or, more simply, by working out the full
integration-by-parts process in terms of limits of difference quotients

instead of partial derivatives. Writing the partial derivative 9s, as the
limit of a difference quotient and using dominated convergence, and
repeating this for x;, we have

. l (2.21),
0 El(L— = | — L(x) — k] dx, nie
Onn K a4] el0es10 je K a no) a4, [*] .

where

(2.22)
[*] = O(R, x + ee; + €,e,) — OCR, x + €e;) — OCR, x + €,e,) + O(R, x).

We split the integration on the right in (2.21) into four integrals
corresponding to the terms in (2.22) and replace x; by x; — ¢; and x,

by x; — €,, and then we use
Teo, & = €) = Vcoyctey)

This relation allows us to transform the differences involving the x;
and x, into differences involving c; and c,. Putting these ideas
\n\n=== PAGE 79 ===\n(2.23)
(2.24)
together, dividing by εj and letting εj↓0, and then dividing by εk and
letting εk↓0, we obtain
Going back to (2.21), we conclude that
We turn now to our main inequality.
Theorem 2.3 Let (X1, …, XN) be an 
-valued Gaussian variable,
with each Xm having mean 0 and variance 1, and covariance matrix
R = [rjk] that is strictly positive definite. Let
where 
, and l1, …, lN > 0. Then, for any a ≥ 0,
and
for all distinct j, k ∈ {1, …, N}.
Again, we present here only the essence of the argument and refer to
[2] for details.
As we saw in the remarks following (2.21)
\n\n=== OCR PAGE 79 ===\ntogether, dividing by e; and letting ¢;|0, and then dividing by e, and
letting €,|0, we obtain

PELL —a),]
Oc;Oc;, .

Going back to (2.21), we conclude that

,,E[(L — a),] = 0,,0,,

E[(L — a),].

QED

We turn now to our main inequality.
Theorem 2.3 Let (X,, ..., Xy) be anR-valued Gaussian variable,
with each X,, having mean 0 and variance 1, and covariance matrix
R = [ry] that is strictly positive definite. Let

N
L= Y Lily <eu)?
m=l
where ‘1: ---»¢nv © R, and L, ..., ly > 0. Then, for any a = 0,

oE[(L -
(ZL —a),] >0, (2.23)
Orin

and

JE[to (Lh
CouiDl 9

(2.24)
Orix ~

for all distinct j, k € {1, ..., N}.

Again, we present here only the essence of the argument and refer to
[2] for details.

As we saw in the remarks following (2.21)
\n\n=== PAGE 80 ===\n(2.25)
is the limit of a difference quotient integrated against Q(R, x). The
difference term is of the form
where l1, l2, w ≥ 0; the term (2.25) is non negative and this leads to
the inequality (2.23).
Next, we recall from (2.11) that the equity tranche loss is given by
The full portfolio loss L is insensitive to the correlations rjk because it
is simply the sum of the individual expected losses:
Hence
In [2], we show that these results extend to copulas more general
than the Gaussian copula (such as elliptically contoured ones
discussed in Gordon [5]).
Acknowledgment
Sengupta acknowledges research support from NSA grant H98230-
13-1-0210.
\n\n=== OCR PAGE 80 ===\ndE[(L —a),]

Orix
is the limit of a difference quotient integrated against Q(R, x). The
difference term is of the form

{(+h4+w},-{h4+w},-{h4+w},+ wh, (2.25)

where 1,, /,, w = 0; the term (2.25) is non negative and this leads to
the inequality (2.23).
Next, we recall from (2.11) that the equity tranche loss is given by

toa(L) = L—(L—a),.

The full portfolio loss L is insensitive to the correlations rj, because it
is simply the sum of the individual expected losses:

N
ELL] = DY UPIX, <q).
j=l
Hence
PE[tojL)] ELL —a), ] <0

Orig ory.

In [2], we show that these results extend to copulas more general
than the Gaussian copula (such as elliptically contoured ones
discussed in Gordon [5]).

Acknowledgment

Sengupta acknowledges research support from NSA grant H98230-
13-1-0210.
\n\n=== PAGE 81 ===\nReferences
1. Ağça, S., and Islam, S., 2010, Spring. Can CDO equity be short on
correlation? The Journal of Alternative Investments, Vol. 12, No.
4, pp. 85–96.
2. Becker, C., and Sengupta, A. N., 2013. Identities and inequalities
for tranche sensitivities. Communications on Stochastic Analysis,
Vol. 7, No. 3, pp. 425–439.
3. Cousin, A., and Laurent, J.-P., 2008. Comparison results for
exchangeable credit risk portfolios, Insurance: Mathematics and
Economics, Vol. 42, No. 3, pp. 1118–1127.
4. Frey, R., and J. McNeil, A. J., Dependent defaults in models of
portfolio credit risk,
http://www.ma.hw.ac.uk/~mcneil/ftp/defaults-jor.pdf
5. Gordon, Y., 1987. Elliptically contoured distributions. Probability
Theory and Related Fields, Vol. 76, pp. 429–438.
6. Hillebrand, E., Sengupta, A. N., and Xu, J., 2011. Impact of
correlation fluctuations on securitized structures, in Handbook of
Modeling High-Frequency Data in Finance (eds. F. G. Viens, M.
C. Mariani, and I. Florescu), John Wiley & Sons, Inc., Hoboken,
NJ.
7. Hillebrand, E., Sengupta, A. N., and Xu, J., 2012. Serially
correlated defaults in subprime securitization, Communications
on Stochastic Analysis, Vol. 6, No. 3, pp. 487–511.
8. Hull, J., and White, A., 2006. Valuing credit derivatives using an
implied copula approach, Journal of Derivatives, Vol. 14, No 2,
pp. 8–28.
9. Jarrow, R. A., and van Deventer, D. R., 2008. Synthetic CDO
equity: short or long correlation, Journal of Fixed Income, Vol.
17, No. 4, pp. 31–41.
10. Joag-Dev, K., Perlman, M. D., and Pitt, L. D., 1983. Association of
normal variables and Slepian’s inequality, The Annals of
Probability, Vol. 11, pp. 451–455.
11. Li, D., 2000, March. On default correlation: a copula function
approach, Journal of Fixed Income, Vol. 9, No. 4, pp. 43-54.
http:// www.defaultrisk.com/pp_corr_05.htm
12. Meng, C., and Sengupta, A. N., 2011, June. CDO tranche
sensitivities in the Gaussian copula model, Communications on
\n\n=== PAGE 82 ===\nStochastic Analysis, Vol. 5, No. 2, pp. 387–403.
13. Müller, A., and Stoyan, D., 2002. Comparison Methods for
Stochastic Models and Risks, John Wiley and Sons, Inc.,
Hoboken, NJ.
14. Plackett, R. L., 1954. Reduction formula for normal multivariate
integrals, Biometrika, Vol. 41, pp. 351–360.
15. Slepian, D., 1962. The one-sided barrier problem for Gaussian
noise, Bell System Technical Journal, Vol. 41, pp. 463–501.
16. CDOs are back: will they lead to another financial crisis?,
Wharton Business School,
http://knowledge.wharton.upenn.edu/article. cfm?
articleid=3230
\n\n=== PAGE 83 ===\nChapter Three
A Nonlinear Lead Lag Dependence Analysis
of Energy Futures: Oil, Coal, and Natural
Gas1
Germán G. Creamer1 and Bernardo Creamer2
1School of Business, Stevens Institute of Technology, Hoboken,
NJ, USA
2Universidad de las Américas, Quito, Ecuador
3.1 Introduction
The1 main fossil fuels (oil, coal, and natural gas) have some common
elements that affect their prices, such as emissions control or energy
crises. Markets and political forces also affect fuel prices, especially,
in the case of oil. It is also possible their prices are mutually
determined or that one price depends on another one as they
partially behave as substitute goods for the production of electricity.
Mohammadi (2011) finds that in the case of the United States, the oil
and natural gas prices are globally and regionally determined,
respectively, and coal prices are defined by long-term contracts.
Mohammadi (2009), using cointegration analysis, exposes a strong
relationship between electricity and coal prices and an insignificant
relationship between electricity and oil and/or natural gas prices.
Asche et al. (2003) and Bachmeier and Griffin (2006) find very weak
linkages among oil, coal, and natural gas prices, using cointegration
analysis, while crude oil and several refined product prices are
integrated (Asche et al., 2003). Hartley et al. (2008) notice an
indirect relationship between natural gas and oil prices. Even more,
Aruga and Managi (2011) detect a weak market integration among a
large group of energy products: West Texas Intermediate (WTI) oil,
Brent oil, gasoline, heating oil, coal, natural gas, and ethanol futures
prices.
\n\n=== PAGE 84 ===\nMjelde and Bessler (2009) observe that oil, coal, natural gas, and
uranium markets are not fully cointegrated. Asche et al. (2006)
indicate that the UK energy market between 1995 and 1998 was
highly integrated where the demand was for energy rather than for a
particular source of energy. Brown and Yucel (2008) show that oil
and natural gas prices have been independent since 2000; however,
when weather and inventories are taken into consideration in an
error correction model, crude oil prices have an effect on natural gas
prices. Similar results are obtained by Ramberg (2010), using
cointegration analysis. Amavilah (1995) observes that oil prices
influence uranium prices.
Most of the studies about fossil fuels mentioned earlier are based on
cointegration analysis and Granger causality; however, none of these
studies have used a nonlinear correlation measure such as the
Brownian distance correlation proposed by Székely and Rizzo
(2009). This distance correlation is a nonlinear multivariate
dependence coefficient that can be used with random vectors of
multiple dimensions. Among the different studies that use this
distance correlation to finance problems, Creamer et al. (2013)
evaluate the impact of corporate news networks on return and
volatility; Grothe et al. (2014) measure the association of European
bonds and stocks during the recent Euro crisis; Puliga et al. (2014)
build networks of credit default swaps to forecast systemic risk, and
Zhang et al. (2014) generate networks of the international shipping
market to forecast systemic risk. This chapter conducts a lead–lag
analysis of coal, oil, and gas with the Brownian distance correlation
and compares its results with the well-known Granger causality test.
3.1.1 CAUSALITY ANALYSIS
Granger causality (Granger, 1969, 1980, 2001) is a very popular
methodology used in economics, financial econometrics, and in
many other areas of study, such as neuroscience, to evaluate the
linear causal relationship among two or more variables. According to
the basic definition of Granger causality, the forecasting of the
dependent variable Yt with an autoregressive process using Yt − l as
its lag-l value, should be compared with another autoregressive
process using Yt − l and the vector Xt − l of independent variables. So,
\n\n=== PAGE 85 ===\nXt − l Granger causes Yt when Xt − l happens before Yt, and Xt − l has
unique information to forecast Yt that is not present in other
variables.
Typically, Granger causality is tested using an autoregressive model
with and without the vector Xt − 1, such as in the following bivariate
example:
where the residual is a white noise series:
Xt − l Granger causes Yt if the null hypothesis H0: βl = 0 is rejected on
the basis of the F test. The order of the autoregressive model is
selected on the basis of the Akaike information criterion or the
Bayesian information criterion.
Székely and Rizzo (2009) also proposed the Brownian distance
covariance, which captures the covariance with respect to a
stochastic process. Distance covariance (ν(X, Y)) between the
random vectors X and Y measures the distance between fXfY and fX, Y
and is obtained as the square root of ν2(X, Y) = ‖fX, Y(t, s) −
fX(t)fY(s)‖2 where ‖.‖ is the norm, t and s are vectors, fX and fY are the
characteristic functions of X and Y, respectively, and fX, Y is the joint
characteristic function of X and Y.
Empirically, ν(X, Y) evaluates the null hypothesis of independence
H0: fXfY = fX, Y versus the alternative hypothesis HA: fXfY ≠ fX, Y. In
this chapter, this test is the distance covariance test of independence.
Likewise, distance variance (ν(X)) is the square root of ν2(X) = ‖fX,
X(t, s) − fX(t)fX(s)‖2.
\n\n=== OCR PAGE 85 ===\nX,_ Granger causes Y, when X; _; happens before Y,, and X,_; has
unique information to forecast Y, that is not present in other
variables.

Typically, Granger causality is tested using an autoregressive model

with and without the vector X;_ ,, such as in the following bivariate
example:

L
Y= Ya, +e
=I
L L
Y= »y ayY,_y + Y BX) + €
i=l

1

where the residual is a white noise series:
Ee ~ N(0,o),j = 1,2.

X,_, Granger causes Y;, if the null hypothesis H,: B; = 0 is rejected on

the basis of the F test. The order of the autoregressive model is
selected on the basis of the Akaike information criterion or the
Bayesian information criterion.

Székely and Rizzo (2009) also proposed the Brownian distance
covariance, which captures the covariance with respect to a
stochastic process. Distance covariance (v(X, Y)) between the
random vectors X and Y measures the distance between fxfy and fy y

and is obtained as the square root of v°(X, Y) = IIfy y(t s) -

Fx(F (5)? where ||.|| is the norm, t and s are vectors, fy and fy are the
characteristic functions of X and Y, respectively, and fy y is the joint
characteristic function of X and Y.

Empirically, v(X, Y) evaluates the null hypothesis of independence
Hy: fxfy = fx, y versus the alternative hypothesis H,: fyfy + fy, y- In
this chapter, this test is the distance covariance test of independence.

Likewise, distance variance (v(X)) is the square root of v?(X) = Ix,

x@ s) - KORG).
\n\n=== PAGE 86 ===\nOnce distance covariance is defined, the distance correlation R(X, Y)
is also defined in the following expression:
Distance correlation takes a value of 0 in case of independence and 1
when there is complete dependence.
3.2 Data
We used the daily time series of 1 month forward futures log prices of
the fossil fuel series for the period 2006–2012: WTI oil, the Central
Appalachian [bituminous] coal (Coal) and natural gas (Gas) from the
New York Mercantile Exchange (see Figure 3.1). These series have
some relevant autoregressive effects according to the autocorrelation
function (ACF) and the partial ACF (see Figure 3.2); however, the
emphasis of this chapter is on the lagged cross-correlation, which is
explored in the next sections.
\n\n=== OCR PAGE 86 ===\nOnce distance covariance is defined, the distance correlation R(X, Y)
is also defined in the following expression:

(X,Y 58
5 VY v-(X)v-(Y) > 0
R=) Vv2(X)v2(¥))
0, v?(X)v?(Y) =0

Distance correlation takes a value of 0 in case of independence and 1
when there is complete dependence.

3.2 Data

We used the daily time series of 1 month forward futures log prices of
the fossil fuel series for the period 2006-2012: WTI oil, the Central
Appalachian [bituminous] coal (Coal) and natural gas (Gas) from the
New York Mercantile Exchange (see Figure 3.1). These series have
some relevant autoregressive effects according to the autocorrelation
function (ACF) and the partial ACF (see Figure 3.2); however, the
emphasis of this chapter is on the lagged cross-correlation, which is
explored in the next sections.
\n\n=== PAGE 87 ===\nFIGURE 3.1 Log prices by product. Horizontal lines represent
structural breaks according to the Bai–Perron test of the Coal/WTI
log prices ratio.
\n\n=== OCR PAGE 87 ===\n5.0

a
e4
7 bt]
=] -
gS E
Oo S| ° ' '
2] *
Pa ' i 1 1
07 T t—T T T T o T ye 1 T
Jan-07  Jan-09 Jan-11 Jan-07 Jan-09  Jan-11

ai 2
o el
d =
N
g =
2
r re)
©
o
Jan-07 Jan-09 Jan-11 Jan-07 Jan-09  Jan-11
FIGURE 3.1 Log prices by product. Horizontal lines represent

structural breaks according to the Bai—Perron test of the Coal/WTI
log prices ratio.
\n\n=== PAGE 88 ===\nFIGURE 3.2 ACF and partial ACF of log returns by product.
3.3 Estimation techniques
We evaluated the stationarity of the series, using the augmented
Dickey–Fuller (ADF) test. We applied the Bai and Perron (1998)’s
test to detect structural breaks of the coal/WTI log prices ratio,
considering that these are the most dominant products of the
causality analysis. The Bai–Perron test is particularly useful when
the break date is unknown and there is more than one break date.
For the complete series and for each of the periods identified with
the Bai–Perron test, we tested the nonlinearity of the series, using
the White (Lee et al., 1993) and the Terasvirta test (Terasvirta et al.,
1993). We also conducted a nonlinear lead–lag relationship analysis,
using the Brownian distance correlation between each pair of
variables and up to seven lags (1 week). We compared these results
with the Granger causality test and evaluated the cointegration of the
different pairs using the Johansen test (Johansen, 1988a, b) to
\n\n=== OCR PAGE 88 ===\nPartial ACF

-0.06 0.02

Coal
6
u 8 <8
§ z°
52
oS f-5~8-8-5-Sv3-Gr FS 3 E-S'5-5-3-S-ES SS ao
7 T t T T 9

Partial ACF

0.06 0.02

RULED N

+
0 5 10 15
Lag

FIGURE 3.2 ACF and partial ACF of log returns by product.

3.3 Estimation techniques

We evaluated the stationarity of the series, using the augmented
Dickey—Fuller (ADF) test. We applied the Bai and Perron (1998)’s
test to detect structural breaks of the coal/WTI log prices ratio,
considering that these are the most dominant products of the
causality analysis. The Bai—Perron test is particularly useful when
the break date is unknown and there is more than one break date.
For the complete series and for each of the periods identified with
the Bai—Perron test, we tested the nonlinearity of the series, using
the White (Lee et al., 1993) and the Terasvirta test (Terasvirta et al.,
1993). We also conducted a nonlinear lead-lag relationship analysis,
using the Brownian distance correlation between each pair of
variables and up to seven lags (1 week). We compared these results
with the Granger causality test and evaluated the cointegration of the
different pairs using the Johansen test (Johansen, 1988a, b) to
\n\n=== PAGE 89 ===\ndecide if we had to use the VAR error correction model. In our
analysis, → denotes relationship. For instance, X → Y indicates that
X Granger causes Y when Granger causality is used or Y is dependent
of X when the Brownian distance correlation is used. Therefore, the p
value of every test evaluates only the effect of one variable into
another one and is not affected by other time series.
3.4 Results
The Bai–Perron test applied to the Coal/WTI ratio series split the
data into the following periods: January 3, 2006–January 17, 2008
(precrisis period), January 18, 2008–November 17, 2010 (financial
crisis period), and November 18, 2010–December 31, 2012 (recovery
period) (see Figure 3.1). We conducted our analysis in these different
periods and in the complete series 2006–2012. The ADF test
indicates that all log price series for the aforementioned periods are
nonstationary and, as expected, the log return series are stationary.
So, we used the log returns (the first difference of the log price) to
conduct the causality tests.
The gas price distribution shows the highest volatility, although it is
less skewed and flatter than the rest, according to Table 3.1 and
Figure 3.1. The correlations between the price series significantly
increase during the crisis period, as can be observed by the
convergence of all the series in Figure 3.1 and in Table 3.2. In the
precrisis period, the correlation between WTI and Coal is 0.64;
however, after the crisis, this correlation falls to 0.08. The opposite
happens with the correlation between Coal and Gas where this value
increases from 0.29 to 0.82, while the correlation between Gas and
WTI changes from 0.07 to 0.15. These cross-correlation changes
indicate a high interrelationship among the three fossil fuel series;
however, the long-term dynamic linkages are better captured by the
lead-lag and Granger causality analysis included in Table 3.3.
\n\n=== PAGE 90 ===\nTable 3.1 Descriptive statistics of log prices.
2006–2012
Coal WTI Gas
Mean
4.08  4.37
 1.62
SD
0.26  0.25  0.41
Skewness 0.79 −0.47  0.05
Kurtosis
0.34  0.48 −0.63
Table 3.2 Correlation matrix of log prices. The table does not
include the diagonal values.
2006-12
Precrisis
Crisis
Postcrisis
Coal WTI Coal WTI Coal WTI Coal WTI
WTI 0.64
0.54
0.67
0.08
Gas 0.16 −0.03 0.28 0.06 0.85 0.65 0.82 −0.15
\n\n=== PAGE 91 ===\nTable 3.3 Significance level of Granger causality († p ≤ 0.05, ‡ p ≤
0.01) and Brownian distance correlation (* p ≤ 0.05, ** p ≤ 0.01) of
log return series. Nonrelevant relationships are excluded. Yellow
indicates nonlinearity according to either the White or Terasvirta
test, and green means that both tests detect nonlinearity with a 5%
significance level.
As none of the log price pairs are cointegrated in the different
periods at the 5% significance level according to the Johansen test,
we used a vector autoregressive (VAR) model of the log return series
to run the Granger causality test with seven lags instead of using the
vector autoregressive error correction model.
3.5 Discussion
During the complete period 2006–2012, WTI and Coal show a
feedback relationship according to the Brownian distance, and only
the WTI → Coal relationship is maintained conforming to the
Granger causality test with a 5% significance level (see Table 3.3).
Coal also Granger causes Gas for the lags 5–7. In addition, the
Brownian distance recognizes the following dependences (lags
between parentheses): Gas (1, 7) → Coal, Coal (2, 3) → Gas, and WTI
(1) → Gas. Very similar relationships are observed during the crisis
\n\n=== OCR PAGE 91 ===\nTable 3.3 Significance level of Granger causality (+ p < 0.05, #p <
0.01) and Brownian distance correlation (* p < 0.05, ** p < 0.01) of
log return series. Nonrelevant relationships are excluded. Yellow
indicates nonlinearity according to either the White or Terasvirta
test, and green means that both tests detect nonlinearity with a 5%
significance level.

i)
we
+
a
a
]

Periods Lags/Effects 1

2006-12 WTI = Coal
Gas + Coal
Coal + WTI
Coal + Gas
WTI — Gas
Precrisis WTI = Coal
Gas + Coal
Gas + WTI
Crisis WTI = Coal
Gas > Coal
Coal + WTI
Coal + Gas
WTI — Gas
Recovery Gas > Coal
Coal + Gas $ *t

oe

As none of the log price pairs are cointegrated in the different
periods at the 5% significance level according to the Johansen test,
we used a vector autoregressive (VAR) model of the log return series
to run the Granger causality test with seven lags instead of using the
vector autoregressive error correction model.

3.5 Discussion

During the complete period 2006-2012, WTI and Coal show a
feedback relationship according to the Brownian distance, and only
the WTI — Coal relationship is maintained conforming to the
Granger causality test with a 5% significance level (see Table 3.3).
Coal also Granger causes Gas for the lags 5-7. In addition, the
Brownian distance recognizes the following dependences (lags
between parentheses): Gas (1, 7) > Coal, Coal (2, 3) > Gas, and WTI
(1) > Gas. Very similar relationships are observed during the crisis
\n\n=== PAGE 92 ===\nperiod (2008–2010). Both tests indicate that the Gas → WTI
dependence is relevant during the precrisis period, and the Brownian
distance recognizes the importance of the relationship WTI (1) →
Coal and Gas (1,2)→ Coal. During the recovery period, only the Coal
→ Gas relationship is relevant for both tests, especially for the
Granger causality tests. Most of the additional relationships observed
using the Brownian distance test, which were not recognized by the
Granger causality test, were confirmed to be relevant nonlinear
relationships according to the White and Terasvirta tests (see Table
3.3). Hence, the Brownian distance correlation recognizes an
important number of dependences, and some of them are confirmed
by the Granger causality test.
These identified nonlinear relationships may have an impact on the
selection of inputs used to generate electricity in the United States.
Electricity generated with coal reached its peak in 2007 and
substantially decreased afterward. On the contrary, electricity
generated with natural gas has been increasing since 1990, especially
since 2009. Between the years 2000 and 2012, the proportions of
electricity generated by coal and oil have decreased from 51.7% and
2.9% to 37.4% and 0.6%, respectively, while the proportion of
electricity generated by natural gas almost doubled from 15.8% to
30.4% (see Table 3.4). The increase of the electricity generated with
natural gas is equivalent to the contraction of electricity generated
with coal. This can be partially explained by the decline of natural
gas log prices since December 2005 to April 2012 (see Figure 3.1).
The US Clean Air Act restrictions on SO2 emissions and the relative
reduction of natural gas prices led the power plants to partially
substitute coal with natural gas as their main input. As more power
plants have increased their consumption of natural gas, its price has
also increased following similar trends of oil and coal. The linear and
nonlinear lead–lag analysis also indicates that coal’s price has an
important effect on natural gas price, especially during the crisis and
recovery period. This particular case illustrates the nonlinear
dynamic among the prices of the different commodities studied and
the major interrelationship that exists among the three fossil fuel
series. The main application of these nonlinear relationships is to
improve the forecast of commodity prices.
\n\n=== PAGE 93 ===\nTable 3.4 Net electricity generation: Participation of energy source.
Ren. en. refers to renewable energy.
Year
Coal
Oil
Nat. Gas Nuclear Ren.En. Other
2000 51.72% 2.92% 15.81%
19.83%
 9.38%
0.35%
2001 50.96% 3.34% 17.10%
20.57%
 7.70%
0.32%
2002 50.10% 2.45% 17.91%
20.22%
 8.90%
0.42%
2003 50.83% 3.07% 16.74%
19.67%
 9.15%
0.55%
2004 49.82% 3.05% 17.88%
19.86%
 8.85%
0.53%
2005 49.64% 3.01% 18.77%
19.28%
 8.82%
0.48%
2006 48.97% 1.58% 20.09%
19.37%
 9.49%
0.51%
2007 48.51% 1.58% 21.57%
19.40%
 8.49%
0.45%
2008 48.21% 1.12% 21.44%
19.57%
 9.25%
0.42%
2009 44.45% 0.98% 23.31%
20.22%
10.57%
0.45%
2010 44.78% 0.90% 23.94%
19.56%
10.36%
0.45%
2011 42.27% 0.74% 24.72%
19.27%
12.52%
0.48%
2012 37.42% 0.56% 30.35%
18.97%
12.22%
0.47%
Source: US Energy Information Administration.
3.6 Conclusions
This chapter proposes the use of the Brownian distance correlation
to conduct a lead–lag analysis of financial and economic time series.
When this methodology is applied to asset prices, the nonlinear
relationships identified may improve the price discovery process of
these assets.
The Brownian distance correlation determines relationships similar
to those identified by the linear Granger causality test, and it also
uncovers additional nonlinear relationships among the log prices of
oil, coal, and natural gas. This research can be extended to explore
the lead–lag relationship between spot and future prices of complex
assets such as commodities and foreign currencies applied to
different markets.
\n\n=== PAGE 94 ===\nAcknowledgments
Authors thank participants of the 2014 Eastern Economics
Association meeting, the AAAI 2014 Fall Symposium on Energy
Market Predictions, Dr. Ionut Florescu, and two anonymous referees
for their comments and suggestions. G.C. also thanks the Howe
School Alliance for Technology Management for financial support
provided to conduct this research. The opinions presented are the
exclusive responsibility of the authors.
References
Amavilah, V. 1995. The capitalist world aggregate supply and
demand model for natural uranium. Energy Economics 17(3) 211–
220.
Aruga, K., S. Managi. 2011. Linkage among the U.S. energy futures
markets. Paper prepared for the 34th IAEE International Conference
Institutions, Efficiency and Evolving Energy Technologies,
Stockholm.
Asche, F., O. Gjolberg, T. Volker. 2003. Price relationships in the
petroleum market: an analysis of crude oil and refined product
prices. Energy Economics 25 289–301.
Asche, F., P. Osmundsen, M. Sandsmark. 2006. The UK market for
natural gas, oil, and electricity: are the prices decouples? Energy
Journal 27 27–40.
Bachmeier, L., J. M. Griffin. 2006. Testing for market integration:
crude oil, coal, and natural gas. Energy Journal 27 55–71.
Bai, J., P. Perron. 1998. Estimating and testing linear models with
multiple structural changes. Econometrica 66(1) 47–78.
Brown, S. P. A., M. K. Yucel. 2008. What drives natural gas prices?
Energy Journal 29 45–60.
Creamer, G. G., Y. Ren, J. V. Nickerson. 2013. Impact of dynamic
corporate news networks on asset return and volatility. Social
Computing/IEEE International Conference on Privacy, Security,
Risk and Trust, 2010 IEEE International Conference on September
8–14, 2013, pp. 809– 814.
Granger, C. W. J. 1969. Investigating causal relations by econometric
models and cross-spectral methods. Econometrica 37(3) 424– 438.
\n\n=== PAGE 95 ===\nGranger, C. W. J. 1980. Testing for causality: a personal viewpoint.
Journal of Economic Dynamics and Control 2 329–352.
Granger, C. W. J. 2001. Essays in Econometrics: The Collected
Papers of Clive W. J. Granger. Cambridge University Press,
Cambridge.
Grothe, O., J. Schnieders, J. Segers. 2014. Measuring association and
dependence between random vectors. Journal of Multivariate
Analysis 123 96–110.
Hartley, P., K. B. Medlock III, J. E. Rosthal. 2008. The relationship
of natural gas to oil prices. Energy Journal 29(3) 47–65.
Johansen, S. 1988a. Estimation and hypothesis testing of
cointegration vectors in gaussian vector autoregressive models.
Econometrica 59(6) 1551– 1580.
Johansen, S. 1988b. Statistical analysis of cointegration vectors.
Journal of Economic Dynamics and Control 12(2/3) 231–254.
Lee, T.-H., H. White, C. W. J. Granger. 1993. Testing for neglected
nonlinearity in time series models. Journal of Econometrics 56
269–290.
Mjelde, J., D. Bessler. 2009. Market integration among electricity
markets and their major fuel source markets. Energy Economics 31
482–491.
Mohammadi, H. 2009. Electricity prices and fuel costs: Long-run
relations and short-run dynamics. Energy Economics 31 503–509.
Mohammadi, H. 2011. Long-run relations and short-run dynamics
among coal, natural gas and oil prices. Applied Economics 43 129–
137.
Puliga, M., G. Caldarelli, S. Battiston. 2014. Credit default swaps
networks and systemic risk. Scientific Reports 4 1–7.
Ramberg, D. J. 2010. The relationship between crude oil and natural
gas spot prices and its stability over time. Master of Science Thesis,
Massachusetts Institute of Technology.
Székely, G. J., M. L. Rizzo. 2009. Brownian distance covariance. The
Annals of Applied Statistics 3(4) 1236–1265.
Terasvirta, T., C.-F. Lin, C W. J. Granger. 1993. Power of the neural
network linearity test. Journal of Time Series Analysis 14(2) 209–
220.
Zhang, X., B. Podobnik, D. Y. Kenett, H. E. Stanley. 2014. Systemic
risk and causality dynamics of the world international shipping
market. Physica A 415 43–53.
\n\n=== PAGE 96 ===\nNotes
1A preliminary version of this paper is included on the Proceedings of
the AAAI 2014 Fall Symposium on Energy Market Predictions.
\n\n=== PAGE 97 ===\nChapter Four
Portfolio Optimization: Applications in
Quantum Computing
Michael Marzec
Stevens Institute of Technology, Hoboken, NJ, USA
4.1 Introduction
The traditional Markowitz mean-variance model of portfolio
selection has provided the framework for portfolio asset selection for
decades (Markowitz, 1952, Fabozzi et al., 2013). In this framework,
an investor or a portfolio manager wishes to minimize the so-called
risk with a particular asset mix. The measurement of risk in a
portfolio, for this model, is rooted in the association of risk with the
measure of variance, which is a measure of the degree of change or
variability of the data compared with its expected value—its mean.
With multiple assets, the expression of variance of the portfolio
includes the covariance—how much the assets vary together. The
mathematical expression is quadratic and the optimal selection of
assets becomes a quadratic optimization problem. This is formulated
as a nonlinear programming problem, which can be solved by
suitable application of various operations research techniques
(Hillier and Lieberman, 2010).
The efficient frontier is a curve, an area, or a surface that traces out
the risk versus return values resulting from various combinations of
assets (Elton et al., 2007). As the number of asset combinations
increases—in both the number of assets and their weightings in the
portfolio—the selection of the optimal portfolio quickly becomes
difficult. The optimization techniques traditionally used to solve this
problem are so-called classical optimization techniques that rely on
mathematically well-defined gradient-based or descent/directional
indications that must be well defined and constrained (Gilli and
Schumann, 2012).
\n\n=== PAGE 98 ===\nMore modern heuristic optimization techniques, such as stochastic
local search, simulated annealing, threshold accepting, tabu search,
genetic algorithms, particle swarm, and ant colony optimization
provide a fast alternative to these classical optimization techniques
(Gilli et al., 2011). These heuristic techniques provide ways to
approach problems that are too hard to solve classically. Many
classical and heuristic techniques will typically yield solutions that
involve some combination of all of the assets under consideration.
How can we make an appropriate selection of a subset of assets? In
other words, in what way can we make a decision to select m assets
out of a universe of n?
With suitable crafting, this can be approached as a quadratic
unconstrained binary optimization (QUBO) problem. This crafting is
core to, and discussed further in, this chapter. The problem is
quadratic because of mixed terms in the signature equation: V =
XTQX. The final model is unconstrained because it is built without
additional restricting equations. It is binary in nature because it is
written as a binary decision problem (yes/no; include/exclude) of
whether to include a particular stock in the final portfolio. The
problem is one of optimization because the goal is to seek the best—
optimal—combination of stocks.
Specifically, this is a combinatorial optimization problem because
the formulation of the objective will seek a discrete set of assets
represented in graph form: this problem can be investigated using a
graph-theoretic approach (Boros et al., 2008; Jallo and Budai, 2010,
Papadimitriou and Steiglitz, 1998).
Iterating through all of the combinations of assets and weightings
can be computationally intensive. Heuristics provide some
improvement over classical techniques but could either run too long,
yield suboptimal results, or not return a solution at all. One
paradigm of solving this type of problem is quantum computing
(Choi, 2010; D-Wave, 2013b, 2013c). This paradigm holds promise
toward many challenging computational problems that are either
difficult to solve or not possible with current techniques.
The problem of making a binary (include/exclude) selection of assets
for a portfolio can be solved in a rudimentary way using the
maximum independent set (MIS) graph-theoretic approach to
\n\n=== PAGE 99 ===\nsolving the QUBO in a quantum computing paradigm (D-Wave,
2013b). Can this technique be applied or extended to solve a
weighted binary asset selection problem in a Markowitz mean-
variance framework?
Overall, this investigation will help bridge the quantum computing
paradigm with financial engineering research topics. It will provide
insight into the problem domain of the quantum computer linked
with associated financial engineering concepts. The fundamental
goal of this chapter is to demonstrate how the framework can be
used to solve financial problems. It will discuss the limitations of the
environment with respect to financial modeling considerations. This
will be accomplished by presenting the formulation of financial
portfolio optimization in the context of this hardware paradigm. This
chapter presents and discusses existing sample work (D-Wave,
2013b).
The research question covers three areas that are reflected in the
literature: classical mean-variance portfolio theory; general
operations research theory with specific consideration given to
combinatorial optimization topics; and the hardware realization of
adiabatic quantum computation.
The remainder of the chapter delves into a survey of background
literature, the models used, experimental methodology, results
obtained, discussion, and conclusion. The background literature
section presents a variety of research and applied papers on the
underlying topics. The model section relates the portfolio
optimization representation to the graph-theoretic domain and into
the underlying Ising problem domain of the target hardware. The
methodology section presents the information relevant to the
implementation undertaken in this study. The results section
presents what came out of the investigation. The discussion section
relates the results to the underlying domain, along with limitations
and future areas for investigation. The conclusion finalizes the
chapter.
\n\n=== PAGE 100 ===\n4.2 Background
Considering the hybrid nature of topics in this chapter, the available
literature is deep and varied, crossing several disciplines. The
following is a survey of material that weaves a path through the
background subjects of classical mean-variance portfolio theory,
general operations research theory, combinatorial optimization
topics, and the hardware realization of adiabatic quantum
computation.
4.2.1 PORTFOLIOS AND OPTIMIZATION
As related at the beginning of the “Introduction” section, the
Markowitz paper on mean-variance portfolio theory historically laid
the foundation for variance–covariance portfolio modeling over the
last several decades (Markowitz, 1952, Fabozzi et al., 2013).
Meanwhile, portfolio optimization has evolved from that work with
different objective representations or constraints. For example, more
recent models consider a conditional value-at-risk (VaR) objective
with constraints (Krokhmal et al., 2001).
There is a lot of literature in the broad areas of portfolio theory,
portfolio optimization, and the general topic of optimization theory
and techniques. In particular, in the optimization arena, a recent
review illustrates a lot of research activity over the 1998–2008
timeframe (Floudas and Gournaris, 2009). Similarly, finance-
oriented discussions are presented in the study by Gilli et al., 2011
with specific consideration of heuristic models in finance with Gilli
and Schumann, 2012.
Other (non–mean-variance) methods include heuristic optimization
with differential evolution under consideration of risk preferences
and loss aversion (Maringer, 2006). Risk parity, equal weighting,
and minimum variance are discussed in the study by Chaves et al.,
2010.
Some very particular formulations of the mean-variance problem are
solved with modern and classical optimization techniques: using
Lagrangian relaxation (Shaw et al., 2008); a hybrid Grey Relational
Analysis approach (Huang et al., 2011); and a genetic algorithm
formulation (Soleimani et al., 2009), as just a few examples.
\n\n=== PAGE 101 ===\nUnderlying all of these approaches is the desire to find improved
methods to solve the optimization problem. Many of the problems
are difficult to solve because their model exhibits several local
optima or have discontinuities in the function expression (Gilli and
Schumann, 2012). Reflecting back on the base mean-variance
formulation of portfolio optimization is a problem that is quadratic
in nature with squared terms of variance and mixed covariance
terms that can exhibit some of these characteristics.
Finding the best overall solution out of these multiple local optima is
the goal of global optimization. A sampling of current research
papers specific to the global optimization of this quadratic
formulation includes equilibrium search techniques (Pardalos et al.,
2008), barrier function formulations (Dang and Xu, 2000), an
interior-point algorithm approach (Akrotirianakis and Rustem,
2005), and an unconstrained max-flow approach (Boros et al.,
2008).
Some of the research considers constraints in the formulation
whereas some are written in unconstrained representation.
Constrained problems may sometimes be rewritten as unconstrained
formulations using various techniques (Hillier and Lieberman, 2010;
Gilli et al., 2011). The unconstrained representations are used in this
investigation.
A link between MISs, cliques, and stock market data is discussed in
the study by Boginski et al. (2005) where the cross-correlations
between stocks are studied over time. The thesis by Jallo and Budai
(2010) further elaborates on market graphs related to pure stock
returns, liquidity-weighted returns, and volume measures.
The article by Charpin and Lacaze (2007) presents a binary
optimization on portfolios that provides a method to constrain a
portfolio to a specific size, that is, cardinality, and also that
determines an optimal portfolio satisfying minimum weight
conditions. They solved the problem with branch-and-bound
optimization in the Lingo or CPLEX commercial application
environments. The binary nature of the problem is similar to what is
considered in this chapter, but the model has a different form and
this chapter's model formulation is quickly translated to graph-
theoretic representation. The paper by Bertsimas et al. (1999) uses
\n\n=== PAGE 102 ===\nmixed-integer programming to perform a constrained optimization
using classical models that have binary selection characteristics.
4.2.2 ALGORITHMIC COMPLEXITY
The topic of algorithmic complexity is an important motivation to,
and marker of, the investigation of new techniques to solving these
problems. Since the core of this chapter is the practical
implementation of a new approach to solving hard problems,
algorithmic complexity deserves at least a cursory overview.
Algorithms, or sequences of steps in a well-defined computational
procedure, are used to specify the way of solving various problems.
Algorithms are used at some point in the description,
implementation, or analysis of all of the methods cited in this
chapter. To pick one algorithm over another, a measure of efficiency
is used to compare their performance. This is typically the
measurement of how quickly a particular algorithm finds an answer
given a certain input size. A few good references for this topic include
Cormen et al. (2009), Kozen (1992), Papadimitriou and Steiglitz
(1998).
Algorithms may be put into three classes: P, NP, and NP-complete.
Problems that can be solved in polynomial time are in class P.
Polynomial time solutions are found in a time proportional to the
size of the problem input raised to some constant power. Problems
that can only be verified, but not solved, in polynomial time are
called nondeterministic polynomial, class NP. Finally, problems that
do not have an algorithmic way of finding an exact solution are in the
class NP-complete and are considered the hardest. Typically,
algorithms applied to NP-complete problems find good enough
solutions or approximations rather than the best or optimal solution.
There is another class called NP-hard, which for the purpose of this
chapter, means problems that are at least as hard as the hardest
problems in NP. However, NP-hard problems do not need to be
verifiable in polynomial time and, thus, do not actually need to be
members of class NP.
This is important because some of the models considered in the
chapter are in the classes NP-complete or NP-hard and the D-Wave
system can be applied to these problems.
\n\n=== PAGE 103 ===\n4.2.3 PERFORMANCE
As mentioned in Section 4.2.2, the performance categorization of
algorithms allows us to measure usefulness, among other things. One
of the reasons for investigating the D-Wave system is that it holds
promise to help solve problems that are too complex, too slow or do
not yield solutions under classical methods. In other words, it
promises to yield performance improvements over the current
paradigm.
Therefore, it would be useful to know how the D-Wave system
compares to current, classical systems used to solve particular
problems. The D-Wave system has been experimentally compared
with three conventional software solvers IBM ILOG CPLEX,
METSlib TABU search, and akmaxsat. Three problems were
investigated from the NP-hard problem domain: QUBO, weighted
maximum 2-satisfiability (W2SAT), and quadratic assignment.
Various problem sizes and implementation situations are used in the
comparison. Problem solution quality and success are also
compared, in addition to timing comparisons. The results do indicate
situations in which the D-Wave equipment may not be ideal or yield
improvement. However, the results show that the hardware
implementation can be several thousand times faster, up to even
10,000 times faster, than current implementations (McGeoch and
Wang, 2013).
4.2.4 ISING MODEL
The Ising model was originally constructed to help understand the
behavior of magnetic materials. Two main terms in the model
represent the collection of individual field strengths at each molecule
and the collection of interactions among neighboring molecules. The
Ising representation discussed in Boixo (Boixo et al., 2012) and, in
the form used here, in Choi (2008) is the energy function
where the si ∈ { − 1, +1} represents the spins of the molecules in a
system that is in an applied magnetic field, hi is the strength of a
\n\n=== OCR PAGE 103 ===\n4.2.3 PERFORMANCE

As mentioned in Section 4.2.2, the performance categorization of
algorithms allows us to measure usefulness, among other things. One
of the reasons for investigating the D-Wave system is that it holds
promise to help solve problems that are too complex, too slow or do
not yield solutions under classical methods. In other words, it
promises to yield performance improvements over the current
paradigm.

Therefore, it would be useful to know how the D-Wave system
compares to current, classical systems used to solve particular
problems. The D-Wave system has been experimentally compared
with three conventional software solvers IBM ILOG CPLEX,
METSlib TABU search, and akmaxsat. Three problems were
investigated from the NP-hard problem domain: QUBO, weighted
maximum 2-satisfiability (W2SAT), and quadratic assignment.
Various problem sizes and implementation situations are used in the
comparison. Problem solution quality and success are also
compared, in addition to timing comparisons. The results do indicate
situations in which the D-Wave equipment may not be ideal or yield
improvement. However, the results show that the hardware
implementation can be several thousand times faster, up to even
10,000 times faster, than current implementations (McGeoch and
Wang, 2013).

4.2.4 ISING MODEL

The Ising model was originally constructed to help understand the
behavior of magnetic materials. Two main terms in the model
represent the collection of individual field strengths at each molecule
and the collection of interactions among neighboring molecules. The
Ising representation discussed in Boixo (Boixo et al., 2012) and, in
the form used here, in Choi (2008) is the energy function

E= YX hjs; + YX Jy Si5;
ieV(G) ijEE(G)

where the s; € { - 1, +1} represents the spins of the molecules in a
system that is in an applied magnetic field, h; is the strength of a
\n\n=== PAGE 104 ===\nmagnetic field at molecule i, and Jij represents the strength of
interaction between the neighboring molecule's spins i and j.
It was later realized that this simple model could be applied to many
other situations in which consideration is given to a collection of
individual properties and their interactions. As stated in the study by
Bian and colleagues (Bian et al., 2010), there were apparently more
than 12,000 papers published between 1969 and 1997 that used the
Ising representation. Presumably, that number may be much higher
now.
This Ising model shares a similar structure to the QUBO problem,
which can be used to represent the portfolio optimization problem.
Moreover, the D-Wave equipment is a hardware implementation of
an optimization engine designed to solve the Ising problem, to which
the QUBO form can be translated (D-Wave, 2013b, 2013c; Bian et al.,
2010).
4.2.5 ADIABATIC QUANTUM COMPUTING
Adiabatic quantum computing is a new computing paradigm that has
potential to yield satisfactory results when used to solve some of the
NP-complete or NP-hard problems. The quantum computing aspect
speaks to the use of special purpose-built analog environments built
to take advantage of quantum physics properties of the core
materials. This is in contrast to our current digital computing
paradigm. Adiabatic, in this sense, means that the underlying
material's quantum nature is kept in its ground state as the system
evolves toward the solution. This contrasts with the classical cooling
usage of the term that is a temperature-based annealing process
(Farhi et al., 2000).
However, one of the heuristic methods that can be applied to solve
difficult optimization problems is simulated annealing, which is
based on that classical physical cooling process mentioned in the
previous paragraph.
In both cases, the general idea is to iteratively evolve a problem by
the introduction of a random component, through either
temperature or quantum processes, to help improve the solution. In
this sense, quantum annealing is conceptually similar and uses
\n\n=== PAGE 105 ===\nentropy of the quantum process to explore the objective. In the
environment discussed in this chapter, the quantum annealing is
realized by the D-Wave hardware system (Farhi et al., 2000; Bian
et al., 2010; Hillier and Lieberman, 2010; Gilli et al., 2011).
The fundamental quantum nature of the D-Wave system is a hot
topic of debate. Recent investigations seem to indicate that quantum
signatures are present in the system (Boixo et al., 2012). No matter
how this debate resolves, if the system can ultimately provide good
cost-effective solutions to difficult problems, then it becomes useful.
In summary, the background literature presents a variety of models
and solution methods that are beneficial to obtaining a portfolio mix
that can be used to represent a range of financial objectives. In
particular, the QUBO problem, mentioned earlier (Boros et al.,
2008), is characteristic to portfolio optimization, the Ising problem
in physics (Palubeckis, 2004, Chen and Zhang 2010, Boixo et al.,
2012), and other discrete mathematics problems such as
combinatorial optimization graph-theoretic representations (Choi,
2010). These topics combine to provide a unique portfolio problem
formulation that will be investigated in this chapter. The model
section discusses the specific representations used in this
investigation.
4.3 The models
The core models are financial (portfolio), graph-theoretic
combinatorial optimization, Ising/QUBO, and the resulting mixed
model. Each is reviewed and discussed with consideration of how
they are used for the current investigation.
4.3.1 FINANCIAL MODEL
The primary model is based on the Markowitz mean-variance
paradigm. As such, the basic representation of portfolio risk as
measured by the variance and covariance between stocks is
considered.
The formula expression for the variance of a portfolio, σ2
P, is
\n\n=== PAGE 106 ===\nHere the first term, 
, is the sum of the variances on the
individual assets, σ2
j, multiplied by the square of the proportion
invested in each, X2
j. The second term, 
, is the
effect of the covariance between the individual assets in the
portfolio.
The matrix representation of the variance, V, equation is
where XT is the transpose vector of the proportional investment for
each stock in the portfolio and Q is the variance–covariance matrix.
Traditionally, we may look to a simplified composite model to
represent the portfolio risk and return, which can then be optimized.
This would fundamentally look like
where R represents the portfolio return and V represents the
portfolio risk.1 In this case, the optimization is a minimization: we
wish to find
This optimization therefore minimizes the risk while maximizing the
return. However, this optimization is usually influenced by
constraints. This is further discussed, in depth, in the very tractable
(Bartholomew-Biggs, 2005), which develops a composite function
form with constraints, discusses the conversion to an unconstrained
problem formulation, and builds up iterations on the problem
formulation, finally considering global unconstrained optimization.
Putting aside this model framework for a while, the transition to the
quantum computing environment needs to be considered.
\n\n=== OCR PAGE 106 ===\nN py? 2
Here the first term, din %; 95) , is the sum of the variances on the

individual assets, o7;, multiplied by the square of the proportion
; ; De De Xe)
invested in each, x. The second term, ” ki , is the

effect of the covariance between the individual assets in the
portfolio.

The matrix representation of the variance, V, equation is

V = x'OX

where X’ is the transpose vector of the proportional investment for
each stock in the portfolio and Q is the variance—covariance matrix.

Traditionally, we may look to a simplified composite model to
represent the portfolio risk and return, which can then be optimized.
This would fundamentally look like

F=-R+V

where R represents the portfolio return and V represents the

portfolio risk.4 In this case, the optimization is a minimization: we
wish to find

min {F}

This optimization therefore minimizes the risk while maximizing the
return. However, this optimization is usually influenced by
constraints. This is further discussed, in depth, in the very tractable
(Bartholomew-Biggs, 2005), which develops a composite function
form with constraints, discusses the conversion to an unconstrained
problem formulation, and builds up iterations on the problem
formulation, finally considering global unconstrained optimization.
Putting aside this model framework for a while, the transition to the
quantum computing environment needs to be considered.
\n\n=== PAGE 107 ===\n4.3.2 GRAPH-THEORETIC COMBINATORIAL
OPTIMIZATION MODELS
Graph-theoretic structures underlie the combinatorial optimization
problem formulation. The combinatorial optimizations used are the
MIS and weighted MIS (WMIS) representations. It is the MIS and
WMIS representations that provide the bridge between the basic
portfolio optimization problems and their formulations that may be
solvable in the quantum computing environment.
The consideration, so far, has been a quadratic problem form but not
a quadratic binary problem. However, papers by Choi and Bian
provide the linking insight between the Ising/QUBO form and the
MIS or WMIS representation that can be used to express problems
that may be solvable in the quantum computing environment (Choi,
2008, 2010, Bian et al., 2010). As mentioned in these references,
optimization and decision problems are related. The decision
problem (looking for yes/no answer or confirmation to a question)
for MIS is NP-complete and the optimization version (finding the
minimum) of MIS is NP-hard. Finding a solution to such problems
can be quite difficult, that is, take a lot of time. The quantum
computing environment solves these sorts of difficult problems.
The initial adapted model used is the MIS. A graph, G(V,E), of stocks
is created with the stock symbol set as the vertices, V, and the edge
connection, E, is determined by a selection criteria. The first
selection criterion is risk represented as the correlation between
stocks. Therefore, an edge is drawn between two vertices if the
corresponding pair of stocks is correlated above some threshold.
The use of correlation here is different from the variance–covariance
model and warrants a quick review. This selection criteria allows the
optimization process to select what is (loosely) termed a diversified
portfolio; here, the diversification metric is correlation whereas
other metrics may be chosen, such as the percentage of risk born in
the portfolio attributable to market movements, with the Dow-Jones
representing the market, for instance. Such portfolio performance
techniques and metrics are discussed in the study by Elton and
colleagues (Elton et al. 2007). The Charpin and Lacaze article
discusses one such representation in which they minimize V(RP −
\n\n=== PAGE 108 ===\nRb), which is the variation of the differential return of the portfolio,
RP, with that of a benchmark, Rb, for example, something like the
Dow–Jones index, etc. (Charpin and Lacaze, 2007). As such, other
selection criteria or formulations could be chosen and would be
suitable as future investigations. It is important to note that with
correlation as a selection parameter the edge selection is made when
stocks are correlated above a certain threshold. With variation as a
selection parameter, drawing edges with the selection parameter is
not too intuitive: using the variation as the vertex weight and then
optimizing for the minimum weight, that is, the negative of the
maximum weight optimization function, would yield a set of stocks
that have low variation.
The enhanced model considers the weighted MIS, WMIS. Here, as in
the initial adapted model, a graph, G(V,E), of stocks is created with
the stock symbol set as the vertices, V, and the edge connection, E, is
determined by a selection criteria. However, each vertex has an
additional parameter: the weight. In this case, the optimization is to
find the MIS that yields the maximum total weight (of vertices
selected in the MIS). The weight, in this implementation, will
represent the historical return of the stock, handicapped by its
variance, represented by the vertex.
4.3.3 ISING AND QUBO MODELS
The Ising representation was briefly discussed in the earlier
background section and is repeated here in the context of the
influential models. The NP-hard Ising representation discussed in
the studies (Boixo et al., 2012, Bian et al., 2010) and, in the form
used here, in the study by Choi (2008) is the energy function
In this model, si ∈ { − 1, +1} represents the spins of molecules within
a system. In this system, the strength of the applied magnetic field at
molecule i is hi. The strength of the interaction between the spins of
neighboring molecules i and j is represented by the matrix term Jij.
\n\n=== OCR PAGE 108 ===\nR,), which is the variation of the differential return of the portfolio,
Rp, with that of a benchmark, R,, for example, something like the
Dow-—Jones index, etc. (Charpin and Lacaze, 2007). As such, other
selection criteria or formulations could be chosen and would be
suitable as future investigations. It is important to note that with
correlation as a selection parameter the edge selection is made when
stocks are correlated above a certain threshold. With variation as a
selection parameter, drawing edges with the selection parameter is
not too intuitive: using the variation as the vertex weight and then
optimizing for the minimum weight, that is, the negative of the
maximum weight optimization function, would yield a set of stocks
that have low variation.

The enhanced model considers the weighted MIS, WMIS. Here, as in
the initial adapted model, a graph, G(V,E), of stocks is created with
the stock symbol set as the vertices, V, and the edge connection, E, is
determined by a selection criteria. However, each vertex has an
additional parameter: the weight. In this case, the optimization is to
find the MIS that yields the maximum total weight (of vertices
selected in the MIS). The weight, in this implementation, will
represent the historical return of the stock, handicapped by its
variance, represented by the vertex.

4.3.3 ISING AND QUBO MODELS

The Ising representation was briefly discussed in the earlier
background section and is repeated here in the context of the
influential models. The NP-hard Ising representation discussed in
the studies (Boixo et al., 2012, Bian et al., 2010) and, in the form
used here, in the study by Choi (2008) is the energy function

E= YX hjs; + YX Jy Si5;
ieV(G) ijEE(G)

In this model, s; € { - 1, +1} represents the spins of molecules within

a system. In this system, the strength of the applied magnetic field at
molecule i is h;. The strength of the interaction between the spins of

neighboring molecules i andj is represented by the matrix term Jy.
\n\n=== PAGE 109 ===\nThe QUBO form that represents the WMIS, discussed in the study by
Choi (2010), is
where ci is the weight applied at vertex i and the Jij is further
discussed in the context of the translation, in Section 4.3.4.
4.3.4 MIXED MODELS
For the implementation considered here, the problem must be
translated, that is, mapped, from WMIS into Ising form so that it can
be implemented on the D-Wave system. The translation follows (D-
Wave, 2013b), which is further elaborated in the study by Choi
(2010).
The mapping generates a matrix J, where Jij entries are essentially
determined by constant*marketGraph; that is, the market graph is
multiplied by a constant. The constant is the parameter J = 1.1, that
is anything greater than 1, discussed in the references: the condition
on Jij is that it satisfies Jij > min{ci, cj} and if the weights (ci, cj) are
all equal-weight with value 1, then J = 1.1 > min{ci, cj}. The market
graph has entries with value 1, indicating the existence of an edge
between the corresponding vertices, otherwise a value of 0. The hi
Ising vectors follow the mapping hi = Jdi − 2ci, where the parameter
J = 1.1, di is a count of the edges connected to the ith vertex and ci is
the weighting considered for that vertex.
The simplified model—the MIS model—is essentially the WMIS, with
the ci (node) weights set to 1, that is, equally weighted.
The WMIS model uses a combination of the stock's return and
variance as the weights for each node in the market graph, that is, for
each of the ci in the hi Ising vectors.
\n\n=== OCR PAGE 109 ===\nThe QUBO form that represents the WMIS, discussed in the study by
Choi (2010), is

Y (x;, wee X,) = x cx; + YX SjXiX;
ieV(G) ijEE(G)
where ¢; is the weight applied at vertex i and the J; is further
discussed in the context of the translation, in Section 4.3.4.

4.3.4 MIXED MODELS

For the implementation considered here, the problem must be
translated, that is, mapped, from WMIS into Ising form so that it can
be implemented on the D-Wave system. The translation follows (D-
Wave, 2013b), which is further elaborated in the study by Choi
(2010).

The mapping generates a matrix J, where Jj entries are essentially
determined by constant*marketGraph; that is, the market graph is
multiplied by a constant. The constant is the parameter J = 1.1, that
is anything greater than 1, discussed in the references: the condition
on J, is that it satisfies Jy> min{c;, cy and if the weights (c,, c;) are
all equal-weight with value 1, then J = 1.1 > min{c;, ci. The market
graph has entries with value 1, indicating the existence of an edge
between the corresponding vertices, otherwise a value of 0. The h;
Ising vectors follow the mapping h; = Jd; — 2c;, where the parameter
J = 1.1, d; is a count of the edges connected to the i” vertex and ¢; is
the weighting considered for that vertex.

The simplified model—the MIS model—is essentially the WMIS, with
the c; (node) weights set to 1, that is, equally weighted.

The WMIS model uses a combination of the stock's return and
variance as the weights for each node in the market graph, that is, for
each of the c; in the h, Ising vectors.
\n\n=== PAGE 110 ===\n4.4 Methods
This proof of concept follows a practitioner's approach with the
following steps: recognizing the potential use for the quantum
computing environment to solve financial-oriented investigations,
discussed earlier; reviewing the general models of relevance to both
paradigms—quadratic and combinatorial optimization; locking in on
analytically demonstrated commonalities between them—choosing a
model representation, discussed earlier; choosing an implementation
environment—MATLAB, in this case; gathering real stock data for
model input—a subset of Dow Jones stocks; coding the
implementation to call the underlying hardware, after preprocessing
the model to fit the needed underlying representation; reviewing the
results; and, finally, considering future research topics, which are
presented in the “Discussion” section.
4.4.1 MODEL IMPLEMENTATION
The model representation, discussed earlier, is realized through the
motivating implementation (D-Wave, 2013b), which yields a
simplified diversification of a stock portfolio by generating
connections in a market graph between two stocks whose correlation
is above a certain threshold. Then an MIS of the graph is found by
rewriting it as an Ising problem. This Ising representation is solved
with the hardware optimization engine. The reference
implementation was written in Python, whereas this chapter looks at
a MATLAB rendition. Also, the implementation will consider
variations in risk-measure threshold and a WMIS implementation
with the weightings represented by each stock's historical return. It is
fully recognized that there may be alternatives to the weighting
assignment, such as by VaR of the stock within the portfolio or,
ultimately, with the actual weight of the stock in the portfolio; see the
“Discussion” section.
Again, the primary objective is to use a known model (context) but to
express it in the language necessary to run it on the D-Wave
hardware optimization engine. The setting of this investigation is
both theoretically analytic and pragmatic: portfolio theory mixes
with quantum computing theory, the practicalities of plausible
\n\n=== PAGE 111 ===\nimplementation and the actual mechanics of programming the
problem to run in the environment.
4.4.2 INPUT DATA
For the implementation, the data were retrieved from Yahoo!Finance
and stored in arrays and structures. The next step calculated the log
return for each of the historical daily returns: measure the day-to-
day return ri of the stock prices (Si) between day i − 1 to day i as the
logarithmic return:
This is calculated over the sample space of historical periodic stock
values. The log return values for each series were calculated and
stored in new arrays.
4.4.3 MEAN-VARIANCE CALCULATIONS
Next the variance–covariance matrix was determined over the
collection period. The correlation matrix is used in the representative
implementation, so that was calculated as well. It is understood that
the values of the variance–covariance matrix will change depending
on the sample interval and the number of samples. In this case, the
values were determined over the whole sample period, with daily
samples starting from the beginning of 2012.
4.4.4 IMPLEMENTING THE RISK MEASURE
The risk measures used in this chapter were the correlation and
covariance value between pairs of stocks. Any other suitable risk
measure could be chosen as the acceptance criteria for building the
market graph, as long as it corresponds to a measure between pairs
(for this use of the graph). The correlation is used as an edge
selection threshold: when the correlation between vertices ij >
threshold, then an edge is drawn between the two vertices in the
graph. This generates the market graph: stocks are connected when
their correlation is above some applicable threshold, say 0.5 for
instance. Stocks that do not have edges to other stocks are
\n\n=== OCR PAGE 111 ===\nimplementation and the actual mechanics of programming the
problem to run in the environment.

4.4.2 INPUT DATA

For the implementation, the data were retrieved from Yahoo!Finance
and stored in arrays and structures. The next step calculated the log
return for each of the historical daily returns: measure the day-to-
day return r; of the stock prices (S;) between day i - 1 to day i as the

logarithmic return:
S.
r,=In{ —
Sit

This is calculated over the sample space of historical periodic stock
values. The log return values for each series were calculated and
stored in new arrays.

4.4.3 MEAN-VARIANCE CALCULATIONS

Next the variance—covariance matrix was determined over the
collection period. The correlation matrix is used in the representative
implementation, so that was calculated as well. It is understood that
the values of the variance—covariance matrix will change depending
on the sample interval and the number of samples. In this case, the
values were determined over the whole sample period, with daily
samples starting from the beginning of 2012.

4.4.4 IMPLEMENTING THE RISK MEASURE

The risk measures used in this chapter were the correlation and
covariance value between pairs of stocks. Any other suitable risk
measure could be chosen as the acceptance criteria for building the
market graph, as long as it corresponds to a measure between pairs
(for this use of the graph). The correlation is used as an edge
selection threshold: when the correlation between vertices ij >
threshold, then an edge is drawn between the two vertices in the
graph. This generates the market graph: stocks are connected when
their correlation is above some applicable threshold, say 0.5 for
instance. Stocks that do not have edges to other stocks are
\n\n=== PAGE 112 ===\nconsidered independent (within the threshold). The optimization
results in the maximum number of independent stocks that includes
the best (single) picking from each of the groups of correlated stocks
along with all of the independent (nonconnected) stocks.
For each model, the risk threshold value is varied as risk ∈ {0.35,
0.4, 0.45, 0.5, 0.55, 0.6, 0.65}. The step value of 0.05 was chosen ad
hoc and could be changed to suit the investigation parameters. The
boundary values of the range were chosen on the basis of
observations of the correlation distribution, so that the resulting
market graph would neither be fully connected nor would it be too
sparse or disconnected.
4.4.5 IMPLEMENTATION MAPPING
To solve a problem on the D-Wave system, it must map into the
structure of the hardware. The D-Wave system implementation,
discussed in (Rose, 2008) and (D-Wave, 2013a), pragmatically
cannot provide edges (links) between all of the pairs of qubits—see
Figure 4.1 for the illustrative graph.
\n\n=== PAGE 113 ===\nFIGURE 4.1 Example embedding.
The fundamental hardware connectivity that has been implemented
is described as a primal graph called Chimera. To solve a problem on
the system, a problem must be expressed in a form that is connected
by a graph that can be embedded into the hardware's Chimera graph.
Finding an embedding from non-Chimera into Chimera is in itself a
hard problem. If an embedding cannot be found, the problem cannot
be directly solved in the system.
The hardware available at the time of the investigation was restricted
to problems of 128 variables or less. Larger problems must be broken
\n\n=== OCR PAGE 113 ===\n@@ @@ @® @°®
@@ @02 @8 @8
O® @0 @@ @8
@@ 02 68 68

2@ ee ee eo
ee 6e@ 68 Se
22 @e@ @@ ee
2200 68 ee

©e@¢ ee ee @e@
22 @e €e €e
@© @@ @8 @®@
@0© @€e €8 ©®8

@e@ @8 08 @9
@@ @8 ©8 68
@e@ @@ 98 6®e
@@ 68 68 69

FIGURE 4.1 Example embedding.

The fundamental hardware connectivity that has been implemented
is described as a primal graph called Chimera. To solve a problem on
the system, a problem must be expressed in a form that is connected
by a graph that can be embedded into the hardware's Chimera graph.
Finding an embedding from non-Chimera into Chimera is in itself a
hard problem. If an embedding cannot be found, the problem cannot
be directly solved in the system.

The hardware available at the time of the investigation was restricted
to problems of 128 variables or less. Larger problems must be broken
\n\n=== PAGE 114 ===\ninto subproblems. For this investigation, it was useful that D-Wave
had already determined a subgraph suitable for use in problems with
17 variables or less. An embedding, labeled here as
K17_Q128_EMBEDDING, that maps a 17-variable into 128-qubit,
was already provided/solved by D-Wave. This embedding is adapted
for the 10 stocks scenario used in this chapter—see the structure
called K10_Q128_EMBEDDING in the Matlab code appendix, at the
end of the chapter. The following graph (Figure 4.1), taken from the
D-Wave materials, is illustrative of what the system's qubit
connectivity might look like for a particular hardware
implementation (in a gray scale) and a possible embedding graph
fitting a particular problem (in color):
To implement a WMIS problem, the mapping hi = Jdi − 2ci is used,
as discussed earlier, in which the ci term is the assigned weighting of
the ith vertex.
The optimization of the WMIS problem, as applied to the portfolio
selection process and as formulated in this chapter, yields the
maximum return within the minimum risk threshold for an equally
weighted selection of stocks.
The Matlab code for the WMIS D-Wave application programming
interface implementation is given in Appendix 4.A: WMIS Matlab
Code. Administrative or utility scripts are not detailed here, such as
implementing the actual down-load and pre-processing of data, for
example, ordering.
4.5 Results
A few model variations were considered in the simulation runs. First,
a simple model following the example presented in the D-Wave
developer portal that uses the stock correlations as a risk measure
indicating the level of market diversification. The next model used in
the simulation was closer to the traditional mean-variance
formulation using the variance–covariance matrix but was restricted
by only considering the stock pair interactions with covariance; the
node values remained equal weighted with unit value. The final
version used covariance for the stock interactions, representing the
\n\n=== PAGE 115 ===\nmarket graph edge selection criteria with the addition of the market
graph node weighting represented by the individual stock return
handicapped by its variance.
4.5.1 THE SIMPLE CORRELATION MODEL
A current run of the stock processing routines is summarized in the
following set of graphs (Figure 4.2), with trading-day data from the
beginning of 2012 to the most recent run of the script, which resulted
in 314 samples.
\n\n=== PAGE 116 ===\nFIGURE 4.2 Summary of input data.
The correlations bar graph indicates the pairwise correlation
between each of the 10 stocks: 10 × 10 = 100 correlations. The unit
value correlations are the expected correlation values of the stock
with itself. The result from the correlations and subsequent analysis
or explanation is not the focus here: the correlation is used as
representative of the risk measure. Since the representative set of
stocks that were chosen from the Dow Jones index indicates little to
moderate correlation, a correlation threshold between 0.4 and 0.6
should be sufficient to generate a somewhat sparse market graph
\n\n=== OCR PAGE 116 ===\nStock Prices

Stocks

FIGURE 4.2 Summary of input data.

The correlations bar graph indicates the pairwise correlation
between each of the 10 stocks: 10 x 10 = 100 correlations. The unit
value correlations are the expected correlation values of the stock
with itself. The result from the correlations and subsequent analysis
or explanation is not the focus here: the correlation is used as
representative of the risk measure. Since the representative set of
stocks that were chosen from the Dow Jones index indicates little to
moderate correlation, a correlation threshold between 0.4 and 0.6
should be sufficient to generate a somewhat sparse market graph
\n\n=== PAGE 117 ===\nwhere the edge connectivity is determined by a fixed correlation
threshold acceptance value, such as 0.5. The representative
correlations are shown in Figure 4.3:
FIGURE 4.3 Log-return correlations.
The market graph connecting stocks greater than the initial risk
threshold of 0.35 are given in Figure 4.4.
FIGURE 4.4 Market graph for threshold 0.35.
This figure indicates that pairs of stocks are linked—connected by an
edge when the pair's entry has value of 1—in the market graph when
their respective risk measure is greater than 0.35.
The simulated resulting equally weighted portfolio from the MIS of
this (0.35 threshold) market graph is BAC, IBM, JNJ, JPM, WMT,
and XOM.
As indicated in the earlier bar graph depicting the correlations
between stocks, as the threshold value is raised, the number of stocks
matching that criterion is reduced (to a point where none of them are
correlated at a high enough threshold). This means that the market
\n\n=== OCR PAGE 117 ===\nwhere the edge connectivity is determined by a fixed correlation
threshold acceptance value, such as 0.5. The representative
correlations are shown in Figure 4.3:

BAC csco GE HD IBM JNJ JPM McD WMT XOM
BAC 1 0.3542 0.5287 0.3665 0.3618 0.3131 0.7055 0.2425 0.1826 0.4925
csco 0.3542 1 0.3671 0.2778 = 80.3971 0.3001 0.3307 0.2676 0.0527 0.4426
GE 0.5287 0.3671 1 0.5124 0.4884 0.5026 0.5545 0.3649 0.25 0.6561
HD 0.3665 0.2778 = 0.5124 1 0.396 0.3782 0.3652 0.2685 0.201 0.4916
IBM 0.3618 0.3971 0.4884 0.396 i 0.3779 0.3929 0.333 0.1835 0.5154
JNJ 0.3131 0.3001 0.5026 0.3782 0.3779 1 0.4414 0.3518 0.2598 0.5715
JPM 0.7055 0.3307 0.5545 0.3652 0.3929 0.4414 | 0.2667 0.1638 0.4856
McD 0.2425 0.2676 0.3649 0.2685 0.333 0.3518 0.2667 1 0.1764 0.3896
WMT 0.1826 0.0527 0.25 0.201 0.1835 0.2598 0.1638 0.1764 1 0.2339
XOM 0.4925 0.4426 0.6561 0.4916 0.5154 0.5715 0.4856 0.3896 0.2339 1

FIGURE 4.3 Log-return correlations.

The market graph connecting stocks greater than the initial risk
threshold of 0.35 are given in Figure 4.4.

BAC csco GE HD IBM JNJ JPM McD WMT XOM

BAC 1 1 1 1 1] 1
csco 1 1 1 1
GE 1 1 1 1 1 1 1 1
HD 1 1 1 1 1 1
BM 1 1 1 1 1 1 1
JNJ 1 1 1 1 1 1
JPM 1 1 1 1 1 1
McD 1 1 1
WMT

XOM 1 1 1 1 1 1 1 1

FIGURE 4.4 Market graph for threshold 0.35.

This figure indicates that pairs of stocks are linked—connected by an
edge when the pair's entry has value of 1—in the market graph when
their respective risk measure is greater than 0.35.

The simulated resulting equally weighted portfolio from the MIS of
this (0.35 threshold) market graph is BAC, IBM, JNJ, JPM, WMT,
and XOM.

As indicated in the earlier bar graph depicting the correlations
between stocks, as the threshold value is raised, the number of stocks
matching that criterion is reduced (to a point where none of them are
correlated at a high enough threshold). This means that the market
\n\n=== PAGE 118 ===\ngraph typically becomes sparse as the threshold is increased. For
example, here are the market graphs (Figure 4.5) for a few of the
threshold levels:
FIGURE 4.5 Market graphs for increasing thresholds.
Comparing the market graph representations, at the various
thresholds, the sparseness does increase with increasing threshold.
Conversely, threshold values less than a certain value result in fully
connected market graphs.
With the range of risk ∈ {0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65}, it was
found that the two levels {0.6, 0.65} resulted in the same market
\n\n=== OCR PAGE 118 ===\ngraph typically becomes sparse as the threshold is increased. For

example, here are the market graphs (Figure 4.5) for a few of the
threshold levels:

BAC csco GE HD IBM JNJ JPM mcD WMT XOM

BAC csco GE HD IBM JNJ JPM McD WMT XOM

BAC cSCcO GE HD IBM JNJ JPM McD WMT XOM

FIGURE 4.5 Market graphs for increasing thresholds.

Comparing the market graph representations, at the various
thresholds, the sparseness does increase with increasing threshold.

Conversely, threshold values less than a certain value result in fully
connected market graphs.

With the range of risk € {0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65}, it was
found that the two levels {0.6, 0.65} resulted in the same market
\n\n=== PAGE 119 ===\ngraph.
The final set of portfolios that correspond to each risk setting is
shown in Figure 4.6:
FIGURE 4.6 Portfolios for correlation-based MIS model.
4.5.2 THE RESTRICTED MINIMUM-RISK MODEL
This model is closer to the mean-variance framework except that it
has been restricted. The market graph edge selection criterion is the
threshold range: Threshold = risk ∈ {0.35, 0.4, 0.45, 0.5, 0.55, 0.6,
0.65}. However, to scale it properly, the edge-selection condition
checked is
where the 
. The restricted part of this model is the
use of unit-valued equal weightings on all of the nodes. This
effectively reduces a WMIS graph optimization to an MIS graph
optimization. Also, the individual returns are not used. This could be
considered a minimum-risk portfolio.
Using the same input data set, the bar graph showing the variance of
each stock and the covariance between pairs is shown in Figure 4.72:
\n\n=== OCR PAGE 119 ===\ngraph.

The final set of portfolios that correspond to each risk setting is
shown in Figure 4.6:

Minimum
Risk 0.35 0.4 0.45 0.5 0.55 0.6 0.65
Threshold:
BAC BAC BAC BAC BAC BAC BAC
CSCO csco csco csco csco csco
GE GE GE
HD HD HD HD HD HD
IBM IBM IBM IBM IBM IBM IBM
JNJ JNJ JNJ JNJ JNJ JNJ JNJ
JPM JPM
MCD MCD MCD MCD MCD MCD
WMT WMT WMT WMT WMT WMT WMT
XOM

FIGURE 4.6 Portfolios for correlation-based MIS model.
4.5.2 THE RESTRICTED MINIMUM-RISK MODEL

This model is closer to the mean-variance framework except that it
has been restricted. The market graph edge selection criterion is the
threshold range: Threshold = risk € {0.35, 0.4, 0.45, 0.5, 0.55, 0.6,
0.65}. However, to scale it properly, the edge-selection condition
checked is

Covariance(x;,.x;) > Threshold’ o;o;

where the % = V Variance(x;) The restricted part of this model is the
use of unit-valued equal weightings on all of the nodes. This
effectively reduces a WMIS graph optimization to an MIS graph
optimization. Also, the individual returns are not used. This could be
considered a minimum-risk portfolio.

Using the same input data set, the bar graph showing the variance of
each stock and the covariance between pairs is shown in Figure 4.7?:
\n\n=== PAGE 120 ===\nFIGURE 4.7 Variance-covariance of the input data.
The resulting market graphs for each of the thresholds were the same
as in the correlation model. The final set of portfolios that
correspond to each risk setting is shown in Figure 4.8:
\n\n=== OCR PAGE 120 ===\nVar-Covar

Stocks

FIGURE 4.7 Variance-covariance of the input data.

The resulting market graphs for each of the thresholds were the same
as in the correlation model. The final set of portfolios that
correspond to each risk setting is shown in Figure 4.8:
\n\n=== PAGE 121 ===\nFIGURE 4.8 Portfolios for the restricted MIS model.
Comparing the restricted minimum-risk results with the earlier
correlation-based model, it can be seen that the results were the
same. This is expected, considering that the two models are related
by the variance–covariance scaling, discussed earlier.
4.5.2.1 Comparative classical minimum-risk result
Using MATLAB to run a “classical” minimum variance quadratic
programming optimization (using the MATLAB function quadprog)
on the data, with the restrictions that the weights sum to 1 and the
weights are nonnegative results in the following comparative
portfolio (Figure 4.9):
\n\n=== OCR PAGE 121 ===\nMinimum
Risk 0.35 04 0.45 0.5 0.55 0.6 0.65
Threshold:
BAC BAC BAC BAC BAC BAC BAC
csco CSCO csco Ccsco csco csco
GE GE GE
HD HD HD HD HD HD
IBM IBM IBM IBM IBM IBM IBM
JNJ JNJ JNJ JNJ JNJ JNJ JNJ
JPM JPM

MCD MCD MCD MCD MCD MCD
WMT WMT WMT WMT WMT WMT WMT
XOM

FIGURE 4.8 Portfolios for the restricted MIS model.

Comparing the restricted minimum-risk results with the earlier
correlation-based model, it can be seen that the results were the
same. This is expected, considering that the two models are related
by the variance—covariance scaling, discussed earlier.

4.5.2.1 Comparative classical minimum-risk result

Using MATLAB to run a “classical” minimum variance quadratic
programming optimization (using the MATLAB function quadprog)
on the data, with the restrictions that the weights sum to 1 and the
weights are nonnegative results in the following comparative
portfolio (Figure 4.9):
\n\n=== PAGE 122 ===\nFIGURE 4.9 Classical results with minimum variance model.
The weights that are zero value in the minimum variance classical
results were actually nonzero but much smaller than the significant
digits shown here. Therefore, if very small weightings are allowed in
the portfolio, then it might be possible to consider some combination
of all of the stocks—this is usually not practical, especially once the
cost of holding is built into the model.
The classical result is close to the MIS result for risk threshold 0.35,
with the addition of BAC and XOM in the MIS results. Further
investigation might reveal the subtleties in the difference; however, it
\n\n=== OCR PAGE 122 ===\nBAC 0.0000

csco 0.0000
GE 0.0000
HD 0.0056 HD
IBM 0.0660 IBM
JNJ 0.5858 JNJ
JPM 0.0000
MCD 0.1844 MCD
WMT 0.1582 WMT
XOM 0.0000
Sum: 1.0000
FIGURE 4.9 Classical results with minimum variance model.

The weights that are zero value in the minimum variance classical
results were actually nonzero but much smaller than the significant
digits shown here. Therefore, if very small weightings are allowed in
the portfolio, then it might be possible to consider some combination
of all of the stocks—this is usually not practical, especially once the
cost of holding is built into the model.

The classical result is close to the MIS result for risk threshold 0.35,
with the addition of BAC and XOM in the MIS results. Further
investigation might reveal the subtleties in the difference; however, it
\n\n=== PAGE 123 ===\nis worth repeating that a key differentiator between the MIS method
adopted here and a classical method is that the MIS weights are
modeled as all equally weighted (unit value) whereas the classical
method looks for the best choice of weights that result in the optimal
solution.
4.5.3 THE WMIS MINIMUM-RISK, MAX RETURN MODEL
This model is even closer to the mean-variance framework. Again, it
is somewhat restricted in the sense that it does not account for any
mixed terms for the graph node weights. This could also be
considered an adapted maximum return, minimum-risk model.
Here, the market graph edge selection criterion is the threshold
range, as used in the restricted minimum-risk model, with the same
form. These results are representative of the full WMIS model.
The variance–covariance matrix and all of the resulting market
graphs are the same as in the previous section. Here, the mapping hi
= Jdi − 2ci with ci representing the node's weight, ci = Ri − Vi. As
such, the D-Wave Ising solutions are different for each of the risk
thresholds, which results in a different set of optimal portfolios.
The final set of WMIS portfolios that correspond to each risk setting
is shown in Figure 4.10:
\n\n=== PAGE 124 ===\nFIGURE 4.10 Portfolios under the WMIS model.
4.5.3.1 Comparative classical minimum-risk, max return result
Using MATLAB to run a “classical” maximum return–minimum
variance quadratic programming optimization (using the MATLAB
function quadprog) on the data with the function F = −R + V (as
discussed earlier), also with the restrictions that the weights sum to 1
and the weights are nonnegative results in the following comparative
portfolio (see Figure 4.11):
\n\n=== OCR PAGE 124 ===\nMinimum Risk

Threshold: 0.35 0.4 0.45 0.5 0.55 0.6 0.65
BAC BAC BAC BAC
CSCO csco cSco CcSco csco csco
GE GE GE GE GE
HD HD HD HD HD
IBM IBM IBM IBM
JNJ JNJ JNJ JNJ
JPM

MCD MCD MCD MCD MCD MCD MCD
WMT WMT WMT WMT WMT WMT WMT
XOM XOM

FIGURE 4.10 Portfolios under the WMIS model.

4.5.3.1 Comparative classical minimum-risk, max return result

Using MATLAB to run a “classical” maximum return—minimum
variance quadratic programming optimization (using the MATLAB
function quadprog) on the data with the function F = -R + V(as
discussed earlier), also with the restrictions that the weights sum to 1
and the weights are nonnegative results in the following comparative
portfolio (see Figure 4.11):
\n\n=== PAGE 125 ===\nFIGURE 4.11 Classical results with mean-variance model.
The previous classical result note regarding the weights also applies
here. As also noted in the previous classical comparative results, the
fact that the weights are allowed to vary continuously between 0 and
1 is a key differentiator to the WMIS results that have implicit
(portfolio) equal-weighting.
\n\n=== OCR PAGE 125 ===\nBAC 0.7270 BAC

CcSsco 0.0000
GE 0.0000
HD 0.2730 HD
IBM 0.0000
JNJ 0.0000
JPM 0.0000
MCD 0.0000
WMT 0.0000
XOM 0.0000
Sum: 1.0000
FIGURE 4.11 Classical results with mean-variance model.

The previous classical result note regarding the weights also applies
here. As also noted in the previous classical comparative results, the
fact that the weights are allowed to vary continuously between 0 and
1is a key differentiator to the WMIS results that have implicit
(portfolio) equal-weighting.
\n\n=== PAGE 126 ===\n4.6 Discussion
Simply put, the significant finding here is that a portfolio problem
can be expressed, in a simple fashion, in a graph-theoretic paradigm,
which may then be translated to the language required for
implementation in a quantum computing environment. A binary
decision on the inclusion/exclusion of particular stocks in the
portfolio may be made resulting in a simplified optimal portfolio.
The interpretation of the model and results is quite specific, for the
implementation considered here: the results pertain to a portfolio of
stocks resulting from the optimization of the graph-theoretic MIS or
WMIS. In this context, the results have meaning but might be
challenged by current industry methods on the same set of data,
especially in consideration of financial return.
In relation to previous research, this chapter has enhanced the D-
Wave, 2013b and D-Wave, 2013c by investigating both in a single
setting, while employing Matlab, and considering iterations over a
range of values of the risk variable and incorporating the stock
returns. It presented the quantum computation and financial-centric
references together. As such, it provides a foundation upon which a
financial engineering practitioner may further investigate the merger
of these problem domains.
A couple of experimental observations are noteworthy. First, the
stocks that become independent, that is, without edges connecting
to/from them in the market graph due to the threshold selection, are
always included in the final portfolio. Second, related to the first
point, as the market graph grows sparse, the size of the portfolio
tends to increase. Both of these characteristics are a result of the
underlying maximum independent set combinatorial optimization.
Also, the minimization does not necessarily pick the optimal stock
(as may be determined by other means, for example, the classical
results, discussed above) from the remaining connected subgraphs
when using equal weights on the nodes because the only noteworthy
Ising parameter, in that case, is the value of hi = Jdi − 2, which is
only dependent on the number of edges into that node. For hi = Jdi −
2ci the node weighting, ci, is not the weight of the stock in the
\n\n=== PAGE 127 ===\nportfolio, that is, the proportion of the total portfolio represented by
each asset. The node weight is a characteristic of the fabricated
market graph and is, in the model considered here, dependent on the
stock's return and variance. In both of the MIS and WMIS models
considered here, the optimization only indicates to include (yes) or
not to (no) include the stock in our final portfolio, not the amount of
each stock to include as a proportion of the overall pie—they are
equally weighted, from that point of view. Determining how to
encode the proportion into the formula is one of the areas for further
investigation.
This point is a conjecture: ultimately, it appears that to get a result
with different characteristics, a fundamentally different model would
need to be considered. Most certainly, models and/or mappings that
include the individual returns and portfolio weightings need to be
investigated. Also, a model that maps F more directly to the
underlying Ising problem, that is, without the use of the MIS/WMIS
representation, is worthy of further investigation.
4.6.1 HARDWARE LIMITATIONS
Several limitations abound such as the following: the number of
qubits that can be used is currently 128 (with the current D-Wave
hardware available at the time of the initial investigation); need to
present problems in Chimera representation, that is, reformulate
non-Chimera into Chimera; break up problems larger than can be
represented by Chimera into smaller problems; and finding an
embedding of the problem into the hardware connectivity is a hard
problem in itself. The exploration of a subgraph of Chimera for this
chapter is not specifically undertaken, although it might be an
interesting exercise for further investigation. Each of these is
discussed in D-Wave, 2013a or some of the other D-Wave
programmer's guides, plus also in the study of Ross (Rose, 2008).
These limitations are workable but may fundamentally provide
further complexity to the problem environment, such as: how would
hundreds of stocks be considered, at once, with such constraints?
\n\n=== PAGE 128 ===\n4.6.2 MODEL LIMITATIONS
The WMIS model is not a direct rendition of the classical mean-
variance model because the WMIS model, in the implementation
used here, considers only equal (portfolio) weightings of stocks
whereas the objective of min{F = −R + V} is to find the best mix of
weights that provide the optimal portfolio. To enable a full rendition
of the mean-variance model requires an effort to map the non-
Chimera problem into Chimera, which is beyond the scope of the
current chapter, whereas the WMIS has already been demonstrated
in this computational framework.
Also, the implementation of WMIS used here does not take into
account the mixed terms contributing to the node weights, such as
seen in the analytic expansion of min{F}. A naïve mapping is used in
this chapter that maps min{F = −R + V} onto the WMIS, which
values the market graph node weights as the composite values − Ri +
Vi, with Ri representing the return of the ith stock and Vi as its
variance.
Thus, the node weight is the stock's return handicapped by its
variance: a large variance reduces the return on the stock and thus
lowers its chance of being selected out of a connected set of stocks.
This brings to light one of the characteristics of the MIS or WMIS
model: the final optimization takes place over the remaining edge-
connected set of nodes. The independent, nonconnected nodes will
always be selected as part of the final set.
4.6.3 IMPLEMENTATION LIMITATIONS
The D-Wave system is in constant use and under scheduling
demand. Unfortunately, the implementation investigated in this
chapter was only able to make use of the D-Wave simulator.
Therefore, all results using the D-Wave simulator must be
considered to be manifestations of the underlying software
implementation. However, the mechanics and background material
are still relevant and future implementations could simply replace
the software simulator with a hardware connection.
As such, the results herein are not representative of any time
reduction in finding a solution and no attempt is made to provide
\n\n=== PAGE 129 ===\nsuch a comparison. The performance characteristics of the D-Wave
equipment were experimentally explored and reported in
comparison to conventional software solvers (McGeoch and Wang,
2013). As mentioned in the “Background” section, the D-Wave
hardware system shows speed-up thousands of times better than
current software implementations, especially in QUBO, which is
relevant to this financial model investigation.
4.6.4 FUTURE RESEARCH
A future consideration is the change of risk measure (Gilli and
Schumann, 2012), for instance, optimizing the portfolio's VaR, which
may not be solvable via classical methods if it does not yield to
reformulation of the objective function.
Weighted maximum independent set: Other selection criteria or
formulations could be chosen for the weight, such as VaR of the
holding, and would be suitable as future investigations. It will be a
useful future exercise to further characterize a version of this model
and compare the results against current techniques.
Recommendations for future research include but are not limited to
further analytical representation of the portfolio problem in the
language of Boolean satisfiability; finding a model representation
that determines the weightings of stocks in the resulting portfolio
optimization; investigation of more complex representations of the
selection criteria for the vertex weights and edge determination, such
as inclusion of min/max weight boundaries or inclusion of trading or
carrying costs, etc.; extension of the problem to further financial
domains, such as derivatives, fixed income instruments, or credit
risk analysis. The average values used in this implementation were
over the full sample period: perhaps other more typical sample
periods and averaging (e.g., weighted or log decay) techniques would
be appropriate for further investigation.
Since optimization methods such as heuristics do provide a way to
solve similar problems using commercially available computers and
applications, one of the most interesting aspects for future research
would be to classify the boundary between what type and size of
financial problem can be left to the current classical environments
versus what type and size of problem should be solved in the new
\n\n=== PAGE 130 ===\nparadigm. It is also possible to reconsider the problem
representations, investigated here, in QUBO form, which may be
easier to understand and compare with other research with QUBOs.
As well, a more general QUBO form may yield more flexibility
compared to the specific MIS/WMIS representations considered in
this chapter.
An investigation into the comparative financial performance of these
models should be undertaken. That is, do the classical models and
environments yield better or worse financial return, over certain
timeframes, when compared with the implementation considered in
this chapter?
Notably, the analysis herein has been with respect to historical
returns. Indeed, the application of time-series models in this
paradigm as a forecasting tool would be an interesting next step.
Generally, if the stock returns were generated with a suitable
forecasting model, for example, GARCH, then this environment
would provide the best portfolio within the forecasting constraints.
The binary decision aspect of the technique is enticing to the extent
that it provides a fairly simple means of determining a yes or no
answer as to a stock's inclusion in a portfolio. The remaining, key,
and unanswered question is how to determine the actual weightings
of each holding within the portfolio: the method used here implicitly
considered equal unit-weighting, that is, to purchase the same
number of units of each stock to form the portfolio. Since this
framework yields binary results, a method to determine the
weightings (not to be confused with the weights attributed with each
market graph node) would have to be concocted. A conjecture might
be some sort of polynomial form involving the known characteristic
variables, such as the number of edges connecting to a node, the
variance and return attributed to each node, the covariance edge
values, etc. This leads to possible multilinear polynomial and
posiform representations of a pseudo-Boolean function to represent
the weights. Some insight into this area but in a wider optimization
context can be seen in the study by Boros and Hammer, 2002. Yet,
this is another area for future work.
\n\n=== PAGE 131 ===\n4.7 Conclusion
This computational environment itself holds great promise toward
solving hard financial engineering problems. However, the MIS and
WMIS models, as investigated in this chapter, may have limited
appeal. Further analytical and experimental investigation is required
to improve upon the existing models or to characterize other viable
alternative models.
Acknowledgments
The author thanks D-Wave Systems Inc. for allowing to investigate
this problem domain with the use of their simulation software.
Zhengbing Bian (D-Wave) has been particularly valuable at helping
me through the process of engaging D-Wave and in providing
constructive feedback.
Thanks also go to supervising Professor David Starer, Stevens
Institute of Technology, for his encouragement to take on this
investigation.
Appendix 4.A: WMIS Matlab Code
%%%   iteratedDWaveRuns_MeanVar_WMIS_compositeF
%
%   PURPOSE: Perform iterations over the stock data using the 
D-Wave
%            hardware optimization engine.
%            Use Mean-Variance with WMIS optimizing F.
%   Author: Michael Marzec, Stevens Institute of Technology
  
% the filename containing our stored data…
loadfile = 'myHistoricalStockData-20130408.mat';
  
% load the data from the file
load(loadfile, 'histData');
dataSize = size(histData(1).adjClse); % number of stocks 
retrieved
  
% Vector of our stock names…
% use Cell Array '{}'
Stocks = { …
\n\n=== PAGE 132 ===\n    histData(1).symbol{1}; …
    histData(2).symbol{1}; …
    histData(3).symbol{1}; …
    histData(4).symbol{1}; …
    histData(5).symbol{1}; …
    histData(6).symbol{1}; …
    histData(7).symbol{1}; …
    histData(8).symbol{1}; …
    histData(9).symbol{1}; …
    histData(10).symbol{1} …
    };
  
% -------------------------------------------------
% create a single Prices matrix from our structures
% -------------------------------------------------
% For each column…
for k = 1:10
    % For each row…
    for i = 1:size(histData(k).adjClse)
        Prices(i,k) = histData(k).adjClse(i);
    end
end
 
% ---------------------------------
% Need to calculate the log returns
% ---------------------------------
logReturns = diff(log(Prices));
 
% Average log-return for each stock
avgLogReturns = mean(logReturns,1);
  
% -------------------------------------------------------------
----
% Calculate the Variance-Covariance between pairs of stock log 
returns…
% -------------------------------------------------------------
----
% Need the var/covar matrix…
covLogReturns = cov(logReturns);
 
% …we'll use the Variance of the logReturns, as well…
varLogReturns = var(logReturns);
 
% …also need the stdDev
stdDevLogReturns = sqrt(varLogReturns);
  
% a visualization of the Var-Covar…
bar(covLogReturns), xlabel('Stocks'), ylabel('Var-Covar'), 
\n\n=== PAGE 133 ===\nlegend(Stocks);
  
% ------------------------------------------
% Static variables/set-up for the D-Wave run
%
% Specify the embedding list…
% Adapted from D-Wave's K17_Q128_EMBEDDING
% ------------------------------------------
K10_Q128_EMBEDDING = cell(1,10);
K10_Q128_EMBEDDING{1} = [0 4 12 20 28];
K10_Q128_EMBEDDING{2} = [1 5 13 21 29];
K10_Q128_EMBEDDING{3} = [2 6 14 22 30];
K10_Q128_EMBEDDING{4} = [3 7 15 23 31];
K10_Q128_EMBEDDING{5} = [8 40 44 52 60];
K10_Q128_EMBEDDING{6} = [9 41 45 53 61];
K10_Q128_EMBEDDING{7} = [10 42 46 54 62];
K10_Q128_EMBEDDING{8} = [11 43 47 55 63];
K10_Q128_EMBEDDING{9} = [16 48 80 84 92];
K10_Q128_EMBEDDING{10} = [17 49 81 85 93];
  
% The J parameter used here; must be any real number > 1
% Note: this re-uses the normJ from the equal-weight example
% Actual condition for selecting normJ is: normJ > min{c_i, 
c_j}
normJ = 1.1;
  
 
% -------------------------------------------------------------
---
% Here is the connectivity to the local or remote D-Wave 
solvers…
% -------------------------------------------------------------
---
% Get a connection to the local solver…
conn=sapiLocalConnection();
% Get a list of solvers available locally…
solvers=sapiListSolvers(conn);
% Print the list of solvers available…
solvers
  
% create a handle to a particular solver
solver = sapiSolver(conn,'c4-sw_optimize');
  
% Create the embedding solver
embeddingSolver = 
sapiEmbeddingSolver(solver,K10_Q128_EMBEDDING);
  
% ---------------------------------------------------
\n\n=== PAGE 134 ===\n% Re-used parameters from correlation-based runs:
% Iteration over various correlation threshold values
% from correlation = 0.35 to 0.65 by 0.05
% ---------------------------------------------------
  
% ------------------------------------------
% Generate a market graph…
% ------------------------------------------
isingSolutions = zeros(length(covLogReturns), 7);
  
% Init the optimumPortfolios array; this is a fast way - not 
the best.
optimumPortfolios = cell(10,7);
  
loopCount = 1;
for cThresh = 0.35 : 0.05 : 0.65
  
    % correlations are used to set up the graph
    marketGraph = covLogReturns;
  
    % iterate through our initial graph and determine Edges
    for m = 1:length(marketGraph)
        % determine connectivity using our threshold
        for n = 1:length(marketGraph)
            % connect or not connect pairs based on threshold
            if m==n
                marketGraph(m,n) = 0;
            elseif abs(marketGraph(m,n)) > 
cThresh*stdDevLogReturns(m)
                *stdDevLogReturns(n)
                marketGraph(m,n) = 1;
            else
                marketGraph(m,n) = 0;
            end
        end
    end
    % print the market graph for each iteration
    cThresh
    marketGraph
  
    % count the number of edges for each stock node
    edgeCount = sum(marketGraph, 2);
    % ---------------------------------------------------
    % Calculate the Ising parameters…
    % Note: Here, node weights are the (return - variance)
    % values for each stock, ie node.
    % ---------------------------------------------------
    % the h_ij values
\n\n=== PAGE 135 ===\n    for p = 1:length(edgeCount)
        % This uses the variance handicapped returns expressed 
in F
        hValues(p) = normJ*edgeCount(p)-2*(avgLogReturns(p)-
varLogReturns(p));
    end
  
    % the J_ij values
    JValues = normJ*marketGraph;
  
    % Solve the Chimera-structured Ising problem
    answerIsing = sapiSolveIsing(embeddingSolver,hValues, 
JValues);
  
    for t = 1:length(answerIsing.solutions)
        if answerIsing.solutions(t) == 1
            %optimumPortfolios(t,loopCount) = Stocks(t);
            optimumPortfolios(t,loopCount) = 
java.lang.String(Stocks(t));
        else
            optimumPortfolios(t,loopCount) = 
java.lang.String('');
        end
    end
  
    % add to a matrix of solutions
    isingSolutions(:,loopCount) = answerIsing.solutions;
    loopCount = loopCount + 1;
    %gplot(marketGraph, [1 1; 2 4; 3 2; 4 8; 5 5; 6 10; 7 7; 8 
3; 9 9; 10 6], '-*')
end
  
% print the optimum portfolios
optimumPortfolios
% ------------
% THE END
% ------------
References
Akrotirianakis, I., and B. Rustem. Globally convergent interior-point
algorithm for nonlinear programming. Journal of Optimization
Theory and Applications, 2005: v125n3: 497–521.
Bartholomew-Biggs, M. Nonlinear optimization with financial
applications. Boston, MA: Kluwer Academic Publishers, 2005.
\n\n=== PAGE 136 ===\nBertsimas, D., C. Darnell, and R. Soucy. Portfolio construction
through mixed-integer programming at Grantham, Mayo, Van
Otterloo and Company. Interfaces, 1999: 29: 49–66.
Bian, Z., F. Chudak, W. Macready, and G. Rose. The Ising model:
Teaching and old problem new tricks. D-Wave Systems, 2010.
Boginski, V., S. Butenko, and P. M. Pardalos. Mining market data: A
network approach. Elsevier Ltd., 2005.
Boixo, S., T. Albash, F. Spedalieri, N. Chancellor, and D. Lidar.
Experimental signature of programmable quantum annealing.
University of Southern California, 2012.
Boros, E., and P. Hammer. Pseudo-boolean optimization. Discrete
Applications in Applied Mathematics, 2002: 123: 155–225.
Boros, E., P. L. Hammer, R. Sun, and G. Tavares. A max-flow
approach to improved lower bounds for quadratic unconstrained
binary optimization (QUBO). Discrete Optimization, 2008: 5(2):
501–529.
Charpin, F., and D. Lacaze. Using binary variables to obtain small
optimal portfolios. The Journal of Portfolio Management, 2007:
Fall: 68–72.
Chaves, D., J. Hsu, F. Li, and O. Shakernia. Risk parity portfolio vs.
other asset allocation heuristic portfolios. Research Affiliates, LLC,
2010.
Chen, W., and L. Zhang. Global optimality conditions for quadratic
0-1 optimization problems. Journal of Global Optimization, 2010:
46(2): 191–206.
Choi, V. Adiabatic quantum algorithms for the NP-complete
maximum-weight independent set, exact cover and 3SAT problems.
Falls Church, VA: Department of Computer Science, Virginia Tech,
2010.
Choi, V. Minor-embedding in adiabatic quantum computation: I.
The parameter setting problem. D-Wave Systems Inc., 2008.
Cormen, T., C. Leiserson, R. Rivest, and C. Stein. Introduction to
algorithms. 3rd ed. Cambridge, MA: MIT Press, 2009.
Dang, C., and L. Xu. A barrier function method for the nonconvex
quadratic programming problem with box constraints. Journal of
Global Optimization, 2000: 18: 165–188.
D-Wave. Programming with QUBOs. Programmer's Guide. Burnaby,
British Columbia (BC), Canada: D-Wave Systems Inc., 2013a.
\n\n=== PAGE 137 ===\nD-Wave. Quantitative finance tutorial: optimizing portfolios. D-
Wave Developer Portal. Burnaby, British Columbia (BC), Canada:
D-Wave Systems Inc., 2013b. http://www.dwavesys.com/en/dev-
tutorial-finance.html (accessed January 2013).
D-Wave. Solving weighted maximum independent set using D-wave
blackbox. D-Wave Developer Portal. Burnaby, British Columbia
(BC), Canada: D-Wave Systems Inc., 2013c.
http://www.dwavesys.com/en/dev-tutorial-wmis.html.
Elton, E., M. Gruber, S. Brown, and W. Goetzmann. Modern
portfolio theory and investment analysis. John Wiley & Sons, Inc.,
2007.
Fabozzi, F. J., H. M. Markowitz, P. N. Kolm, and F. Gupta. Mean-
variance model for portfolio selection. In Encyclopedia of Financial
Models, by Frank J. Fabozzi 2013. http://onlinelibrary.wiley.com/
doi/10.1002/9781118182635.efm0003/full.
Farhi, E., J. Goldstone, S. Gutmann, and M. Sipser. Quantum
computation by adiabatic evolution. 2000.
http://arxiv.org/pdf/quant-ph/0001106.pdf.
Floudas, C., and C. Gournaris. A review of recent advances in global
optimization. Journal of Global Optimization, 2009: 45: 3–38.
Gilli, M., and E. Schumann. Heuristic optimisation in financial
modelling. Annals of Operations Research, 2012: 193: 129–158.
Gilli, M., D. Maringer, and E. Schumann. Numerical methods and
optimization in finance. Waltham, MA: Academic Press, Elsevier
Inc., 2011.
Hillier, F., and G. Lieberman. Introduction to operations research,
9th ed. New York: McGraw-Hill, 2010.
Huang, K. Y., C.-J. Jane, and T.-C. Chang. An enhanced approach to
optimizing the stock portfolio selection based on Modified
Markowitz MV Method. Journal of Convergence Information
Technology, 2011: 6(2): 226–239.
Jallo, D., and D. Budai. The Market Graph: A study of its
characteristics, structure and dynamics. Master's thesis. The Royal
Institute of Technology, December 2010.
Kozen, D.. The design and analysis of algorithms. New York:
Springer-Verlag, 1992.
Krokhmal, P., J. Palmquist, and S. Uryasev. Portfolio optimization
with conditional value-at-risk objective and constraints. The Journal
\n\n=== PAGE 138 ===\nof Risk, 2002: 4(2): 11–27
http://www.ise.ufl.edu/uryasev/files/2011/11/kro_CVaR.pdf.
Maringer, Dr. Risk preferences and loss aversion in portfolio
optimization. Colchester, UK: CCFEA, University of Essex, 2006.
Markowitz, H. Portfolio selection. The Journal of Finance, 1952:
7(1): 77–91.
McGeoch, C., and C. Wang. Experimental evaluation of an adiabatic
quantum system for combinatorial optimization. In: Conference on
Computing Frontiers, ACM, 2013.
Palubeckis, G. Multistart tabu search strategies for the
unconstrained binary quadratic optimization problem. Annals of
Operations Research, 2004: 131: 259–282.
Papadimitriou, C., and K. Steiglitz. Combinatorial optimization:
Algorithms and complexity. Mineola, NY: Dover Publications, 1998.
Pardalos, P. M., O. A. Prokopyev, O. V. Shylo, and V. P. Shylo. Gobal
equilibrium search applied to the unconstrained binary quadratic
optimization problem. Optimization Methods and Software, 2008:
23(1): 129–140.
Rose, G. The 128-qubit Rainier chip: II. Graph embedding. 2008.
http://dwave.wordpress.com/2008/10/23/the-128-qubit-rainier-
processor-ii-graph-embedding/.
Shaw, D., S. Liu, and L. Kopman. Lagrangian relaxation procedure
for cardinality-constrained portfolio optimization. Optimization
Methods and Software, 2008: June: 411–420.
Soleimani, H., H. R. Golmakani, and M. H. Salimi. Markowitz-based
portfolio selection with minimum transaction lots, cardinality
constraints and regarding sector capitalization using genetic
algorithm. Expert Systems With Applications, 2009: 5058–5063.
Notes
1 An alternative composite function, not investigated in this chapter,
is the risk–return ratio 
.
2 In the graph, the x-axis numbers correspond to the list of stock
names, that is, BAC = 1, CSCO = 2, …
\n\n=== OCR PAGE 138 ===\nof Risk, 2002: 4(2): 11-27
http://www.ise.ufl.edu/uryasev/files/2011/11/kro_CVaR.pdf.
Maringer, Dr. Risk preferences and loss aversion in portfolio
optimization. Colchester, UK: CCFEA, University of Essex, 2006.
Markowitz, H. Portfolio selection. The Journal of Finance, 1952:
7(4): 77-91.

McGeoch, C., and C. Wang. Experimental evaluation of an adiabatic
quantum system for combinatorial optimization. In: Conference on
Computing Frontiers, ACM, 2013.

Palubeckis, G. Multistart tabu search strategies for the
unconstrained binary quadratic optimization problem. Annals of
Operations Research, 2004: 131: 259-282.

Papadimitriou, C., and K. Steiglitz. Combinatorial optimization:
Algorithms and complexity. Mineola, NY: Dover Publications, 1998.
Pardalos, P. M., O. A. Prokopyev, O. V. Shylo, and V. P. Shylo. Gobal
equilibrium search applied to the unconstrained binary quadratic
optimization problem. Optimization Methods and Software, 2008:
23(1): 129-140.

Rose, G. The 128-qubit Rainier chip: II. Graph embedding. 2008.
http://dwave.wordpress.com/2008/10/23/the-128-qubit-rainier-
processor-ii-graph-embedding/.

Shaw, D., S. Liu, and L. Kopman. Lagrangian relaxation procedure
for cardinality-constrained portfolio optimization. Optimization
Methods and Software, 2008: June: 411-420.

Soleimani, H., H. R. Golmakani, and M. H. Salimi. Markowitz-based
portfolio selection with minimum transaction lots, cardinality
constraints and regarding sector capitalization using genetic
algorithm. Expert Systems With Applications, 2009: 5058-5063.

Notes

+ An alternative composite function, not investigated in this chapter,

is the risk—return ratio * = 2.

2 In the graph, the x-axis numbers correspond to the list of stock
names, that is, BAC = 1, CSCO = 2, ...
\n\n=== PAGE 139 ===\nChapter Five
Estimation Procedure for Regime Switching
Stochastic Volatility Model and Its
Applications
Ionut Florescu and Forrest Levin
Financial Engineering Division, Stevens Institute of Technology,
Hoboken, NJ, USA
5.1 Introduction
Consider a continuous time stochastic signal whose variability is
constant. However, the variability changes its value at random times.
We may view this behavior of the variability as a regime switching
model or a hidden Markov chain model. The purpose of the present
work was to introduce a new methodology based on particle-filtering
techniques to estimate the behavior of the variability. We then study
applications.
The structure of this chapter is as follows. The remainder of this
section gives an overview of the model and existing literature.
Section 5.2 describes the main methodology employed in this
chapter. Section 5.3 contains the results and conclusions obtained
when using the estimating methodology with real data from two
different areas: finance (Section 5.3.1) and geophysics (5.3.3).
Section 5.4 concludes the chapter. Appendix 5.A.4 contains some of
the proofs.
5.1.1 THE ORIGINAL MOTIVATION
The original motivation of the work comes from the world of
Finance. Today, it is well accepted in the financial literature that the
Black–Scholes model (Black and Scholes, 1973) is not complex
enough to capture the dynamics of data sampled with high frequency
(e.g., see Mariani et al., 2009). Various other models extending the
original one have been proposed. We mention here stochastic
\n\n=== PAGE 140 ===\n(5.1)
volatility models (Shephard, 2005), jump diffusion models (Merton,
1992), and general Lévy models (Cont and Tankov, 2003).
In this chapter, we present a model where the variability of the signal
is driven by a continuous time Markov chain. In the financial
literature, the model we analyze here is called a regime switching
volatility model, or a stochastic volatility model with volatility driven
by a hidden Markov chain (Hamilton and Lin, 1996; Hamilton,
2005; Chib et al., 2004). While regime switching (in the drift) is
prevalent in science and engineering, to our knowledge, the volatility
switching model has not been applied outside finance literature.
5.1.2 THE MODEL AND THE PROBLEM
We assume as given a complete probability space 
. On this
space, we are given a complete filtration 
 (see Protter,
2005, p. 3). We consider a continuous time signal S, which satisfies
the following stochastic differential equation:
where r is known, Wt is a standard Brownian motion with respect to
the filtration 
 defined on the original probability space, and Yt is
modeled as a continuous time Markov chain. The Markov chain has
finite state space {a1, …, ap}, transition probability matrix Λ = (λij),
and the transition times given by exponential random variables with
parameter λi, that is,
In order for the stochastic differential equation (5.1) to make sense,
we assume that both processes S and Y are adapted to the filtration
. We note that the process Y is not directly observable, this process
will be referred as the “hidden factor” or the “hidden Markov
process.”
Our goal is to estimate of the number of states p, the values of the
states {a1, …, ap}, as well as the transition rates λi and the transition
\n\n=== OCR PAGE 140 ===\nvolatility models (Shephard, 2005), jump diffusion models (Merton,
1992), and general Lévy models (Cont and Tankov, 2003).

In this chapter, we present a model where the variability of the signal
is driven by a continuous time Markov chain. In the financial
literature, the model we analyze here is called a regime switching
volatility model, or a stochastic volatility model with volatility driven
by a hidden Markov chain (Hamilton and Lin, 1996; Hamilton,
2005; Chib et al., 2004). While regime switching (in the drift) is
prevalent in science and engineering, to our knowledge, the volatility
switching model has not been applied outside finance literature.

5.1.2 THE MODEL AND THE PROBLEM

We assume as given a complete probability space (Q,F,P), On this
space, we are given a complete filtration % = {*;}: (see Protter,
2005, p. 3). We consider a continuous time signal S, which satisfies
the following stochastic differential equation:

t t
S,=S)+ [ rS,du + [ s,¥,dW,,.
0 0

where r is known, W, is a standard Brownian motion with respect to
the filtration F defined on the original probability space, and Y, is
modeled as a continuous time Markov chain. The Markov chain has
finite state space {q,, ..., ap}, transition probability matrix A = (Aj),
and the transition times given by exponential random variables with
parameter );, that is,

(5.1)

PLY, =i,Vs € (t,¢+ullY, =) =e*".

In order for the stochastic differential equation (5.1) to make sense,
we assume that both processes S and Y are adapted to the filtration
F. We note that the process Y is not directly observable, this process
will be referred as the “hidden factor” or the “hidden Markov
process.”

Our goal is to estimate of the number of states p, the values of the
states {a,, ..., dp}, as well as the transition rates ); and the transition
\n\n=== PAGE 141 ===\nprobability matrix Λ based on a discrete sample 
 of
the observed signal St. We note here, since we wish to apply the
model to fields other than Finance, we do not use any other input
other than a history of the process S. Specifically, we do not rely on
options or futures or indeed any derivative data.
5.1.3 A BRIEF HISTORICAL NOTE
As already mentioned, the literature on the model presented in
equation (5.1) can be found in the finance area. In finance, the
problem of estimating parameters of the model (5.1) was studied in
the 1990s; however, either the number of states and the actual state
values are given a priori (Hamilton and Lin, 1996), or the estimation
technique uses specific financial derivative data such as call and put
options (Barndorff-Nielsen and Shephard, 2002).
We discuss in more detail two techniques that are related to our
methodology. The first technique performing a full estimation
technique using only the observed signal may be found in the studies
by Cvitanić, Liptser, and Rozovskii (2006) and Cvitanić, Rozovskii,
and Zaliapin (2006). The first article presents the theoretical results,
while the second implements the theory and presents numerical
results. Our methodology is different in the following aspects. While
the model used is similar, the estimation methodology we propose is
different. Specifically, the work cited uses a sequential Bayes
methodology designed for the particular model under consideration.
This allows the authors to obtain specific convergence rates for their
algorithm. Furthermore, the analysis devoted to the number of states
and the specific values of the states of the hidden Markov chain is
somewhat ad hoc and is performed by visual inspection of the results
given by the Bayesian filter. In our work, we use a more general
particle-filtering method; however, we are not capable of obtaining
precise convergence rates without making extra assumptions on the
model. Our methodology is capable of recognizing the correct
number of states using a mechanical approach easy to implement in
a computer.
A second technique that we see related to our methodology is
provided by Genon-Catalot, Jeantheau, and Larédo (2000). This
article culminates a series of three earlier papers dedicated to
\n\n=== OCR PAGE 141 ===\nprobability matrix A based on a discrete sample 181,551,929 5s,) of
the observed signal S,. We note here, since we wish to apply the
model to fields other than Finance, we do not use any other input
other than a history of the process S. Specifically, we do not rely on
options or futures or indeed any derivative data.

5.1.3 A BRIEF HISTORICAL NOTE

As already mentioned, the literature on the model presented in
equation (5.1) can be found in the finance area. In finance, the
problem of estimating parameters of the model (5.1) was studied in
the 1990s; however, either the number of states and the actual state
values are given a priori (Hamilton and Lin, 1996), or the estimation
technique uses specific financial derivative data such as call and put
options (Barndorff-Nielsen and Shephard, 2002).

We discuss in more detail two techniques that are related to our
methodology. The first technique performing a full estimation
technique using only the observed signal may be found in the studies
by Cvitani¢é, Liptser, and Rozovskii (2006) and Cvitanié, Rozovskii,
and Zaliapin (2006). The first article presents the theoretical results,
while the second implements the theory and presents numerical
results. Our methodology is different in the following aspects. While
the model used is similar, the estimation methodology we propose is
different. Specifically, the work cited uses a sequential Bayes
methodology designed for the particular model under consideration.
This allows the authors to obtain specific convergence rates for their
algorithm. Furthermore, the analysis devoted to the number of states
and the specific values of the states of the hidden Markov chain is
somewhat ad hoc and is performed by visual inspection of the results
given by the Bayesian filter. In our work, we use a more general
particle-filtering method; however, we are not capable of obtaining
precise convergence rates without making extra assumptions on the
model. Our methodology is capable of recognizing the correct
number of states using a mechanical approach easy to implement in
a computer.

A second technique that we see related to our methodology is
provided by Genon-Catalot, Jeantheau, and Larédo (2000). This
article culminates a series of three earlier papers dedicated to
\n\n=== PAGE 142 ===\n(5.2)
parameter estimation for stochastic volatility models. In the cited
article, the authors use a filtering methodology similar to our
approach to a more general stochastic volatility model. The process
Yt in the cited work is a stationary continuous time and continuous
state space process. After using the particle filter, the authors use a
method of moments estimation for the parameters. In our context
when Yt has a discrete distribution translating this method would
need special care since there are multiple discrete values that provide
the same moments up to many orders. This is why, we use a different
estimation technique based on our specific discrete model.
For completeness, we mention earlier work dedicated to estimating
parameters of stochastic volatility models from discrete data. Even
though these papers are using models different from our work, they
were a great source of inspiration. Nielsen and Vestergaard (2000),
Sorensen (2003), Bladt and Sorensen (2007), and Aït-Sahalia and
Kimmel (2007) are some of these important references for us.
5.2 The methodology
The theoretical foundation of the estimating methodology is
extending fundamental work in Del Moral, Jacod, and Protter
(2001). We note that because of the specific form of the process St in
(5.1), we may consider the process Xt = log St. This process has a
simplified dynamic as an application of the Itô’s lemma shows:
The Yt process remains unmodified under this transformation and
the observations are now 
 with 
.
We use a particle-filtering methodology on the basis of the study by
Del Moral et al. (2001) to estimate the distribution of the volatility
process {Yt}t at time t. We then use the theoretical distribution of Yt
to find parameter values that allows the best match with the
estimated distribution. This estimation methodology is described in
the next two sections.
\n\n=== OCR PAGE 142 ===\nparameter estimation for stochastic volatility models. In the cited
article, the authors use a filtering methodology similar to our
approach to a more general stochastic volatility model. The process
Y, in the cited work is a stationary continuous time and continuous
state space process. After using the particle filter, the authors use a
method of moments estimation for the parameters. In our context
when Y, has a discrete distribution translating this method would
need special care since there are multiple discrete values that provide
the same moments up to many orders. This is why, we use a different
estimation technique based on our specific discrete model.

For completeness, we mention earlier work dedicated to estimating
parameters of stochastic volatility models from discrete data. Even
though these papers are using models different from our work, they
were a great source of inspiration. Nielsen and Vestergaard (2000),
Sorensen (2003), Bladt and Sorensen (2007), and Ait-Sahalia and
Kimmel (2007) are some of these important references for us.

5.2 The methodology

The theoretical foundation of the estimating methodology is
extending fundamental work in Del Moral, Jacod, and Protter
(2001). We note that because of the specific form of the process S, in

(5.1), we may consider the process X; = log S;. This process has a
simplified dynamic as an application of the It6’s lemma shows:

t y2 t 2
X,=Xp +f (: - =) +/ Y dW, (5.2)
0 2 0

The Y, process remains unmodified under this transformation and
the observations are now (x, geseoe Xt, } with “4, = log St,

We use a particle-filtering methodology on the basis of the study by
Del Moral et al. (2001) to estimate the distribution of the volatility
process {Y;}, at time t. We then use the theoretical distribution of Y,
to find parameter values that allows the best match with the
estimated distribution. This estimation methodology is described in
the next two sections.
\n\n=== PAGE 143 ===\n5.2.1 OBTAINING FILTERED EMPIRICAL DISTRIBUTIONS
AT t1, …, tT
Suppose that we are looking at the process evolution over the
interval [ti − 1, ti). We are given the two endpoints 
, and a
previous estimate of the distribution of the volatility process 
,
denoted by Φi − 1. We want to estimate the distribution of the
volatility process at ti: 
. To proceed, we need to choose the
following basic elements:
A weighting function φ( · ). This would be called a kernel function
in other areas of statistics (any nonnegative function with L1
norm equal to 1), but for practical consideration it has to have
most of the mass centered around zero. The kernel does not need
to be symmetric, though in practical applications it always is. In
practice, we normally do not know if we should penalize more if
we under- or overestimate parameters; thus, we always choose a
symmetric φ. In our applications, we typically use one of the
triangular, Epanechnikov, or Gaussian kernels. The choice used
has typically little effect on the estimation.
A number n that represents the number of intermediate paths for
the filter. This number is typically large.
A number m that represents the number of intermediate points
within the interval [ti − 1, ti). For theoretical convergence reasons,
we need to have 
.
With these basic elements, the filtering methodology consists in two
steps: an evolution step and a selection step.
5.2.1.1 Evolution step
In this step of the algorithm, we create n paths from ti − 1 to ti, using
the following process evolution. Take 
 and draw a value Yi
− 1 from the previously estimated distribution Φi − 1. Each evolution
path j is then constructed using a simple Euler scheme:
\n\n=== OCR PAGE 143 ===\n5.2.1 OBTAINING FILTERED EMPIRICAL DISTRIBUTIONS
AT ty, ..., t7

Suppose that we are looking at the process evolution over the
interval [t;_,, t;). We are given the two endpoints *4.°*s, and a

previous estimate of the distribution of the volatility process Ye,

denoted by ®; _ ,. We want to estimate the distribution of the

volatility process at t;: Y,. To proceed, we need to choose the
following basic elements:

¢ A weighting function g(- ). This would be called a kernel function
in other areas of statistics (any nonnegative function with L*
norm equal to 1), but for practical consideration it has to have
most of the mass centered around zero. The kernel does not need
to be symmetric, though in practical applications it always is. In
practice, we normally do not know if we should penalize more if
we under- or overestimate parameters; thus, we always choose a
symmetric @. In our applications, we typically use one of the
triangular, Epanechnikov, or Gaussian kernels. The choice used
has typically little effect on the estimation.

A number n that represents the number of intermediate paths for
the filter. This number is typically large.

A number m that represents the number of intermediate points

within the interval [t; _ ,, t;). For theoretical convergence reasons,
3
we need to have” 2 Vn.

With these basic elements, the filtering methodology consists in two
steps: an evolution step and a selection step.

5.2.1.1 Evolution step

In this step of the algorithm, we create n paths from t; _ , to t;, using
tt)

the following process evolution. Take Aim = m and drawa value Y;
_, from the previously estimated distribution ®;_ ,. Each evolution
path j is then constructed using a simple Euler scheme:
\n\n=== PAGE 144 ===\n(5.3)
where ΔWj
k and ΔZj
k represent the increments of independent
Brownian motions [i.e., they are i.i.d. N(0, Δ2
i, m)]. β is a parameter
of the algorithm. At the end of each evolution path, we keep the pairs
, j = 1, …, n.
5.2.1.2 Selection step
The idea of this step is to give greater weights to the “better” values
obtained. Specifically, we assign a weight to each final value 
 equal
to:
where C(n) is a normalizing constant that makes the weights sum to
1.
Our approximating distribution at the end of step i is the discrete
distribution 
. We denote this distribution Φi. The
selection step is equivalent to an importance sampling technique
(Florescu and Tudor, 2013, Chapter 6).
5.2.2 OBTAINING THE PARAMETERS OF THE MARKOV
CHAIN
Once we have the filtered distributions Φ1, …, ΦT, we calculate the
means for each of them 
 and we take these as estimated
realizations of the process Yt. The next step is straightforward. The
values 
 represent realizations from a mixture of Gaussian
distributions with means precisely a1, …, ap, the states of the Markov
\n\n=== OCR PAGE 144 ===\n+Y,, 44-1), 4Wy,

=: “i on Py (5:3).
7] _ vi i HD Ain

y — ji a] -
Yi stkOiy ~ Yi thay tT BAZ,, ke {l,...,m}

where AW, and AZ, represent the increments of independent

Brownian motions [i.e., they are i.i.d. N(o, A*; »,)]. B is a parameter

of the algorithm. At the end of each evolution path, we keep the pairs
yw py

%, Y, ij =1,..,n.

5.2.1.2 Selection step

The idea of this step is to give greater weights to the “better” values

yw
obtained. Specifically, we assign a weight to each final value Y, equal
to:

w= C(nyVn Q (1x; —x,1 Vn) :
where C(n) is a normalizing constant that makes the weights sum to

1.

Our approximating distribution at the end of step 7 is the discrete

selection step is equivalent to an importance sampling technique
(Florescu and Tudor, 2013, Chapter 6).

5.2.2 OBTAINING THE PARAMETERS OF THE MARKOV
CHAIN

Once we have the filtered distributions ©,, ..., ®;, we calculate the
means for each of them 1. --- » Yr and we take these as estimated
realizations of the process Y,. The next step is straightforward. The
values 1; --- » Yr represent realizations from a mixture of Gaussian
distributions with means precisely a,, ..., Ap; the states of the Markov
\n\n=== PAGE 145 ===\nchain. Thus, we may use one of the established methods to estimate
the nature of the means, for example, the Expectation Maximization
(EM) algorithm. However, the EM algorithm requires knowledge of
the number of distributions present and it is slow so we prefer a
simpler method. We follow the Minimum Error Thresholding
method of Kittler and Illingworth as improved by Cho, Haralick, and
Yi (1989). This method assumes that the best separating values for
the mixture in the distribution are at T1, …, Tp − 1. Given this
assumption, the method then calculates the likelihood of seeing the
values 
. In this way, the method estimates the likelihood
function. The maximization may be performed using any nonlinear
optimization method. Under the hypothesis of multivariate normal
distribution for Y the function to be maximized has a global
maximum. The result of the maximization is the set of optimal
threshold values 
. Once these are obtained, the algorithm
uses a truncated normal methodology to obtain estimates of the
means for each distribution in the mixture. These are our estimates
for a1, …, ap. The value of p is chosen such that it corresponds to the
largest maximum likelihood. Complete details of the methodology
may be found in Levin (2010).
Finally, to obtain the rates λi and the transition probability matrix Λ,
we use the estimated states 
 and the thresholds 
.
We go back to the estimated sequence 
 for a second time
and we link them by grouping the values that are within the
estimated thresholds 
 and corresponding to the ai values.
This allows to identify the time periods when the Markov chain stays
constant in state i as well as the destination of subsequent jumps.
Using the times, we then estimate the rates λi, using a simple
exponential distribution, and using the destination of the jumps, we
estimate the transition probabilities and correspondingly the rates
λij.
In Appendix 5.A.4, we present convergence results, both theoretical
and empirical. We feel that these results, while demonstrating the
algorithm provides correct results, will detract from the principal
\n\n=== OCR PAGE 145 ===\nchain. Thus, we may use one of the established methods to estimate
the nature of the means, for example, the Expectation Maximization
(EM) algorithm. However, the EM algorithm requires knowledge of
the number of distributions present and it is slow so we prefer a
simpler method. We follow the Minimum Error Thresholding
method of Kittler and Illingworth as improved by Cho, Haralick, and
Yi (1989). This method assumes that the best separating values for
the mixture in the distribution are at T,, ..., T,, _ ,. Given this
assumption, the method then calculates the likelihood of seeing the
values ‘1, --- » Yr, In this way, the method estimates the likelihood
function. The maximization may be performed using any nonlinear
optimization method. Under the hypothesis of multivariate normal
distribution for Y the function to be maximized has a global
maximum. The result of the maximization is the set of optimal

threshold values /1> --- 7-1, Once these are obtained, the algorithm
uses a truncated normal methodology to obtain estimates of the
means for each distribution in the mixture. These are our estimates
for a,, ..., @). The value of p is chosen such that it corresponds to the
largest maximum likelihood. Complete details of the methodology
may be found in Levin (2010).

Finally, to obtain the rates A, and the transition probability matrix A,

we use the estimated states 1» +++ » 4p and the thresholds Tyo Tho,

We go back to the estimated sequence "1; --- . Yr for a second time
and we link them by grouping the values that are within the

estimated thresholds /1>--- > 7-1 and corresponding to the q; values.

This allows to identify the time periods when the Markov chain stays
constant in state i as well as the destination of subsequent jumps.
Using the times, we then estimate the rates A,, using a simple
exponential distribution, and using the destination of the jumps, we

estimate the transition probabilities and correspondingly the rates

In Appendix 5.A.4, we present convergence results, both theoretical

and empirical. We feel that these results, while demonstrating the
algorithm provides correct results, will detract from the principal
\n\n=== PAGE 146 ===\nmerit of the paper that is the application to real data in the next
section.
It is worth mentioning that the correct identification of the rates λi
and λij is the most difficult problem. The only hope for the estimation
methodology to work correctly is to have enough samples to detect
when the regime shift is happening. It is for this reason that the
methodology is particularly suitable to analysis of data sampled with
high frequency so that regime shifts are correctly identified.
5.3 Results obtained applying the model to
real data
5.3.1 PART I: FINANCIAL APPLICATIONS
Volatility, defined as the squared root of the quadratic variation, has
a crucial role in financial applications. It provides a way to measure
the risk associated with a financial asset. This has been known for a
long time in Mathematical Finance and indeed estimating volatility
has a special place in this area. The estimation uses either historical
data (realized volatility, integrated volatility, heteroskedastic models;
Tsay, 2005; Shephard, 2005) or interpolating values computed using
financial derivatives written on the stock (implied volatility, local
volatility surfaces, etc.). However, despite the long history, in
practice there are irreconcilable differences between the values
obtained using historical values and on-the-spot derivative values.
Traditionally, the difference is explained using the time paradigm—
implied volatility reflect present risk values while the historical
volatility represents risks from the past and including the present.
In the present work, we work under the following assumption. There
are shifts in volatility value; these shifts are captured using the
methodology presented in this chapter. These shifts may be due to
news, extraneous events, or trading patterns that change during the
day.
The financial data we chose to illustrate the method is from March
2008. Bear Stearns began the week on Monday, March 10, 2008,
with a market stock price of about < ent > $ </ent > 70/share. The
\n\n=== PAGE 147 ===\n85-year-old firm, the fifth largest Wall Street securities firm at the
time, had never recorded a quarterly loss until the fourth quarter of
the previous year (2007). They were anticipating a profit for the first
quarter report for 2008. At the end of the week (Friday, March 14,
2008) the share price had dropped to $30 and over the weekend
Bear Stearns, JPMorgan and the Federal Reserve Bank of New York
arranged for JPMorgan to Purchase Bear Stearns for < ent > $ </ent
>2/share to avoid a bankruptcy filing (source for general
information: Cohan, 2009).
How did an apparently very healthy firm get in this situation? We do
not know and perhaps will never know without analyzing the internal
balance of the company. However, this is a perfect setting for testing
our model. By getting closer to the conclusion of that week, the
market should become more and more volatile and consequently the
volatility should perform a shift in magnitude. Will we be able to
capture this shift (provided it exists)? Will we be able to determine
an exact moment when the crisis had started? Were other companies
affected by the collapse? Will we be able to determine all this from
the analysis of stock data alone? All these are questions that we
hoped to answer when we started the analysis of the minute data.
5.3.1.1 The events as they happened
At the time of the events mentioned, there had already been a large
bankruptcy related to companies involved in mortgage instruments:
the buy-out of Countrywide Financial, “the nation’s largest mortgage
lender” by Bank of America early in January 2008 (Landon and
Sorkin, 2008; Ross Sorkin, 2008; Dash, 2008, —New York Times
articles). Many investment banks and other corporations, either
large or small, and including government agencies were in the
volatile subprime market. From the market facts and the media
attention, both the corporate sector and the investing public were
aware of the subprime mortgage loan crisis.
The people in charge of the business at Bear Stearns (including those
working at the time for the company) knew that there was trouble
with funding, overnight loans, and liquidity in general from the start
of the week of the collapse (source: undisclosed former employee).
\n\n=== PAGE 148 ===\nThroughout the investment industry, there were warning signals, in
particular, home mortgage defaults (Geithner, 2008).
As late as Tuesday March 11, the stock market was heading up after
the Federal Reserve opened up its lending policies to investment
companies in addition to just commercial banks (which was the
previous practice). On the evening of Thursday March 13, Bear
Stearns executives were meeting with SEC regulators and Tim
Geithner (president of NY Fed). The president of the Federal
Reserves Bank of New York expressed the opinion that Bear Stearns
would have to file for bankruptcy (source: U.S. Senate Committee on
Banking, Housing and Urban Affairs, 2008). We assume that the
market would react on Friday to this news so it is interesting to
analyze the volatility behavior.
5.3.1.2 Data analysis and results
In Figure 5.1, we may observe the behavior of the Bear Stearns
volatility during the week of March 10–14 and continuing with the
following two days (trading did not stop after the weekend since the
company still existed although under different management). The
estimated states for the volatility of the Bear Stearns stock are 1.9,
5.4, and 6.8. All these values are outside the normal [0, 1] annual
volatility values generally characteristic for the stock market.
Furthermore, clearly the volatility levels shift upward toward the end
of the week starting as early as the middle of Tuesday and certainly
after the Thursday meeting.
\n\n=== PAGE 149 ===\nFIGURE 5.1 Bear Stearns stock price/volatility March 10 to March
18, 2008.
We recognized that the high volatility values may be due to the
Monday (March 17) and Tuesday (March 18) following the buy-out
announcement. During those days, the stock traded at $3–$4 range
even though the announcement was that shares will be bought by
JPMorgan at $2/share. The explanation for the high trading activity
came on March 24, 2008, when a class action lawsuit was filed on
behalf of shareholders, challenging the terms of JPMorgan recently
announced acquisition of Bear Stearns. That same day, a settlement
was reached that raised JPMorgan Chase’s offer to $10 a share.
Recognizing the situation, we had eliminated the days after the buy-
out and have redone the analysis separately for each of the 3 weeks
preceding March 14 (see Figures 5.2, 5.3, and 5.4 on page 8). From
these pictures, we see that the volatility levels (~ 0.59 and ~ 0.86) for
the 2 weeks preceding the critical week are similar. We see that the
levels during the final week are all higher than 1 (1.1, 2.4, and 5.2) so
the whole week behavior is abnormal. The second week does exhibit
prolonged periods of higher volatility so we decided to pull the final 2
\n\n=== OCR PAGE 149 ===\nBear stearns 3-10-08 to 3-18-08

a 70

$

ow 50

>

S 30

2

O 40
T T T T T 1 7
ie} 1 2 3 4 5 6

Days

Bear stearns 3-10-08 to 3-18-08; discrete levels {1.945, 5.403, 6.807}

discrete estimate
oN FD

Volatility scatter vs.

Days

FIGURE 5.1 Bear Stearns stock price/volatility March 10 to March
18, 2008.

We recognized that the high volatility values may be due to the
Monday (March 17) and Tuesday (March 18) following the buy-out
announcement. During those days, the stock traded at $3-$4 range
even though the announcement was that shares will be bought by
JPMorgan at $2/share. The explanation for the high trading activity
came on March 24, 2008, when a class action lawsuit was filed on
behalf of shareholders, challenging the terms of JPMorgan recently
announced acquisition of Bear Stearns. That same day, a settlement
was reached that raised JPMorgan Chase’s offer to $10 a share.

Recognizing the situation, we had eliminated the days after the buy-
out and have redone the analysis separately for each of the 3 weeks
preceding March 14 (see Figures 5.2, 5.3, and 5.4. on page 8). From
these pictures, we see that the volatility levels (~ 0.59 and ~ 0.86) for
the 2 weeks preceding the critical week are similar. We see that the
levels during the final week are all higher than 1 (1.1, 2.4, and 5.2) so
the whole week behavior is abnormal. The second week does exhibit
prolonged periods of higher volatility so we decided to pull the final 2
\n\n=== PAGE 150 ===\nweeks (March 3–14, 2008) together for a final analysis in Figure 5.5.
In this figure, we can finally see that abnormal levels were touched in
the beginning and end of day 4 (Friday, March 7) and certainly
during the entire next week (as we had observed from the earlier
analysis). Furthermore, we do observe lowered values for volatility
during the Wednesday March 12, this is the day before the SEC
meeting with Bear Stearns executives and we suspect from this
behavior that the bankruptcy announcement came as a surprise to
the general market participants.
\n\n=== PAGE 151 ===\nFIGURE 5.2 Bear Stearns stock price/volatility 2 weeks before
collapse.
\n\n=== OCR PAGE 151 ===\nBear stearns 2-25-08 to 2-29-08

©
ro}

82

Stock values

78

Days

Bear stearns 2-25-08 to 2-29-08; discrete levels {0.578, 0.855}

So
BR

discrete estimate

o
o
lo
lo
lo
lo
fe
lo

Volatility scatter vs.

Days
FIGURE 5.2 Bear Stearns stock price/volatility 2 weeks before
collapse.
Bear stearns 3-3-08 to 3-7-08

Nn
a

Stock values
N
LS)

QD
©

Days

Bear stearns 3-3-08 to 3-7-08; discrete levels {0.599, 0.860}

Zo
- ©
aes
an
Zo
ta

@ 0.4
Zo
S300
gs

\n\n=== PAGE 152 ===\nFIGURE 5.3 Bear Stearns stock price/volatility 1 week before the
collapse.
FIGURE 5.4 Bear Stearns stock price/volatility week of the
collapse.
\n\n=== OCR PAGE 152 ===\nFIGURE 5.3 Bear Stearns stock price/volatility 1 week before the
collapse.

Bear stearns 3-10-08 to 3-14-08

Baan
oooo

Stock values

[)
oS

Days

Bear stearns 3-10-08 to 3-14-08; discrete levels {1.058, 2.440, 5.208}

discrete estimate
oN FD @

Volatility scatter vs.

Days

FIGURE 5.4, Bear Stearns stock price/volatility week of the
collapse.

Bear stearns 3-3-08 to 3-14-08

Stock values
a
o

Days

Bear stearns 3-3-08 to 3-14-08; discrete levels {0.724, 2.145, 5.386}

Volatility scatter vs.
discrete estimate
oN FD DW

Days
\n\n=== PAGE 153 ===\nFIGURE 5.5 Bear Stearns stock price/volatility during the last 2
weeks.
We have performed a thorough investigation, using data for several
other equities from the investment sector (JPMorgan, Lehman Bros,
Bank of America, Merrill Lynch) as well as noninvestment sector
(IBM, Chevron, Exxon, FedEx) during the week of the collapse
March 10–14. For lack of space, we present the plots and the detailed
results for these stocks in the supplementary material Supplement B.
The results that were evident from this analysis are commented next.
The volatility levels estimated for each stock in the investment sector
during the week are extremely high (though not at the levels
encountered for Bear Stearns). Beyond the actual values, all of them
exhibit a common behavior during the week. All stocks went to high
levels at the beginning of the trading day on Thursday until about the
middle of the day and then again on Friday. The corresponding stock
values do not exhibit any obvious pattern from which this volatility
behavior may be easily obtained.
When looking at the noninvestment sector, we finally observe
volatility levels that are characteristic of the respective stock. For
example, the estimated levels for IBM were 0.36, 0.63, and 0.78 (the
typical implied volatility level for IBM stock calculated from option
data using the Black Scholes model is historically around 0.4).
Furthermore, IBM does exhibit increased levels of volatility as early
as Thursday morning and further increasing during Friday. The oil
companies stock analyzed (Chevron and Exxon Mobil) are very
similar in both volatility values and behavior. They show only an
increase in volatility starting Friday. This discrepancy in the volatility
behavior between technology stock and others was explained to us by
a trader who stated that in general technology, stocks react much
faster to the market movement than the commodities stock. Among
the noninvestment sector, we also analyze FedEx volatility behavior
during the week. This equity behavior exhibits slight to no reaction to
the Bear Stearns collapse. In conclusion, we believe that the model
provided insight into market behavior during that week. First, we
believe that the volatility levels for Bear Stearns equity were
abnormal starting with Friday March 7, 2008, and continued
throughout the week of March 10–14 until the collapse of the
\n\n=== PAGE 154 ===\ninvestment firm. Signs were present in other investment equity as
well but none as in the Bear Stearns case. We see higher volatility
levels on Thursday morning (before SEC meets with the Bear Stearns
officials) exhibited by the entire investment sector as well as the
highly traded technology stock. Commodity sector was affected on
Friday following the remarks by the Federal Reserve Bank of New
York that a bankruptcy is imminent. The event did not seem to affect
the entire market only the highly traded equity.
5.3.2 PART II: PHYSICAL DATA APPLICATION.
TEMPERATURE DATA
Global warming seems to be a “hot” topic these days so we wanted to
see if we may adapt the model to study climate-related problems.
Temperature record is the most important signal for global warming.
The problem we faced was finding temperature data sampled with
high enough frequency (which is needed to detect shifts in
variability). The highest frequency sampled data we found was
hourly temperature data gathered in Central Park, New York City,
New York, USA, starting in 2000. We realize that this data is
obtained for a very large park located in a middle of arguably the
largest metropolis on Earth and therefore our results are not to be
extrapolated. However, the results we found when estimating
variability of this data were most interesting. We present two typical
pictures of estimated volatility in Figure 5.6 on page 10 (the rest of
images are in the supplementary material Supplement C).
\n\n=== PAGE 155 ===\nFIGURE 5.6 Hourly temperature data from 2000 and 2006.
It is remarkable that all the plots show evidence of a predominant
variability value and the deviations from this value do not last very
long. Accordingly, we conclude that in general the hourly data show a
large degree of consistency in variability of temperature.
Consequently, Table 5.1 presents the value of the predominant
variability each year. The last data available was from July 2010 and
in order to have a consistent way of estimating we overlap some time
period in the end to provide an entire year worth of data.
\n\n=== OCR PAGE 155 ===\n2000 Central Park discrete levels {0.023, 0.270, 0.516} 2006 Central Park discrete levels (0.024, 0.430}

Volatility scatter vs. discrete estimate
Volatility scatter vs. discrete estimate

0 100 200 300
Days

FIGURE 5.6 Hourly temperature data from 2000 and 2006.

It is remarkable that all the plots show evidence of a predominant
variability value and the deviations from this value do not last very
long. Accordingly, we conclude that in general the hourly data show a
large degree of consistency in variability of temperature.
Consequently, Table 5.1 presents the value of the predominant
variability each year. The last data available was from July 2010 and
in order to have a consistent way of estimating we overlap some time
period in the end to provide an entire year worth of data.

\n\n=== PAGE 156 ===\nTable 5.1 Yearly predominant node volatility 2000–10 Central Park,
New York City.
Year
Predominant volatility level
2000
0.27
2001
0.269
2002
0.271
2003
0.276
2004
0.276
2005
0.293
2006
0.43
2007
0.423
2008
0.425
08/08–07/09 0.424
08/09–07/10 0.411
When looking at these values, one clear distinction stands out. For
the years 2000–2005, the variability is remarkably close to 0.3.
Beginning with 2006, there is a notable jump in variability (the rows
are separated in the table for a better visibility) and the variability
level stays remarkably close to 0.42. What exactly does this mean
and what caused the shift in the temperature variability we do not
know but we interpret this as some evidence of a certain change in
the local climate in the New York City area.
We would have liked to compare the results obtained in this way with
data from earlier years. Unfortunately, before 2000 the data was not
gathered with high enough frequency. The only other temperature
data we found was gathered at the same location (Central Park, New
York City) from 1976 to 1977, but this data was sampled every 3
hours and thus the numerical results obtained are not directly
comparable. We may see the output in Figure 5.7 on page 11. This
time the filter outputs only one volatility node as best fitting the data.
The values for the two consecutive years are close too (0.49 vs. 0.48).
However, as mentioned, we may not compare this value with the
results obtained for the 2000–2008 since the sampling frequency is
\n\n=== PAGE 157 ===\ndifferent. Having said that, if the stationary distribution of the
stochastic volatility process is Gaussian (as it seems to be), then a
simple transformation should give an indication of the hourly
volatility value.
FIGURE 5.7 Three-hour volatility estimates for 1976–1977.
With this, we may guess the hourly values as 
 and
, which certainly look consistent with values we
obtained from 2000 to 2005.
5.3.3 PART III: ANALYSIS OF SEISMOMETER READINGS
DURING AN EARTHQUAKE
For the final practical application of the model presented in this
chapter, we consider data collected for the Parkfield California
earthquake of September 28, 2004: magnitude 6.0 on the Richter
scale. Studies in the 1980s had predicted an earthquake for 1993 in
this area near the San Andreas fault. As a result, a network of sensors
was built and on December 20, 1994, an M = 5.0 earthquake did
occur (CSMIP, 2004; Borcherdt et al., 2004, 2006, and references
within). We owe the data analyzed herein to the buildup of these
array sensors.
\n\n=== OCR PAGE 157 ===\ndifferent. Having said that, if the stationary distribution of the
stochastic volatility process is Gaussian (as it seems to be), then a
simple transformation should give an indication of the hourly
volatility value.

03, = Oh * v3

3-hourly temp 3-hourly temp
S 300 £ wl
3 ie
5 2 $ 2604
g # x0
260 260

1976.0 1976.2 19764 19766 19768 1977.0 19770 19772 19774 19776 19778 19780
Years Years

3-hourly temp 1976; discrete levels - one level - 0.49 ‘3-hourly temp 1977; discrete levels - one level - 0.48

t .

°
®
a

2
FS

ot a]

ones p
42
19760 1976.2 1976.4 19766 19768 1977.0 197.0 19772 19774 19776 19778 19780
Years Years

Volatility scatter vs.
discrete estimate

2
8

FIGURE 5.7 Three-hour volatility estimates for 1976-1977.

With this, we may guess the hourly values as 0.48/ v3 = 0.277 and

0.49/73 = 0.283, which certainly look consistent with values we
obtained from 2000 to 2005.

5.3.3 PART Ill: ANALYSIS OF SEISMOMETER READINGS
DURING AN EARTHQUAKE

For the final practical application of the model presented in this
chapter, we consider data collected for the Parkfield California
earthquake of September 28, 2004: magnitude 6.0 on the Richter
scale. Studies in the 1980s had predicted an earthquake for 1993 in
this area near the San Andreas fault. As a result, a network of sensors
was built and on December 20, 1994, an M = 5.0 earthquake did
occur (CSMIP, 2004; Borcherdt et al., 2004, 2006, and references
within). We owe the data analyzed herein to the buildup of these
array sensors.
\n\n=== PAGE 158 ===\nThe seismographic readings were programmed to trigger for a P
wave (primary wave) of magnitude M > 3.0. The P wave is a
compression wave analogous to sound waves. It is the fastest seismic
of the seismic waves and consequently arrives first. The S waves
(secondary waves) and the surface waves that are responsible for
most of the destruction are slower. The Richter scale magnitude is a
logarithmic (base 10) scale. This means that an M = 5.0 event has
100 times the movement of a 3.0 event, a 6.0 event has 1000 times
the movement of a 3.0 event. The signal used is the ground
acceleration reading at 0.005-second intervals.
5.3.4 ANALYSIS OF THE EARTHQUAKE SIGNAL:
BEGINNING
In the analysis we use data for the same earthquake (the 2004
Parkfield earthquake) obtained from two different research stations:
Red Hills and Donna Lee stations. The two stations are in a
mountainous region about at the same elevation and the distance
between the stations is about 23.8 miles (or 38.3 km, distance
calculated using their coordinates). Figure 5.8 presents the raw
ground acceleration signal at the two stations. It is evident from the
two pictures that Donna Lee was much closer to the epicenter. If we
use the threshold arrival time (the time at which the raw acceleration
passes magnitude 3) as the time at which the initial P wave energy
arrived at each location, then the earthquake starts at Donna Lee at
17:27.849 and at Red Hills at 17:29.259.
\n\n=== PAGE 159 ===\nFIGURE 5.8 Raw acceleration signal at the two stations.
We perform the variability estimates for a restricted region at the
beginning of the signal and we plot the estimates obtained in Figure
5.9. While the Donna Lee signal presents a spike in variability at
exactly the same time as the P wave is observed (29.85 s), for the Red
Hills station there is a spike in variability 20 hundreds of a seconds
earlier than the official earthquake starts. The second time in the
image corresponds to the variability increase and this is remarkably
close to the official P wave time (29.26 s). The difference may be due
to the interference in signal traveling through the ground.
\n\n=== OCR PAGE 159 ===\nDonna Lee Parkfid CA Sept 4

150

readings
50

Accelerometer
i)

T T T
30 35 40

Seconds

-100

Red Hills Parkfld CA Sept 4

T T T

30 35 40
Seconds

FIGURE 5.8 Raw acceleration signal at the two stations.

150

readings
50

Accelerometer
oO

-100

We perform the variability estimates for a restricted region at the
beginning of the signal and we plot the estimates obtained in Figure
5.9. While the Donna Lee signal presents a spike in variability at
exactly the same time as the P wave is observed (29.85 s), for the Red
Hills station there is a spike in variability 20 hundreds of a seconds
earlier than the official earthquake starts. The second time in the
image corresponds to the variability increase and this is remarkably
close to the official P wave time (29.26 s). The difference may be due
to the interference in signal traveling through the ground.
\n\n=== PAGE 160 ===\nFIGURE 5.9 Variability comparison. Beginning of an earthquake.
One of the methods for locating the actual origin or epicenter of the
earthquake is by using the arrival times of the P waves. A simple
triangulation method may be used if one knows the speed of travel of
the waves through the ground and such speeds are charted and
known. However, there are a number of factors affecting this speed
that are not charted. Ground temperature and humidity, for
instance, are factors that change during the day and their influence is
unknown. Furthermore, the more dispersed the wave (further from
the epicenter) the greater the interference (and influence) of these
factors. Note that the signal at Red Hills does not look anything like
the signal at a location closer to the epicenter. For this reason, we
believe that the first time a shift in variance is detected may be an
alternative way of providing triangulation results. The results
obtained this way could be then compared with the more traditional
methods either reinforcing or contradicting the results. In either
case, we believe that the methodology may be useful.
Calculating the focus or point of origin of the earthquake is very
useful for the geophysical analysis and long-term estimates of
earthquake hazard. Even though short-term prediction has not been
successful, some correlation between smaller earthquakes and
detectable activity shortly before an earthquake has been observed.
\n\n=== OCR PAGE 160 ===\nof

wo
oD
w GH
£ RH: 29.04
3 DL:27.85 RH: 29.26
: -
5
a
Do
2 N
=
&
& 2
>

ol:

25 26 27 28 29 30 31 32
Seconds

FIGURE 5.9 Variability comparison. Beginning of an earthquake.

One of the methods for locating the actual origin or epicenter of the
earthquake is by using the arrival times of the P waves. A simple
triangulation method may be used if one knows the speed of travel of
the waves through the ground and such speeds are charted and
known. However, there are a number of factors affecting this speed
that are not charted. Ground temperature and humidity, for
instance, are factors that change during the day and their influence is
unknown. Furthermore, the more dispersed the wave (further from
the epicenter) the greater the interference (and influence) of these
factors. Note that the signal at Red Hills does not look anything like
the signal at a location closer to the epicenter. For this reason, we
believe that the first time a shift in variance is detected may be an
alternative way of providing triangulation results. The results
obtained this way could be then compared with the more traditional
methods either reinforcing or contradicting the results. In either
case, we believe that the methodology may be useful.

Calculating the focus or point of origin of the earthquake is very
useful for the geophysical analysis and long-term estimates of
earthquake hazard. Even though short-term prediction has not been
successful, some correlation between smaller earthquakes and
detectable activity shortly before an earthquake has been observed.
\n\n=== PAGE 161 ===\nThe timing and amplitude of seismographic signals is also used for
fundamental research about the earth’s interior.
5.3.5 ANALYSIS: DURING THE EARTHQUAKE
In Figure 5.10, we plot the the variability analysis obtained using the
entire acceleration signal at the two stations. Figure 5.11
superimposes the two estimates on the same timeline for the ease of
comparison.
\n\n=== PAGE 162 ===\nFIGURE 5.10 Parkfield CA, Donna Lee Peak seismograph readings.
\n\n=== OCR PAGE 162 ===\nDonna Lee Parktid CA Sept 4

Seconds

discrete estimate

Volatility scatter vs.
os 888s

Red Hills Parkfld CA Sept 4

Volatility scatter vs.
discrete estimate

FIGURE 5.10 Parkfield CA, Donna Lee Peak seismograph readings.
\n\n=== PAGE 163 ===\nFIGURE 5.11 Parkfield CA earthquake. Comparison of the
variability signals, Donna Lee variability in green and Red Hills
variability in blue.
We believe that the variability estimates may prove to be most useful
for this period. First note that the shifts in variability are not evident
from the accelerogram. People living through earthquakes will tell of
the pulses that they feel during the earthquake. This shift in
variability may indicate exactly that. Furthermore, by combining the
magnitude and the frequency of these intervals, we may be able to
measure the destructive power of an earthquake. This is done only
approximately with the Richter scale. The Richter scale (adjusted
logarithm base 10 of the highest perceived acceleration value at the
epicenter), even though supposedly measuring the earthquake
strength has, in fact, nothing to do with the actual destructive power.
For example, the 2010 Haiti Earthquake, an extremely devastating
earthquake (230,000 mortal cases, 3 million people affected), was
recorded as 7.0 on the Richter scale. The only other recorded
earthquake at 7.0 is the West Java earthquake of 2009—a very
serious earthquake nonetheless one that cannot be compared with
the severity of the Haiti earthquake (79 deaths displacing 210,000
people). We should also mention that Java is a much more densely
populated region. The 2007 Tocopilla earthquake in Northern Chile
\n\n=== OCR PAGE 163 ===\noo
i)
© ©]
a N
£
8 8 —
2
oO
5 wo
27
co]
2 oo]
ze
oO
=.
o
> oH J |
co
30 35 40
Seconds

FIGURE 5.11 Parkfield CA earthquake. Comparison of the
variability signals, Donna Lee variability in green and Red Hills
variability in blue.

We believe that the variability estimates may prove to be most useful
for this period. First note that the shifts in variability are not evident
from the accelerogram. People living through earthquakes will tell of
the pulses that they feel during the earthquake. This shift in
variability may indicate exactly that. Furthermore, by combining the
magnitude and the frequency of these intervals, we may be able to
measure the destructive power of an earthquake. This is done only
approximately with the Richter scale. The Richter scale (adjusted
logarithm base 10 of the highest perceived acceleration value at the
epicenter), even though supposedly measuring the earthquake
strength has, in fact, nothing to do with the actual destructive power.
For example, the 2010 Haiti Earthquake, an extremely devastating
earthquake (230,000 mortal cases, 3 million people affected), was
recorded as 7.0 on the Richter scale. The only other recorded
earthquake at 7.0 is the West Java earthquake of 2009—a very
serious earthquake nonetheless one that cannot be compared with
the severity of the Haiti earthquake (79 deaths displacing 210,000
people). We should also mention that Java is a much more densely
populated region. The 2007 Tocopilla earthquake in Northern Chile

\n\n=== PAGE 164 ===\nrecorded at 7.7 on the Richter scale caused two deaths and 150 minor
injuries, 15,000 people were displaced. The economic damage of an
earthquake is, in fact, measured by the Mercalli intensity scale, an
entirely subjective scale that rates the earthquake from I—
Instrumental (not felt) to XII—Cataclysmic. But, in fact the
earthquake is rated, thus months after it takes place and by looking
at the total devastation it produces. On the contrary, looking at the
estimated variability, we may be able to provide a much more
accurate measure for this quantity immediately after the earthquake
takes place.
Furthermore, if we look at the variability estimates (Figure 5.11) and
the ground signal itself (Figure 5.8), it is pretty clear that this is the
same earthquake even though one could not tell this from the signal
itself. The variability patterns are parallel at the two locations. Given
the relatively short distance between the two stations, one would
expect the signal at Red Hills station to have a similar destructive
power. This may not be the case based on this preliminary study.
5.3.6 ANALYSIS: END OF THE EARTHQUAKE SIGNAL,
AFTERSHOCKS
Figure 5.10 presents the variability estimates at the two stations.
Note that after about 10 s at both locations, the predominant
variability changes to a lower regime. The accelerometer signal
shows lower values for the ground acceleration, but from that signal
alone there does not seem to be a clear end point of the earthquake.
By looking at the variability on this last region, perhaps one could
determine the probability of a future aftershock. An aftershock is a
smaller earthquake that occurs after a previous large earthquake in
the same area (the main shock). If an aftershock is larger than the
main shock, the aftershock is redesignated as the main shock and the
original main shock is redesignated as a foreshock. Aftershocks are
mainly produced as the crust around the displaced fault plane
adjusts to the effects of the main shock, and information about this
perhaps may be gathered by looking at the variability pattern at the
end of the earthquake signal.
\n\n=== PAGE 165 ===\n(5.A.1)
5.4 Conclusion
The work presents a methodology of estimating the unobservable
variability of a signal. The method was used for three different areas
and for each area provided insight into the working of the real-life
signal. We believe that this is the main purpose of a model and that
we have accomplished this purpose.
Appendix 5.A: Theoretical results and
empirical testing
In this section, we provide convergence results of the estimation
algorithm presented in the chapter. The arguments presented are
both: theoretical—by providing estimates for convergence rates, and
empirical—by generating paths from a known process and
comparing the estimates with the ground truth.
5.A.1 HOW DOES THE PARTICLE FILTER WORK?
To start this section, we note that the discrete process in (5.3) does
not correspond to a discretization of the original continuous process
in (5.2). Indeed, locally between any two times ti − 1, ti the evolved
process approximates the following continuous volatility process:
The approximation in (5.3) of this auxiliary model is used to estimate
the parameters of the Markov chain Yt.
The convergence of the discretized filter Yt in (5.3) to the continuous
version from which it comes [equation (5.A.1)] is a well-studied
problem. Del Moral, Jacod, and Protter [Del Moral et al. (2001);
DelMoral (2004)] develop bounds on the expected error of the filter
and the filter expected value. In the cited work, the authors work
with a given observed process {Xt∣t ≤ tN} a “noisy” measurement of
\n\n=== OCR PAGE 165 ===\n5.4 Conclusion

The work presents a methodology of estimating the unobservable
variability of a signal. The method was used for three different areas
and for each area provided insight into the working of the real-life
signal. We believe that this is the main purpose of a model and that
we have accomplished this purpose.

Appendix 5.A: Theoretical results and
empirical testing

In this section, we provide convergence results of the estimation
algorithm presented in the chapter. The arguments presented are
both: theoretical—by providing estimates for convergence rates, and
empirical—by generating paths from a known process and
comparing the estimates with the ground truth.

5.A.1 HOW DOES THE PARTICLE FILTER WORK?

To start this section, we note that the discrete process in (5.3) does
not correspond to a discretization of the original continuous process
in (5.2). Indeed, locally between any two times t; _ ,, t; the evolved

process approximates the following continuous volatility process:

pt y- ' (5.4.1),
X,=X,, +/ r—> |du+ / YdW,,
ty - Jt,

i-t
Y,=Y,,+P(Z,-Z,_,)
The approximation in (5.3) of this auxiliary model is used to estimate
the parameters of the Markov chain Y,.

The convergence of the discretized filter Y, in (5.3) to the continuous
version from which it comes [equation (5.A.1)] is a well-studied
problem. Del Moral, Jacod, and Protter [Del Moral et al. (2001);
DelMoral (2004)] develop bounds on the expected error of the filter
and the filter expected value. In the cited work, the authors work
with a given observed process {X, | t < ty} a “noisy” measurement of
\n\n=== PAGE 166 ===\n(5.A.2)
(5.A.3)
(5.A.4)
{Yt∣t ≤ tN} the unobservable process or in our case the auxiliary
volatility process.
Specifically, the following general situation is presented, where Xt,
Yt, a, a′, b, and b′ are q-dimensional random vectors and valued
functions.
subject to the condition that the first and second derivatives of b and
b′ exist and are bounded. Furthermore, ‖b‖2 and ‖b′‖2 are not
identically zero.
The distribution of the process being estimated σ(Yt) is denoted with
ΠNσ = {σt∣Xt, t ≤ tN). The output of the numerical filter at the Nth
observation using n test paths is the estimated distribution of σ
process, denoted with 
. The work cited
provides bounds for the expected error 
 with a
function of N, n, and σ.
The weighting function used in the general filtering algorithm ϕ may
be any arbitrary function integrating to 1 and the weights are
assigned using:
which is identical with what we use in our more specific algorithm (q
= 1), while n is the number of generated test paths and N is the total
number of observations.
The cited results prove two convergence results of the filter to the
auxiliary model as n → ∞. Specifically, when q = 1:
\n\n=== OCR PAGE 166 ===\n{Y, | t < ty} the unobservable process or in our case the auxiliary
volatility process.

Specifically, the following general situation is presented, where X,,
Y,, a, a’, b, and b’ are q-dimensional random vectors and valued
functions.

dX, =a(X,,Y,)dt + b(X,, Y,)dW, (5.A.2)
dY, =a'(Y,)dt+b'(Y,)dZ,

subject to the condition that the first and second derivatives of b and
b’ exist and are bounded. Furthermore, ||b||? and ||b’||? are not
identically zero.

The distribution of the process being estimated o(Y;) is denoted with
Tyo = {0;| X;, t < ty). The output of the numerical filter at the Nt
observation using n test paths is the estimated distribution of o
process, denoted with yo = {6,1 X;,i < N }. The work cited

EdI\o — ayo)

provides bounds for the expected error with a

function of N, n, and o.
The weighting function used in the general filtering algorithm @ may

be any arbitrary function integrating to 1 and the weights are
assigned using:

b,(v) = C(N)d(vn®*) (5.4.3)

which is identical with what we use in our more specific algorithm (q
= 1), while n is the number of generated test paths and N is the total
number of observations.

The cited results prove two convergence results of the filter to the
auxiliary model as n > . Specifically, when q = 1:

E [IIo - tyo| < co (5.A.4)
ns
\n\n=== PAGE 167 ===\n(5.A.5)
as n → ∞ and C = C(N), where n is the number of trial paths and N is
the number of observation points.
In our case, the convergence proof needs to be modified because of
the presence of the auxiliary process. The idea in our case is to show
that the auxiliary process converges to the discretization of the true
process generated from the Markov chain volatility. The only
theoretical case considered is when the weighting function is a
Gaussian (we take advantage of the normal–normal conjugate
distributions). In the empirical testing section (5.A.4), we look at the
effects of many other weighting functions. They have little effect on
the estimation.
We state only the relevant results in the Appendix. For a more
thorough analysis and the proof of the results, we refer to the original
thesis (Levin, 2010).
5.A.2 THEORETICAL RESULTS ABOUT CONVERGENCE
AND PARAMETER ESTIMATES
The objective of this section is to derive some simple and applicable
convergence results for the hypothesized system (equation ** (5.2)
**) and the filter (equation ** (5.3) **) used to analyze it.
5.A.2.1 First pass: initial volatility estimates
For a process X with volatility a Markov chain Y as used in this
chapter, we use the following notations. For each volatility state i at
time t when the estimator is obtained:
– σi is the “true” underlying volatility state.
– N is the number of consecutive observations during one
uninterrupted stay in the respective state, up until time t.
– The total number of observations at the respective state i
(throughout all the observed data available at time t) is denoted
with Pts.
– The number of test paths the filter generates is denoted with n.
\n\n=== OCR PAGE 167 ===\nP (|IT.0 — xyo)| > 6) < Ce G.A'5)

as n — © and C = C(N), where n is the number of trial paths and Nis
the number of observation points.

In our case, the convergence proof needs to be modified because of
the presence of the auxiliary process. The idea in our case is to show
that the auxiliary process converges to the discretization of the true
process generated from the Markov chain volatility. The only
theoretical case considered is when the weighting function is a
Gaussian (we take advantage of the normal—normal conjugate
distributions). In the empirical testing section (5.4.4), we look at the
effects of many other weighting functions. They have little effect on
the estimation.

We state only the relevant results in the Appendix. For a more
thorough analysis and the proof of the results, we refer to the original
thesis (Levin, 2010).

5.A.2 THEORETICAL RESULTS ABOUT CONVERGENCE
AND PARAMETER ESTIMATES
The objective of this section is to derive some simple and applicable

convergence results for the hypothesized system (equation ** (5.2)
**) and the filter (equation ** (5.3) **) used to analyze it.

5.A.2.1 First pass: initial volatility estimates

For a process X with volatility a Markov chain Yas used in this
chapter, we use the following notations. For each volatility state i at
time t when the estimator is obtained:

— 6; is the “true” underlying volatility state.

— Nis the number of consecutive observations during one
uninterrupted stay in the respective state, up until time t.

— The total number of observations at the respective state i
(throughout all the observed data available at time t) is denoted
with Pts.

— The number of test paths the filter generates is denoted with n.
\n\n=== PAGE 168 ===\n(5.A.6)
(5.A.7)
–  is volatility of the discrete state system.
– σest is the filter estimated volatility (which uses a continuous
state space approximation).
– σbound is the theoretical point at which extreme test values for
 are ignored or truncated. Its value is adapted to the filter
performance and decreases as the filter converges.
– σφ is the standard deviation of the penalty function and is
usually much smaller than σ bound and .
– σB is a derived factor in the theoretical description of the filter
defined by:
With these notations, the following result relates the mean of the
estimator obtained using the auxiliary process with the mean of the
estimator we would have obtained using the real process at a certain
point in time.
Theorem 5.1 For point volatility estimates after N points at a
given state given ε > 0 there exists C large enough so that:
The next theorem relates the nodes estimated using all the data
history.
Theorem 5.2 For volatility node estimates after Pts total number
observations at a given level and N observations per mean
duration:
\n\n=== OCR PAGE 168 ===\n— 9; is volatility of the discrete state system.

— Oegt is the filter estimated volatility (which uses a continuous
state space approximation).

— Obouna is the theoretical point at which extreme test values for
> are ignored or truncated. Its value is adapted to the filter
performance and decreases as the filter converges.

— Og is the standard deviation of the penalty function and is
usually much smaller than 6} ynq and %.

— Opis a derived factor in the theoretical description of the filter
defined by:

2

oR = % + 6 + bound (5.A.6)

With these notations, the following result relates the mean of the
estimator obtained using the auxiliary process with the mean of the
estimator we would have obtained using the real process at a certain
point in time.

Theorem 5.1 For point volatility estimates after N points at a
given state given € > o there exists C large enough so that:

OR (5.4.7)
P| |6; — G4) > C1/ —— ] <e€
Nn&t

The next theorem relates the nodes estimated using all the data
history.

Theorem 5.2 For volatility node estimates after Pts total number
observations at a given level and N observations per mean
duration:
\n\n=== PAGE 169 ===\n(5.A.8)
(5.A.9)
(5.A.10)
The next result relates the realized volatility of the discretized system
to the parameter value.
Theorem 5.3.
For a two-state system {a1, a2} with transitions rates λ1 from state a1
to state a2 and λ2 the transition rate from state a2 to state a1, we want
to estimate the node levels {a1, a2} and the transition rates at each
level. The node levels are determined from fitting a mixture of
Gaussian distributions to the volatility estimates and using the
means as node estimates. With enough points, the convergence
specified in Theorems 5.1–5.3 allows us to distinguish and estimate
nodes if the mean duration at the levels is long enough with enough
transitions. From the convergence result for point estimates (ref to
equation 5.A.7) we see an increase in accuracy as the number of
paths and the number of observations per duration at a given level
increase. This holds even more strongly for the node estimates (refer
to equation 5.A.8) and also increases as the total number of points at
a given level increases.
5.A.3 MARKOV CHAIN PARAMETER ESTIMATES
The conditional transition probability from state i to state j is given
by:
This formula allows estimates for transition probabilities to be
computed using the empirical transition rates. Confidence intervals
for these transition rates for individual cases may be found using the
\n\n=== OCR PAGE 169 ===\no In(N (5.4.8)
PY |6;-—o.4| > CX 2s BN) <e
nX Pts x At N
The next result relates the realized volatility of the discretized system
to the parameter value.

2 (5.4.9)
P ||6,-—o,| > Cx wei <e

For a two-state system {a,, a,} with transitions rates \, from state a,
to state a, and A, the transition rate from state a, to state a,, we want
to estimate the node levels {a,, a,} and the transition rates at each
level. The node levels are determined from fitting a mixture of
Gaussian distributions to the volatility estimates and using the
means as node estimates. With enough points, the convergence
specified in Theorems 5.1—5.3 allows us to distinguish and estimate
nodes if the mean duration at the levels is long enough with enough
transitions. From the convergence result for point estimates (ref to
equation 5.A.7) we see an increase in accuracy as the number of
paths and the number of observations per duration at a given level
increase. This holds even more strongly for the node estimates (refer
to equation 5.A.8) and also increases as the total number of points at
a given level increases.

5.A.3 MARKOV CHAIN PARAMETER ESTIMATES

The conditional transition probability from state i to state j is given
by:

Theorem 5.3.

p= Ai (5.4.10)
ij- >on)

dia Ais
This formula allows estimates for transition probabilities to be

computed using the empirical transition rates. Confidence intervals
for these transition rates for individual cases may be found using the
\n\n=== PAGE 170 ===\nbinomial approximation (for two states) or a multinomial
distribution or Pearson’s χ2 test with more states.
In practice, the larger the window or frame of points used, the
greater the percentage of correct estimates. One problem we
encountered is the way in which isolated errors affect the estimates
for mean duration at a certain state. If the algorithm erroneously
detects a jump in volatility in the middle of a stay at a certain state,
this effectively halves the duration time at the state. With large
enough data, the problem goes away. However, this usually has
rather absolute practical limitations. The quantitative relationship of
probability of isolated errors to accuracy of mean durations
estimates seems to be very complicated.
5.A.4 EMPIRICAL TESTING
In this section, we generate observations, using a known Markov
chain (ground truth). Then we perform the estimation procedure and
we compare the estimates with the known values.
We performed simulation results for 2, 3, and 4 node Markov chains.
We simulated 2000 observation points (which translates into about a
week of minute financial data), 8–10 thousand simulation points per
path and about 50–200 observations per state duration. In Figure
5.12, we present the algorithm output for a 4-node estimation. The
first image plots the real volatility (black line) and the estimated
means (green points). The second image presents the histogram of
the means (green) and the fitted gaussian mixture distribution on
top. Finally, the third image displays the real volatility (black line)
and the estimated points restricted to the node values. In general, the
algorithm needs several observations at a new level, before it detects
a volatility shift. For the results obtaining extra test data, we refer the
reader to the supplementary document Supplement A.
\n\n=== PAGE 171 ===\nFIGURE 5.12 Test data: 4 Nodes (0.1, 0.3, 0.5, and 0.75).
5.A.5 A LIST OF SUPPLEMENTARY DOCUMENTS
Because of the page constraint of this document, we were forced to
leave out many results and images that we felt would have enhanced
the work and demonstrated the extensive testing put into this work.
Thus we have added three documents that are visible to the links
provided as well as on the journal site.
Supplement A: Selected test results using 2, 3, and 4
nodes This pdf file presents test results using data generated
from a process with known parameter values. The values we
chose resemble the numerical values, frequencies, and distances
\n\n=== OCR PAGE 171 ===\n4y level in (0.1, 0.3, 0.5, 0.75} vs y estimate levels Discriminant fit - means {0.111, 0.323, 0.509, 0.711}

08 0.10
fl ‘ 0.08
06 - } By | =
3
2 | { grr
= ’ =
5 o44 i |
3 ' | 4 & £ 0.04
Li 3s
} é
024 | is : 0.02
2 | \)
ue 200
a
o 1 2 3 4 5 6 02 00 02 04 06 08 10 12
Days Volatility
10,000 paths 2500 points 10,000 trials 2500 observation points

Test input levels {0.1, 0.3, 0.5, 0.75)/est.
out {0.111, 0.323, 0.509, 0.711}

1.0
08
2064 T
= —_ -
S 04 |
= bk
0.2 |
— sa lod
0.0

0.000 0.005 0.010 0.015 0.020 0.025

Years
10,000 paths 2500 points

FIGURE 5.12 Test data: 4 Nodes (0.1, 0.3, 0.5, and 0.75).
5.A.5 A LIST OF SUPPLEMENTARY DOCUMENTS

Because of the page constraint of this document, we were forced to
leave out many results and images that we felt would have enhanced
the work and demonstrated the extensive testing put into this work.
Thus we have added three documents that are visible to the links
provided as well as on the journal site.

Supplement A: Selected test results using 2, 3, and 4
nodes This pdf file presents test results using data generated
from a process with known parameter values. The values we
chose resemble the numerical values, frequencies, and distances
\n\n=== PAGE 172 ===\nbetween nodes as observed in our applications. Some values were
chosen because of their difficulty in estimation (nodes close to
each other) to illustrate the power of the algorithm.
http://www.math.stevens.edu/~ifloresc/SupplementA.pdf
Supplement B: Other stock volatility analysis This pdf file
presents volatility runs obtained from minute stock data during
the week of the Bear Stearns collapse. We use financial stocks and
commodity stock to illustrate the differences in the volatility
patterns.
http://www.math.stevens.edu/~ifloresc/SupplementBStocks.pdf
Supplement C: Temperature variability results This pdf
file presents volatility runs obtained from the climate data in
Central Park, New York City, from 2000 to 2010.
http://www.math.stevens.edu/~ifloresc/SupplementCClimate.pd
f
References
Aït-Sahalia, Y., and R. Kimmel (2007). Maximum likelihood
estimation of stochastic volatility models. Journal of Financial
Economics 83, 413–452.
Barndorff-Nielsen, O. E., and N. Shephard (2002). Non-Gaussian
Ornstein-Uhlenbedk–based models and some of their uses in
financial economics. Journal of the Royal Statistical Society B 64.
Black, F., and M. Scholes (1973). The valuation of options and
corporate liability. Journal of Political Economy 81, 637–654.
Bladt, M., and M. Sorensen (2007). Simple simulation of diffusion
bridges with application to likelihood inference for diffusions.
Preprint.
Borcherdt, R. D., M. J. S. Johnston, G. Glassmoyer, and C. Dietel
(2004). Recordings of the 2004 Parkfield earthquake on the general
earthquake observation system array. Technical report, U.S.
Geological Survey. Open-file Report 2004-1376, October 11, 2004.
Borcherdt, R. D., M. J. S. Johnston, G. Glassmoyer, and C. Dietel
(2006, June). Recordings of the 2004 Parkfield earthquake on the
\n\n=== PAGE 173 ===\ngeneral earthquake observation system array. Bulletin of the
Seismological Society of America 96(4B), S73–S89.
Chib, S., M. K. Pitt, and N. Shephard (2004, August). Likelihood
based inference for diffusion driven models. Economics Papers
2004-W20, Economics Group, Nuffield College, University of
Oxford.
Cho, S., R. Haralick, and S. Yi (1989). Improvement of Kittler and
Illingworth’s Minimum Error Thresholding. Pattern Recognition
22(5), 609–617.
Cohan, W. D. (2009). House of Cards. Thorndike Press.
Cont, R., and P. Tankov (2003). Financial Modelling with Jump
Processes. Chapman & Hall/CRC.
CSMIP (2004). Strong-motion data from the Parkfield earthquake of
20 december 1994. Technical report, California Strong Motion
Instrumentation Program, Division of Mines, Department of
Conservation. Open-file Report 2004–1376, October 11, 2004.
Cvitanić, J., R. Liptser, and B. Rozovskii (2006). A filtering approach
to tracking volatility from prices observed at random times. Annals
of Applied Probability 16(3), 1633–1652.
Cvitanić, J., B. Rozovskii, and I. Zaliapin (2006). Numerical
estimation of volatility values from discretely observed diffusion
data. The Journal of Computational Finance 9(4), 1–36.
Dash, M. (2008, September). Bank agrees to buy troubled loan giant
for 4 billion. New York Times. 9-25-08 archive resource;
www.nytimes.com.
DelMoral, P. (2004). Feynman–Kac Formulae: Genealogical and
Interacting Particle Systems with Applications. Springer.
Del Moral, P., J. Jacod, and P. Protter (2001). The Monte-Carlo
method for filtering with discrete time observations. Probability
Theory and Related Fields 120, 346–368.
Florescu, I., and C. Tudor (2013, November). Handbook of
Probability. John Wiley & Sons, Inc.
Geithner, T. (2008). The Current Financial Challenges: Policy and
Regulatory Implications. New York: Council on Foreign Relations
Corporate Conference 2008. March 6: Remarks by Timothy
Geithner, President and CEO of the Federal Reserve Bank of New
York.
Genon-Catalot, V., T. Jeantheau, and C. Larédo (2000). Stochastic
volatility models as hidden Markov models and statistical
\n\n=== PAGE 174 ===\napplications. Bernoulli 6(6), 1051–1079.
Hamilton, J. D. (2005). Regime-switching models. Palgrave
Dictionary of Economics.
Hamilton, J. D., and G. Lin (1996). Stock market volatility and the
business cycle. Journal of Applied Econometrics, 11, 573–593.
Landon and Sorkin (2008, March). J.P. Morgan acts to buy ailing
bear stearns at huge discount. New York Times. 3-15-08 archive
resource; www.nytimes.com.
Levin, F. (2010). Monte Carlo estimation of stochastic volatility for
stock values and potential applications to temperature and
seismographic data. PhD thesis, Stevens Institute of Technology,
Castle Point on the Hudson, Hoboken, NJ, USA.
Mariani, M., I. Florescu, M. B. Varela, and E. Ncheuguim (2009,
April). Long correlations and levy models applied to the study of
memory effects in high frequency (tick) data. Physica A 388(8),
1659–1664.
Merton, R. C. (1992). Continuous-Time Finance (Macroeconomics
and Finance). Wiley-Blackwell.
Nielsen, J. N., and M. Vestergaard (2000). Estimation in
continuous-time stochastic volatility models using nonlinear filters.
International Journal of Theoretical and Applied Finance 3(2),
279–308.
Protter, P. E. (2005). Stochastic Integration and Diferential
Equations (2nd ed.). Springer.
Ross Sorkin, A. (2008, March). JP Morgan pays 2 dollars a share for
Bear Stearns. New York Times. 3-17-08 archive resource;
www.nytimes.com.
Shephard, N. (Ed.) (2005). Stochastic Volatility: Selected Readings.
Oxford University Press.
Sorensen, H. (2003). Simulated likelihood approximations for
stochastic volatility models. Scandinavian Journal of Statistics
30(2), 257–276.
Tsay, R. (2005). Analysis of Financial Time Series (2nd ed.). John
Wiley & Sons, Inc.
U.S. Senate Committee on Banking, Housing and Urban Affairs
(2008). Actions by the New York Fed in Response to Liquidity
Pressures in Financial Markets. U.S. Senate Committee on Banking,
Housing and Urban Affairs. April 3: Testimony given by Timothy
\n\n=== PAGE 175 ===\nGeithner, President and CEO of the Federal Reserve Bank of New
York.
\n\n=== PAGE 176 ===\nChapter Six
Detecting Jumps in High-Frequency Prices Under
Stochastic Volatility: A Review and a Data-Driven Approach
Ping-Chen Tsai1 and Mark B. Shackleton2
1Department of Finance, Southern Taiwan University of Science and Technology,
Yongkang, Tainan City, Taiwan.
2Department of Accounting and Finance, Lancaster University Management School,
Lancaster, England, United Kingdom.
6.1 Introduction
Jumps in prices have been shown to be an indispensable characteristic of asset returns from
a modeling perspective,1 or when volatility models are calibrated with option data.2 Over
the past decade, nonparametric methods afforded by the advent of high-frequency data
have made the direct identification of jumps possible and uncovered many more jumps than
are suggested by parametric models [2; 15; 16; 62; 63].3 Many of these tests are, however,
executed at daily level and therefore rejecting the null of no jumps in effect indicates that at
least one jump has occurred during a trading day. The exact timing, direction, and size, that
is, the empirical distributions of individual jumps are unknown from the daily test results.
When testing for simultaneous arrivals of jumps across different assets,4 or for the
correlation between jumps in price and in volatility,5 it is desirable that jumps in prices can
be identified up to an intraday interval. Assuming that in a finite time span there are only a
finite number of jumps in prices6 [7], and [58] devise jump tests to be implemented for
high-frequency returns. Although the two studies propose similar test statistics, they differ
in estimating the spot volatility of returns and in controlling for spurious detection due to
multiple comparisons. In the study by Andersen et al. [7], hereafter ABD test, the
bandwidth chosen to estimate spot volatility may include high-frequency returns before and
after the interval under consideration, whereas in the study by Lee and Mykland [58],
hereafter LM test, only the most recent high-frequency returns are used. To correct for
spurious detection, the ABD and LM tests, respectively, consider the finite-sample
distribution and the asymptotic distribution for the maxima of their test statistics to
determine the rejection region.
We give a detailed review on the ABD and LM tests in Section 6.2. In particular, we discuss
the validity of the smooth-volatility assumption made by the LM test. If volatility does not
change abruptly over the neighborhood of the return interval being tested, the ABD and LM
tests would give identical results, ceteris paribus. Despite being constantly used to prove
asymptotic theorems [55; 61], the smooth-volatility assumption may not be empirically
valid in the context of high-frequency jump detection.7 In ABD (2007), it is pointed out that
because of stochastic volatility, returns standardized by ex ante volatility measures are
necessarily fat-tailed and so a forward-looking test is doomed to be biased toward
overrejection. The ABD test is thus implemented in a backward-looking manner. In the two
studies, however, the ABD test is moderately oversized whereas the LM test is undersized.8
We find that this unexpected result on the LM test is due to a printing error in the original
\n\n=== PAGE 177 ===\npaper of Lee and Mykland [58], and show via simulation that a corrected version of the LM
test indeed gives more spurious detections than the ABD test.
The printing error in the LM test is pertinent to the normalizing constants of the maxima of
test statistics, which have a limiting Gumbel distribution under the null hypothesis of i.i.d.
normal variables. Since both the ABD test and the corrected LM test present overrejection,
their critical regions can be modified. We note that the Gumbel distribution can be
interpreted as the generalized extreme value (GEV) distribution with a shape parameter ξ
approaching zero [32]. Accordingly, by admitting a strictly positive shape parameter ξ in the
GEV distribution, namely a GEV distribution of Fréchet type, the overrejection issue can be
resolved. This innovated methodology represents our main contribution to the literature.
The non-zero-shape parameter ξ can thus be seen as bias correction for high-frequency
jump tests under stochastic volatility.9
To formalize these ideas, in Section 6.3 we devise a data-driven procedure that generalizes
the ABD and LM tests in estimating spot volatility and in selecting critical regions. An
intermediate volatility measure between those of the two tests is expected to possess
superior size properties than the LM test. We study the performance of our tests under a
volatility model that consists of a two-factor structure, a compound Poisson process for
jumps in prices, and correlated diffusions between prices and volatility. The model is
directly estimated from our data, which are high-frequency Spyder (SPY) returns during
January 2002 and April 2010. Simulation shows that when the sizes of tests are controlled
at a given nominal level (1%), the backward-looking test has the highest power in detecting
jumps in prices.
In the simulation, the shape parameter ξ of GEV distribution is estimated between 0.075
and 0.089 for the backward-looking test to have correct sizes. The value of ξ has to increase
for the forward-looking tests to show no overrejection. In the empirical analysis, we use the
calibrated values of ξ from simulation to detect jumps in SPY returns. For three sampling
frequencies, our test identifies jumps with daily intensities from 6.12% at the 10-min
frequency to 28.7% at the 2-min frequency. In particular, at the 10-min frequency the
estimated daily intensity (>6%) is close to the results in [45], in which a daily jump intensity
around 5.6% is recorded for SPY data at a 11-min sampling frequency.
Our empirical results indicate that jump arrivals tend to cluster at the higher sampling
frequencies, and that the empirical distributions of identified jumps, after the embedded
intraday volatility pattern (IVP) is removed, do not appear to be Gaussian. The chapter
contributes to the literature by designing a framework in which the jump tests are
approximately exempt from size distortion and thus the testing procedure with the highest
detecting power can be determined. Moreover, the shape parameter ξ of GEV distribution
for the maxima of test statistics can be interpreted as the degree of deviation from the null
of i.i.d. Gaussian. Our work hence complements those in [28; 39], which modify and
improve the jump tests by accounting for IVP and proposing a bootstrap rejection region.
Closest in spirit to our study is Corsi et al. [28], who use a threshold function scaled by spot
volatility to detect jumps, with the optimal scaling coefficient also determined by
simulation.
This chapter is organized as follows. Section 6.2 reviews the intraday jump tests that are
constructed using realized volatility measures. Section 6.3 describes the Spyder data and
introduces a generalized testing procedure. Simulation study is performed in Section 6.4,
and empirical results are presented in Section 6.5. Section 6.6 concludes.
\n\n=== PAGE 178 ===\n(6.1)
(6.2)
(6.3)
(6.4)
(6.5)
6.2 Review on the intraday jump tests
6.2.1 REALIZED VOLATILITY MEASURE AND THE BNS TESTS
In financial economics, the no-arbitrage axiom requires that asset prices dynamics follow a
semimartingale process.10 Specifically, the efficient log-price process Xt defined on a filtered
probability space 
 can be decomposed as:
where At is of finite variation and Mt is a local martingale [15].11 A special case is the Itô
semimartingale with Poisson jumps in prices:
where a = (at)t ≥ 0 is a locally bounded predictable drift function, σ = (σt)t ≥ 0 an adapted
càdlàg volatility process, W = (Wt)t ≥ 0 a standard Brownian motion, N(t) a counting
process, and Jk the magnitude of price jumps [31]. Given the specification in (6.2), the total
variation of price process is measured by the quadratic variation QV [71]:
for a partition of time 0 = t0 < t1 < … < tM = t. The QV then consists of the continuous [X]c
t
and the discontinuous component [X]d
t:
In (6.4), the first term is also known as the integrated volatility (IV) while the second term is
the QV due to jumps. Given the definition of QV in (6.3), and assuming a frictionless market
structure, it is natural to estimate the QV of prices from 0 to t by the sum of squared
returns, known as the realized variance measure RV [6; 14]:
where Δ is the length of a high-frequency interval and ⌊t/Δ⌋ is the integer part of t/Δ. We
follow the convention in the literature and set t = 1. In (6.5), Xt, iΔ − Xt, (i − 1)Δ = rt, iΔ is thus
the i-th high-frequency return with length Δ on day t. Denote the number of high-frequency
returns per day by M = ⌊1/Δ⌋; it is seen that as Δ shrinks to 0 or equivalently as M → ∞, RV
converges to the QV of price process.12
\n\n=== OCR PAGE 178 ===\n6.2 Review on the intraday jump tests
6.2.1 REALIZED VOLATILITY MEASURE AND THE BNS TESTS

In financial economics, the no-arbitrage axiom requires that asset prices dynamics follow a
semimartingale process.*? Specifically, the efficient log-price process X, defined on a filtered
probability space (Q,F, (F;)20:P) can be decomposed as:

X,=X9+A,+M,,t > 0, (6.1)

where A; is of finite variation and M, is a local martingale [15]." A special case is the It6
semimartingale with Poisson jumps in prices:

Ni) (6.2),

t t
x =X + [ade [ o,aW,+ Yd
0 0 fel

where a = (a,);5 9 is a locally bounded predictable drift function, o = (0,);, 9 an adapted
cadlag volatility process, W = (W,); = 9 a standard Brownian motion, N(t) a counting
process, and J; the magnitude of price jumps [31]. Given the specification in (6.2), the total
variation of price process is measured by the quadratic variation QV [71]:

M
F 2 (6.3)
[X1, = plimy cs DX, — Xi,

i=l
for a partition of time 0 = ty < t, < ... < ty = t. The QV then consists of the continuous [X]°,
and the discontinuous component [xX]:

r (6.4)
[X], = [X], + [XI = / o2du+ Yo. -,I2:
0

In (6.4), the first term is also known as the integrated volatility (IV) while the second term is
the QV due to jumps. Given the definition of QV in (6.3), and assuming a frictionless market
structure, it is natural to estimate the QV of prices from 0 to t by the sum of squared
returns, known as the realized variance measure RV [6; 14]:

Wal (6.5),
RV, = > (X1ia — XiG-ya)”>
i=

where A is the length of a high-frequency interval and Lt/A] is the integer part of t/A. We
follow the convention in the literature and set t = 1. In (6.5), X, in ~ Xp, @—-ya =", ia is thus
the i-th high-frequency return with length A on day t. Denote the number of high-frequency

returns per day by M = |1/A]; it is seen that as A shrinks to 0 or equivalently as M > 0, RV

converges to the QV of price process./#
\n\n=== PAGE 179 ===\n(6.6)
(6.7)
(6.8)
(6.9)
Barndorff-Nielsen and Shephard [15; 16] propose to estimate the continuous part of QV,
using the co-called bipower variation (BP):
where μa = E(|Z|a), Z ~ N(0, 1) and a > 0, that is, 
. Barndorff-
Nielsen and Shephard [15; 16] show that BP is a consistent estimator of IV in the presence
of jumps, as the probability of a single jump influencing both returns rt, i and rt, i − 1 is
asymptotically zero—provided no consecutive jumps occurred. Therefore, an estimator for
the jump part of QV is the difference RV − BP. A test statistic can be constructed for the null
hypothesis of no jumps, given the bivariate distribution of RV and BP; in [51], the following
statistic is recommended:
where RJt = (RVt − BPt)/RVt and TPt is the tripower quarticity estimator for the daily
integrated quarticity. The null hypothesis of no jumps for day t is rejected if ZBNS, t is larger
than the critical value from the standard normal distribution at some significance level. In
the simulation study of Barndorff-Nielsen and Shephard [16], although the test has a mild
degree of overrejection, its power in detecting jumps is relatively low.13 Nevertheless,
empirical results from References [16; 51] find many more jumps in prices than are
suggested by the parametric models in [40]. Further theoretical results on BP and on
multipower variation are given in [19; 78].14
6.2.2 THE ABD AND LM TESTS
To identify jumps in high-frequency prices, ABD (2007) considers a high-frequency return
rt, j and show that under the null of no jumps, the scaled high-frequency return Δ− 1/2rt, j has
a vanishing mean and a variance approaching the integrated variance IV:
where Δ = 1/M. ABD (2007) suggests that although the exact distribution of Δ− 1/2rt, j is
unknown, under the null of no jumps it is approximately Gaussian. Assuming that spot
volatility is constant within a trading day, the ABD test detects a jump in prices if:
where Φ1 − β/2 is the critical value from a standard normal distribution for significance level
β. Equivalently, under the null of no jumps the high-frequency return rt, j standardized by
its spot volatility 
 is a standard normal variable. The rationale of the ABD test is
thus consistent with the comment in [17] that “… non-Gaussian increments and jumps are
synonymous.”
\n\n=== OCR PAGE 179 ===\nBarndorff-Nielsen and Shephard [15; 16] propose to estimate the continuous part of QV,
using the co-called bipower variation (BP):

M M

where [lg = E(|Z|%), Z ~ N(o, 1) and a > 0, that is, fa = 2°T(0.5(a + 1))/ vz, Barndorff-
Nielsen and Shephard [15; 16] show that BP is a consistent estimator of IV in the presence
of jumps, as the probability of a single jump influencing both returns r, ; and r;, ;_ , is
asymptotically zero—provided no consecutive jumps occurred. Therefore, an estimator for
the jump part of QV is the difference RV - BP. A test statistic can be constructed for the null
hypothesis of no jumps, given the bivariate distribution of RV and BP; in [51], the following

statistic is recommended:
1 TP, (6.7).
0.609— max {| 1, — ] ~ N(0,1),
M BP?

t

(6.6)

NIN

Zpns. = RJ,/

where RJ; = (RV; - BP,)/RV; and TP; is the tripower quarticity estimator for the daily
integrated quarticity. The null hypothesis of no jumps for day t is rejected if Zgys, ; is larger
than the critical value from the standard normal distribution at some significance level. In
the simulation study of Barndorff-Nielsen and Shephard [16], although the test has a mild
degree of overrejection, its power in detecting jumps is relatively low.*? Nevertheless,
empirical results from References [16; 51] find many more jumps in prices than are
suggested by the parametric models in [40]. Further theoretical results on BP and on
multipower variation are given in [19; 78].14

6.2.2 THE ABD AND LM TESTS

To identify jumps in high-frequency prices, ABD (2007) considers a high-frequency return
r;,j and show that under the null of no jumps, the scaled high-frequency return A~ 1: °r,,jhas
a vanishing mean and a variance approaching the integrated variance IV:

} (6.8),
limy .£ [A7'?r,,] = 0, lim, var [A-'/?,,] = | ofdu,
m1
where A = 1/M. ABD (2007) suggests that although the exact distribution of A~ °r, jis

unknown, under the null of no jumps it is approximately Gaussian. Assuming that spot
volatility is constant within a trading day, the ABD test detects a jump in prices if:

|ril /VBP,/M > ®_p)> (6.9)

where ®, _ 72 is the critical value from a standard normal distribution for significance level
B. Equivalently, under the null of no jumps the high-frequency return r,, ; standardized by
its spot volatility V BP,/M ig a standard normal variable. The rationale of the ABD test is

thus consistent with the comment in [17] that “... non-Gaussian increments and jumps are
synonymous.”
\n\n=== PAGE 180 ===\n(6.10)
(6.11)
(6.12)
In the ABD test, the spot volatility of return rt, j, denoted by σt, j, is estimated by 
and so the test cannot be performed until the end of trading hours when all intraday returns
are realized. In [58], on the contrary, the detection of jumps is done on a real-time basis.
For a high-frequency return rt, j, its spot volatility σt, j is estimated using past returns up to
interval (j − 1) on day t:
Specifically, in (6.10), the first (k − 1) returns from a local window of size k are used to
estimate σt, j. The sum in (6.10) is proportional to BPt and so the window size k should be
sufficiently large such that the asymptotic property of BP being a consistent, jump-robust
estimator of IV holds. From their simulation results, Lee & Mykland [58] suggest 
,
where T is the number of days in the sample. For a year with 250 trading days, sampling at
5-min frequency for a day with 24 trading hours (exchange rate data) would give k = 268
(rounded to integer).
The ABD and LM tests define their test statistics as the absolute standardized return |rt, j|/
σt, j, with the spot volatility σt, j being estimated differently. To select a critical region, ABD
(2007) considers for a daily significance level α the distribution of the maximal observation
from M intraday statistics, maxj = 1..M(|rt, j|/σt, j). Under the null hypothesis, each of the M
statistics is independent standard normal, and hence the distribution function of maxj =
1..M(|rt, j|/σt, j) is given by the probability of M independent binomial trials with no success:
where Bin(M, β) is the binominal variable with success probability β, which is also the
intraday significance level, that is, β = 2(1 − FΦ(u)), with FΦ( · ) the standard normal CDF.
The all-acceptance probability (1 − β)M in (6.11) is thus equal to 1 − α and so:
Rasmussen [69] and Boudt et al. [28] point out that the adjustment between α and β in
(6.12) is the Šidàk correction for tests with multiple comparison. A common alternative is
the Bonferroni procedure, which gives β = α/M [1]. In their simulation study, ABD (2007)
finds that their test is moderately oversized, with an empirical size of 0.153% for 2-min
returns given a 0.1% level. As a result, a more effective correction is needed.15
To solve the overrejection problem due to multiple testing, Lee and Mykland [58] consider
the asymptotic distribution for the maximum of test statistics from a sample of size n = MT.
Under the null of no jumps, with σt, j given in (6.10) the LM test statistics are proportional
to standard normal and are asymptotically independent. Denote the largest observation of
statistics by Mn, Lee and Mykland [58] use extreme value theory to show that there are two
constants cn and sn such that Mn normalized by these constants has a limiting Gumbel
distribution, as n → ∞:
\n\n=== OCR PAGE 180 ===\nIn the ABD test, the spot volatility of return r, ;, denoted by 0; ;, is estimated by V BP,/M
and so the test cannot be performed until the end of trading hours when all intraday returns
are realized. In [58], on the contrary, the detection of jumps is done on a real-time basis.
For a high-frequency return r,, ;, its spot volatility 0, ; is estimated using past returns up to
interval (j - 1) on day t:

(6.10)

Irial [ial
i=j—k42

Specifically, in (6.10), the first (k - 1) returns from a local window of size k are used to
estimate 0; ;. The sum in (6.10) is proportional to BP; and so the window size k should be
sufficiently large such that the asymptotic property of BP being a consistent, jump-robust
estimator of IV holds. From their simulation results, Lee & Mykland [58] suggest k= VMT,
where T is the number of days in the sample. For a year with 250 trading days, sampling at
5-min frequency for a day with 24 trading hours (exchange rate data) would give k = 268
(rounded to integer).

The ABD and LM tests define their test statistics as the absolute standardized return |r, ;|/
©;, » With the spot volatility 0, ; being estimated differently. To select a critical region, ABD
(2007) considers for a daily significance level a the distribution of the maximal observation
from M intraday statistics, max; - ,_ (|r, ;|/0;, )). Under the null hypothesis, each of the M

statistics is independent standard normal, and hence the distribution function of max; _
M(t, jl /0;,;) is given by the probability of M independent binomial trials with no success:

(6.11),

Pr} max, <u|=Pr(Bin(M, fp) =0)=(1- py”,

j=1M

where Bin(M, B) is the binominal variable with success probability B, which is also the
intraday significance level, that is, B = 2(1 — Fg(u)), with Fg(- ) the standard normal CDF.

The all-acceptance probability (1 - py” in (6.11) is thus equal to 1 - a and so:

p=1-(1-a)'™. (6.12)

Rasmussen [69] and Boudt et al. [28] point out that the adjustment between a and £ in
(6.12) is the Sidak correction for tests with multiple comparison. A common alternative is
the Bonferroni procedure, which gives B = a/M [1]. In their simulation study, ABD (2007)
finds that their test is moderately oversized, with an empirical size of 0.153% for 2-min
returns given a 0.1% level. As a result, a more effective correction is needed.!5

To solve the overrejection problem due to multiple testing, Lee and Mykland [58] consider
the asymptotic distribution for the maximum of test statistics from a sample of size n = MT.
Under the null of no jumps, with o;, ; given in (6.10) the LM test statistics are proportional
to standard normal and are asymptotically independent. Denote the largest observation of
statistics by M,,, Lee and Mykland [58] use extreme value theory to show that there are two
constants c,, and s, such that M,, normalized by these constants has a limiting Gumbel
distribution, as n — o:
\n\n=== PAGE 181 ===\n(6.13)
(6.14)
(6.15)
(6.16)
where
in which 
. The LM test detects a jump in rt, j if:
given a significance level α. In other words, if the test statistic after centered and scaled by
cn and sn is larger than the critical value of Gumbel distribution, which is the asymptotic
distribution for the largest observation from n i.i.d. Gaussian variables, we have confidence
in rejecting the null of no jumps in rt, j.
Table 6.1 Simulation on Šidàk correction and Gumbel critical values, 1% level.
Frequency
2 min 5 min 10 min
M
195
78
39
Šidàk correction
0.0105 0.0104 0.0099
Gumbel: cn in (6.14) 0.0047 0.0039 0.0030
Gumbel: cn in (6.16) 0.0121 0.0103 0.0084
Table 6.1 reports the average rejection rates of Šidàk correction and Gumbel critical values from 1000 samples of i.i.d. N(0,
1) variables at a 1% daily level α. Each sample is of length 1 year, that is, 250 trading days; each day with 6.5 trading hours
is sampled every 2, 5, and 10 min and thus M is 195, 78, and 39. The Šidàk correction has standard normal critical value Φ1
− β/2, where β = 1 − (1 − α)1/M. The Gumbel critical value − log( − log(1 − α)) in (6.15) is evaluated with cn from (6.14)
and (6.16), respectively.
Here we note that in (6.14), the equation for cn contains a printing error, and the correct
expression is given in [3; p. 46]16:
Importantly, the missing factor “4” in cn explains the very conservative, undersized results
in [58]. To quantify the impact of this factor, we simulate 1000 samples of i.i.d. N(0, 1)
variables and calculate over these samples the average rejection rate given a 1% daily level.
Each sample is of length 1 year, that is, 250 trading days; each day with 6.5 trading hours is
sampled every 2, 5, and 10 min and thus M is 195, 78, and 39. To permit direct comparison
between the ABD and LM tests, we let k = M = n, that is, both tests estimate spot volatility
with the same bandwidth M. In Table 6.1, the reported result suggests that the Šidàk
correction used in the ABD test performs well. The correct expression of cn gives rejection
rate close to the nominal 1% level at the 5-min frequency; on the contrary, the Gumbel
critical values with cn in (6.14) are severely undersized across three frequencies.
\n\n=== OCR PAGE 181 ===\nPr{(M,, —c,,)/s,, < u} ~ Gumbel (wu) = exp (—exp(—u)), (6.13)

where

C, = V2 log(n)/c — flog (x) + log (log(n))| / (2 Vv Zlog(n)) : (6.14),
5, =1/ (cV2I08@) :
in which © = V2/7, The LM test detects a jump in 7, ; if:

— -c,]/s, > —log (log (1 — @)),

o,;

given a significance level a. In other words, if the test statistic after centered and scaled by
c,, and s, is larger than the critical value of Gumbel distribution, which is the asymptotic
distribution for the largest observation from n i.i.d. Gaussian variables, we have confidence
in rejecting the null of no jumps in ry, ;.

Table 6.1 Simulation on Sidak correction and Gumbel critical values, 1% level.
Frequency 2min 5 min 10 min
M 195 78 39
Sidak correction 0.0105 0.0104 0.0099
Gumbel: c, in (6.14) 0.0047 0.0039 0.0030
Gumbel: c, in (6.16) 0.0121 0.0103 0.0084

Table 6.1 reports the average rejection rates of Sidak correction and Gumbel cri s from 1000 samples of i.

1) variables at a 1% daily level a. Each sample is of length 1 year, that is, 250 trading days; each day with 6.5 trading hours
is sampled every 2, 5, and 10 min and thus M is 195, 78, and 39. The Sidak correction has standard normal critical value
~ B/a» Where B= 1 - (1 a)". The Gumbel critical value — log( - log(1 - «)) in (6.15) is evaluated with cy from (6.14)
and (6.16), respectively.

Here we note that in (6.14), the equation for c, contains a printing error, and the correct
expression is given in [3; p. 46}°:

c, = 2 log(n) — [log (4x) + log (log(n))| / (2v2I08) : (6.16),
Importantly, the missing factor “4” in c, explains the very conservative, undersized results
in [58]. To quantify the impact of this factor, we simulate 1000 samples of i.i.d. N(0, 1)
variables and calculate over these samples the average rejection rate given a 1% daily level.
Each sample is of length 1 year, that is, 250 trading days; each day with 6.5 trading hours is
sampled every 2, 5, and 10 min and thus M is 195, 78, and 39. To permit direct comparison
between the ABD and LM tests, we let k = M = n, that is, both tests estimate spot volatility
with the same bandwidth M. In Table 6.1, the reported result suggests that the Sidak
correction used in the ABD test performs well. The correct expression of c,, gives rejection
rate close to the nominal 1% level at the 5-min frequency; on the contrary, the Gumbel
critical values with c,, in (6.14) are severely undersized across three frequencies.
\n\n=== PAGE 182 ===\nWe conclude this section by summarizing the many empirical works that have implemented
the two tests. Table 6.2 lists some of these studies; it can be seen that much of attention has
been paid to the effect of microstructure noise on the tests. This issue is either mitigated by
sampling at a safe frequency or directly accounted for using noise-robust volatility
measures. For example, Bos et al. [27] use the preaveraging estimator of Jacod et al. [52] to
obtain consistent estimates of IV in the presence of noise and jumps. On the other hand,
Boudt et al. [28] show that it is necessary for both the ABD and LM tests to be adjusted for
the IVP to avoid size distortion. In terms of rejection region, most studies simply follow
those given by the ABD and LM tests.
Table 6.2 List of studies on ABD and LM tests.
Studies
onintraday
jumptests
Test
performed
Microstructure
noise
IVP
Data &
overnight
period
Critical
region
selection
[27]
LM
Preaveraged BP
Adjusted
FX data
24h
same as LM
[28]
ABD, LM
5-min return
Adjusted#
FX data
excluding
weekend
same as LM
[39]
ABD, LM
Simulated
Noted/unadjusted Equity
excluding
overnight
period
bootstrap
approximation
[41]
ABD
Staggered 5-min
return
Noted/unadjusted Futures
data from
08:20 EST
same as ABD
[45]
ABD, LM
Simulated; 11-
min return
Same as #
Equity
data from
09:35 EST
Controlled at
nominal level
[57]
ABD, LM
15-min return
Same as #
FX 24h;
futures
from
08:30 EST
same as LM
[59]
LM
15-min return
Controlled after
detection
Equity
including
overnight
period
same as LM
[69]
LM
Modeled and
corrected
Unadjusted
Equity
data
same as ABD
[75]
LM
Simulated;
staggered return
Adjusted
Equity,
futures &
FX w/o
overnight
same as LM
Table 6.2 lists nine studies that implement or modified the ABD and LM tests and summarize how issues such as
microstructure noise, IVP, overnight period, and selection of critical region are addressed in these papers.
\n\n=== PAGE 183 ===\n6.3 A data-driven testing procedure
6.3.1 SPY DATA AND MICROSTRUCTURE NOISE
The data studied in this chapter are the traded prices of SPY, an exchange traded fund
designed to represent ownership of the S&P 500 Index. SPY trading has a high liquidity,
which avoids sampling errors at high frequency and reduces the stale price effect [60]. SPY
is shown to have significant contribution to the price discovery of the S&P 500 Index [50].
Many studies also analyze high-frequency SPY data.17 From the NYSE TAQ database, we
download the traded prices of SPY during the trading hours 09:30–16:00 EST from January
2, 2002, to April 30, 2010. The cleaning procedure described in [21] is used to remove
incomplete trading days and unusual price records, leaving us a total of 2076 days of SPY
price records.
The efficient price process observed at ultra-high frequency is subject to various forms of
distortion caused by the structure of market, such as price discreteness and bid-ask bounce
effect [11; 47]. Consequently, realized volatility measures constructed from returns sampled
at or above some frequencies may be contaminated by these microstructure noises. In the
literature, there are three main approaches in dealing with the effect of noises18; in this
study, we follow the conventional approach of seeking a frequency at which RV is not
influenced by noises [5]. In Figure 6.1, we plot RV (averaged over the data period) against
sampling frequency; it can be seen that there is an upward bias in RV above the 1-min
frequency. Accordingly, we choose to sample the SPY data at the 2-, 5-, and 10-min
frequencies [65] and ABD (2007) also sample the S&P 500 Index and Index futures at the 2-
min frequency.
FIGURE 6.1 Volatility signature plot of RV, SPY.
Figure 6.1 plots the average RV of SPY in annual standard deviation against the sampling
frequency of intraday prices. The average RV over our sample period increases above the 1-
min frequency.
\n\n=== OCR PAGE 183 ===\n6.3 A data-driven testing procedure

6.3.1 SPY DATA AND MICROSTRUCTURE NOISE

The data studied in this chapter are the traded prices of SPY, an exchange traded fund
designed to represent ownership of the S&P 500 Index. SPY trading has a high liquidity,
which avoids sampling errors at high frequency and reduces the stale price effect [60]. SPY
is shown to have significant contribution to the price discovery of the S&P 500 Index [50].
Many studies also analyze high-frequency SPY data.17 From the NYSE TAQ database, we
download the traded prices of SPY during the trading hours 09:30-16:00 EST from January
2, 2002, to April 30, 2010. The cleaning procedure described in [21] is used to remove
incomplete trading days and unusual price records, leaving us a total of 2076 days of SPY
price records.

The efficient price process observed at ultra-high frequency is subject to various forms of
distortion caused by the structure of market, such as price discreteness and bid-ask bounce
effect [11; 47]. Consequently, realized volatility measures constructed from returns sampled
at or above some frequencies may be contaminated by these microstructure noises. In the
literature, there are three main approaches in dealing with the effect of noises!8; in this
study, we follow the conventional approach of seeking a frequency at which RV is not
influenced by noises [5]. In Figure 6.1, we plot RV (averaged over the data period) against
sampling frequency; it can be seen that there is an upward bias in RV above the 1-min
frequency. Accordingly, we choose to sample the SPY data at the 2-, 5-, and 10-min
frequencies [65] and ABD (2007) also sample the S&P 500 Index and Index futures at the 2-
min frequency.

0.30 - - - -
—o— Avg. RV
0.25 F |
0.20 F 4
0.15 + 4

10s 30s min 2min 3min Smin 6min 10min 13min 15min 26min 30min

FIGURE 6.1 Volatility signature plot of RV, SPY.

Figure 6.1 plots the average RV of SPY in annual standard deviation against the sampling
frequency of intraday prices. The average RV over our sample period increases above the 1-
min frequency.
\n\n=== PAGE 184 ===\n(6.17)
We summarize some empirical aspects of SPY data. First, the intraday SPY returns have a
typical leptokurtic distribution and become more so at the higher frequencies [73]. A
diurnal effect in the autocorrelations (ACFs) of absolute returns, equivalently an IVP, is
visible with a peak at 10:00 EST [24]. Second, for daily volatility measures RV and BP, both
of their logarithms are close to Gaussian but with fatter right tails. The slow decay of ACFs
for log volatility measures is evident, yet augmented Dickey–Fuller test rejects the null
hypothesis of unit root [8]. The XCFs of RV and BP with respect to daily open-to-close
returns confirm a significant leverage effect. Third, consistent with [18], the jump
component RV − BP has a modest degree of serial correlation. The XCFs of RV − BP with
respect to open-to-close returns, on the contrary, do not suggest any signs for leverage effect
[25]. For the ratio RJ = (RV − BP)/RV, our estimates range between 7.1% and 8.7% and are
comparable to the estimate 7% in [51]. Fourth, we find that daily open-to-close returns
standardized by RV or BP are approximately Gaussian but have thin tails, an observation
also documented in [42]. Further details are given in a report available upon request.
6.3.2 A GENERALIZED TESTING PROCEDURE
6.3.2.1 Spot volatility estimation
As discussed in the “Introduction” section, the ABD test detects jumps retrospectively (ex
post) whereas the LM test does it in a forward-looking manner (ex ante). The two tests
would give identical results if the smooth-volatility assumption holds:
Figure 6.2 plots the 2-min returns of SPY during the 2-day period from October 9 to 10,
2008, with the overnight period posts a negative return of −5.54%.
where ϵ > 0, Δt = ti + 1 − ti (Assumption 1; [58]). For the studies in Table 6.2, few question
the empirical validity of (6.17). The presence of overnight and weekend periods, however,
could potentially undermine this assumption. As an illustration, in Figure 6.2, we plot the 2-
min returns of SPY during the 9th and 10th of October 2008. It can be seen that the
overnight period posts a large negative return (−5.54%) and the data is much more volatile
on the second day than the first day. Indeed, BP on October 10, 2008, is estimated to be at
least 2.5 times larger than the previous day.19 The 2-day period in Figure 6.2 hence
represents an example in which spot volatility changes (increases) unexpectedly, with the
overnight period being the change point. In Table 6.3, we construct contingency tables of
overnight returns with respect to changes in BP and find that the null of no association is
soundly rejected, providing further evidence against the smooth volatility assumption in
(6.17).
\n\n=== OCR PAGE 184 ===\nWe summarize some empirical aspects of SPY data. First, the intraday SPY returns have a
typical leptokurtic distribution and become more so at the higher frequencies [73]. A
diurnal effect in the autocorrelations (ACFs) of absolute returns, equivalently an IVP, is
visible with a peak at 10:00 EST [24]. Second, for daily volatility measures RV and BP, both
of their logarithms are close to Gaussian but with fatter right tails. The slow decay of ACFs
for log volatility measures is evident, yet augmented Dickey—Fuller test rejects the null
hypothesis of unit root [8]. The XCFs of RV and BP with respect to daily open-to-close
returns confirm a significant leverage effect. Third, consistent with [18], the jump
component RV - BP has a modest degree of serial correlation. The XCFs of RV - BP with
respect to open-to-close returns, on the contrary, do not suggest any signs for leverage effect
[25]. For the ratio RJ = (RV - BP)/RV, our estimates range between 7.1% and 8.7% and are
comparable to the estimate 7% in [51]. Fourth, we find that daily open-to-close returns
standardized by RV or BP are approximately Gaussian but have thin tails, an observation
also documented in [42]. Further details are given in a report available upon request.

6.3.2 A GENERALIZED TESTING PROCEDURE

6.3.2.1 Spot volatility estimation

As discussed in the “Introduction” section, the ABD test detects jumps retrospectively (ex
post) whereas the LM test does it in a forward-looking manner (ex ante). The two tests
would give identical results if the smooth-volatility assumption holds:

Figure 6.2 plots the 2-min returns of SPY during the 2-day period from October 9 to 10,
2008, with the overnight period posts a negative return of —5.54%.

where € > 0, At = t; , , — t; (Assumption 1; [58]). For the studies in Table 6.2, few question
the empirical validity of (6.17). The presence of overnight and weekend periods, however,
could potentially undermine this assumption. As an illustration, in Figure 6.2, we plot the 2-
min returns of SPY during the 9th and 10th of October 2008. It can be seen that the
overnight period posts a large negative return (—5.54%) and the data is much more volatile
on the second day than the first day. Indeed, BP on October 10, 2008, is estimated to be at
least 2.5 times larger than the previous day.*? The 2-day period in Figure 6.2 hence
represents an example in which spot volatility changes (increases) unexpectedly, with the
overnight period being the change point. In Table 6.3, we construct contingency tables of
overnight returns with respect to changes in BP and find that the null of no association is
soundly rejected, providing further evidence against the smooth volatility assumption in
(6.17).
\n\n=== PAGE 185 ===\n(6.18)
(6.19)
Table 6.3 Contingency table of overnight returns w.r.t. changes in BP, SPY.
2-min
5-min
10-min
Frequency ron
t − < 0 ron
t − > 0 ron
t − < 0 ron
t − > 0 ron
t − < 0 ron
t − > 0
BPt < BPt − 1
396
621
406
632
428
638
BPt > BPt − 1
539
498
529
487
507
481
χ2
(3) test
35.20
(1.1E-7)
34.84
(1.3E-7)
25.78
(1.1E-5)
Table 6.3 reports the contingency tables for the association between overnight returns and changes in BP. The overnight
returns ront − refer to those before day t. Twenty overnight returns of 0 are excluded in the table. Across three frequencies,
χ2 test statistics indicate that the association is significant, with p values in the parenthesis smaller than 0.1%.
FIGURE 6.2 Two-min SPY returns on October 9 and 10, 2008.
The observation in Table 6.3 that BP tends to increase following negative overnight returns
reinforces the leverage effect of volatility. Therefore, to obtain a suitable ex ante volatility
measure E[BPt|It − 1], we consider and modify the HAR model of Corsi [33] as:
In (6.18), log (BP)l, l = d, w, m, q, are the past daily, weekly, monthly, and quarterly log of
BP, with 
, and roc
l and ron
l are the corresponding variables
for past open-to-close and overnight returns. A residual analysis of ut in Appendix 6.A leads
to the MA (6.2) design in (6.19). The model thus has an approximate long memory structure
with leverage effect.20 In Table 6.4, we report the summary statistics of roc
t standardized by
\n\n=== OCR PAGE 185 ===\nTable 6.3 Contingency table of overnight returns w.r.t. changes in BP, SPY.
2-min 5-min 10-min

Frequency r™,_ <0 7r,_ > 0 7r°,_ <0 r™,_ >07r°,_ <O7r°",_ >0

BP, < BP; _, 396 621 406 632 428 638
BP,>BP;_, 539 498 529 487 507 481
X() test 35-20 (11E-7) 34.84 (13E-7) 25.78 (.-1E-5)

Table 6. reports the contingency tables for the association between overnight returns and changes in BP. The overnight
returns 1; _ refer to those before day t. Twenty overnight returns of o are excluded in the table. Across three frequencies,

x2 test statistics indicate that the association is significant, with p values in the parenthesis smaller than 0.1%.

0.06

0.04 +

0.02

0,06 We
Open 9th Oct Close/open 10th Oct Close

FIGURE 6.2 Two-min SPY returns on October 9 and 10, 2008.
The observation in Table 6.3 that BP tends to increase following negative overnight returns
reinforces the leverage effect of volatility. Therefore, to obtain a suitable ex ante volatility
measure E[BP,|J;_ ,], we consider and modify the HAR model of Corsi [33] as:
log (BP), = c + b, log (BP), + 6, log (BP),, + 5; log (BP), + b, log (BP), (6.18),
Ft sr + ber + bar + byr" + byr" + bygr" + u,

U, = &, + Dye,_1 + Pre,-2,&, ~ N (0,02). (6.19),
In (6.18), log (BP), 1 = d, w, m, q, are the past daily, weekly, monthly, and quarterly log of
5
BP, with log(BP),, = (1/5) Yin log(BP),_;, and r°; and r°”, are the corresponding variables
for past open-to-close and overnight returns. A residual analysis of u, in Appendix 6.A leads

to the MA (6.2) design in (6.19). The model thus has an approximate long memory structure
with leverage effect.2° In Table 6.4, we report the summary statistics of r°°, standardized by
\n\n=== PAGE 186 ===\n(6.20)
BPt − 1, E[BPt|It − 1] and BPt. The degree of nonnormality in 
 is evident but
greatly reduced when compared with 
.
Table 6.4 Standardized returns with different information flow of BP, SPY.
Frequency 2 min
5 min
2 min
5 min
2 min
5 min
Mean
−0.0067 −0.0084 −0.0078 −0.0087 0.0486 0.0511
SD
1.1321
1.1853
1.0650
1.0960
1.0632 1.0937
Skewness
−0.4292 −0.4313 −0.3609 −0.3571 0.0183 0.0002
Kurtosis
4.3609
4.4987
3.6492
3.6468
2.7744 2.7045
JB Test
223.82
258.51
81.53
80.32
4.5174
7.5525
p value
<0.1%
<0.1%
<0.1%
<0.1%
0.1007* 0.0243
Table 6.4 reports the summary statistics of daily open-to-close returns roct of SPY standardized by different information
flow of BP, with E[BPt|It − 1] obtained from the HAR model in (6.18) and (6.19). Note that in the calculation the sample
means of roct are removed. The results for 10-min frequency are similar and thus are not reported to save space. The JB
Test stands for Jarque–Bera test for normality.
The fact that 
 is closer to normal than 
 is relevant in jump
detection. The leverage effect in volatility as captured by past returns in (6.18) is reflected in
E[BPt|It − 1], but this is ignored by the LM test in spot volatility estimation. In this study, we
therefore introduce a third way in estimating spot volatility by interpolating E[BPt|It − 1]
with realized high-frequency returns:
for 3 ≤ j ≤ M. For the first two intervals in a day (j = 1, 2), the spot volatility is estimated
from E[BPt|It − 1], because E[BPt|It − 1] is a less-biased measure than BPt − 1 in obtaining spot
volatility estimates. In the following, we will compare jump test results when the forward-
looking (LM test), backward-looking (ABD test), and the interpolated BP are used to
estimate spot volatility. It is obvious that the three measures converge as j → M.
6.3.2.2 Selecting a critical region
In Table 6.1, we have seen that for i.i.d. normal variables, the Šidàk correction and the
Gumbel critical values give test results with correct sizes. For empirical data, the test
statistics |rt, j|/σt, j are likely to be dependent as spot volatility σt, j often involves estimation
errors. In this case, the extreme value theory is still feasible, provided that the extreme
observations are nearly independent or sufficiently distant in time [32]. This condition will
be assumed to hold for our SPY data.
\n\n=== OCR PAGE 186 ===\nBP; _ ,, E[BP;|J;_ ,] and BP;. The degree of nonnormality in */VEIBP,|L,-1] is evident but
greatly reduced when compared with’ °° / VBP,
Table 6.4 Standardized returns with different information flow of BP, SPY.

r°°/4/BP,_, r°/y/ EIBP, [l,i] r°°//BP,

Frequency2min 5min 2min 5min 2min 5min

Mean -0.0067 -0.0084 —0.0078 -0.0087 0.0486 0.0511
SD 1.1321 1.1853 1.0650 1.0960 1.0632 1.0937
Skewness -0.4292 -0.4313 -0.3609 -0.3571 0.0183 0.0002
Kurtosis 4.3609 4.4987 3.6492 3.6468 2.7744 2.7045
JB Test 223.82 258.51 81.53 80.32 4.5174 7.5525
p value <0.1% <0.1%  <0.1% <0.1% 09,1007" 0.0243

Table 6.4 reports the summary statistics of daily open-to-close returns r°°; of SPY standardized by different information
flow of BP, with E[BP;|J; — ] obtained from the HAR model in (6.18) and (6.19). Note that in the calculation the sample
means of r°°; are removed. The results for 10-min frequency are similar and thus are not reported to save space. The JB
Test stands for Jarque—Bera test for normality.

‘The fact that + / V£[BP,I/,_11 is closer to normal than’*’/ VBP, is relevant in jump
detection. The leverage effect in volatility as captured by past returns in (6.18) is reflected in
E[BP,|J; - ,], but this is ignored by the LM test in spot volatility estimation. In this study, we
therefore introduce a third way in estimating spot volatility by interpolating E[BP,|J;_ ,]

with realized high-frequency returns:

(6.20)
Interpolated BP = (

a

(=) }

for 3 <j < M. For the first two intervals in a day (j = 1, 2), the spot volatility is estimated

5

a

M-j+l

+
M

from E[BP,|J;_ ,], because E[BP,|I;_ 1
volatility estimates. In the following,
looking (LM test), backward-looking
estimate spot volatility. It is obvious tl
6.3.2.2 Selecting a critical region

In Table 6.1, we have seen that for i.i.

is a less-biased measure than BP; _ , in obtaining spot

we will compare jump test results when the forward-

(ABD test), and the interpolated BP are used to
at the three measures converge as j > M.

. normal variables, the Sidak correction and the

Gumbel critical values give test results with correct sizes. For empirical data, the test
statistics |r, ;|/o,, ; are likely to be dependent as spot volatility 0, ; often involves estimation

errors. In this case, the extreme value

theory is still feasible, provided that the extreme

observations are nearly independent or sufficiently distant in time [32]. This condition will

be assumed to hold for our SPY data.
\n\n=== PAGE 187 ===\n(6.21)
(6.22)
(6.23)
(6.24)
To identify a critical region that efficiently eradicates size distortion due to multiple
comparisons and due to test statistics being dependent, we consider the GEV distribution,
which nests the Gumbel distribution as a special case. Defined on the support
, the GEV distribution is given as
where 
 is the location parameter, 
 is the scaled parameter, and − ∞ < ξ <
∞ is the shape parameter. The expression in (6.21) is given by the limiting distribution of the
binomial variable in (6.11) as M → ∞, that is, a Poisson distribution with intensity measure
. If 
, 
, and ξ → 0, the GEV distribution admits the Gumbel
distribution in (6.13). As a result, the Gumbel critical value − log( − log(1 − α)) can be
generalized by allowing a nonzero ξ to give the GEV critical value, and the decision rule
(6.15) then becomes
where cn and sn are given in (6.16). It can be shown that the GEV critical value is larger than
the Gumbel value when ξ > 0 and vice versa when ξ < 0. Since in ABD (2007) the
backward-looking ABD test is moderately oversized, we expect a strictly positive ξ to be
used in (6.22) to eliminate spurious detection.
Standard maximum likelihood method is available for the estimation of the GEV
parameters including ξ [32]. However, as the data itself contains jumps, the estimated ξ
from the maximums of test statistics |rt, j|/σt, j cannot be directly used in the test.21 What is
needed is a ξ value such that the decision rule in (6.22) correctly rejects the null of no jumps
given significance level α. For this purpose, we therefore resort to simulation study to
calibrate ξ at which the tests have correct sizes. As a result, the non-zero-shape parameter ξ
in GEV distribution may be regarded as a measure of deviation from i.i.d. normal for the
test statistics.22
6.4 Simulation study
6.4.1 MODEL SPECIFICATION
For the calibrated ξ to be credible, data must be simulated from a model that closely
resembles the empirical SPY returns. From Section 6.3.1, such a model has a persistent but
mean-reverting volatility, a jump component in prices, correlated return innovations with
volatility, and a close-to-normal distribution for log volatility. We thus specify the model as:
\n\n=== OCR PAGE 187 ===\nTo identify a critical region that efficiently eradicates size distortion due to multiple
comparisons and due to test statistics being dependent, we consider the GEV distribution,
which nests the Gumbel distribution as a special case. Defined on the support
{u:1+&(u-L)/S > 0} the GEV distribution is given as

ey (6.21)
.

where —oo < £ < ois the location parameter, S > 0 is the scaled parameter, and - © < &<
co is the shape parameter. The expression in (6.21) is given by the limiting distribution of the
binomial variable in (6.11) as M —> ©, that is, a Poisson distribution with intensity measure
[l+é(u- 2/5)" 1¢¢ = 0,5 = 1, and §— 0, the GEV distribution admits the Gumbel
distribution in (6.13). As a result, the Gumbel critical value — log( - log(1 - a)) can be
generalized by allowing a nonzero & to give the GEV critical value, and the decision rule
(6.15) then becomes

GEV (u) = exp {-

In| : (6.22)
ay /s, > —(1/&) [1 -(- log (1 — @))*],

where c,, and s, are given in (6.16). It can be shown that the GEV critical value is larger than
the Gumbel value when € > 0 and vice versa when & < 0. Since in ABD (2007) the
backward-looking ABD test is moderately oversized, we expect a strictly positive § to be
used in (6.22) to eliminate spurious detection.

Standard maximum likelihood method is available for the estimation of the GEV
parameters including § [32]. However, as the data itself contains jumps, the estimated &
from the maximums of test statistics |r, ;|/o;, ; cannot be directly used in the test.2 What is
needed is a § value such that the decision rule in (6.22) correctly rejects the null of no jumps
given significance level a. For this purpose, we therefore resort to simulation study to
calibrate € at which the tests have correct sizes. As a result, the non-zero-shape parameter €
in GEV distribution may be regarded as a measure of deviation from i.i.d. normal for the
test statistics.°?

6.4 Simulation study

6.4.1 MODEL SPECIFICATION

For the calibrated & to be credible, data must be simulated from a model that closely
resembles the empirical SPY returns. From Section 6.3.1, such a model has a persistent but
mean-reverting volatility, a jump component in prices, correlated return innovations with
volatility, and a close-to-normal distribution for log volatility. We thus specify the model as:

dlog (p(t) = [H - svoo| dr+ VV@pdwiny + (dN), (6.23),

log (V(t) = 6 + log (V,() + log (V,(0)) : (6.24)
\n\n=== PAGE 188 ===\n(6.25)
(6.26)
(6.27)
(6.28)
(6.29)
In (6.23), the log-price process has drift μr − V(t)/2, volatility 
 for the Brownian
motion W(t)p and jumps J(t)dN(t), with Poisson arrivals N(t) ~ Poi(λt) and normally
distributed jump sizes J(t) ~ N(0, σ2
j). With a mean θ, the log variance log (V(t)) in (6.24)
has two components log(V1(t)) and log(V2(t)), each following an Ornstein–Uhlenbeck (OU)
process with κi the rates of mean reversion and σi the volatility for the Brownian motions
W(t)v
i in (6.25). The two-factor structure has been widely used in the literature to describe
the long-range dependence in volatility.23 For generating the leverage effect in volatility, we
require W(t)p and W(t)v
i to be negatively correlated:
Given the two-factor structure, the jumps in prices and the two correlation coefficients ρi,
the model is labeled SV2FJ_2ρ and has 10 parameters: {μr, θ, κ1, σ1, κ2, σ2, λ, σj, ρ1, ρ2}.
To obtain the parameter estimates of the SV2FJ_2ρ model, we first assume that μr can be
derived from the sample mean of open-to-close returns. For the parameters of the two-
factor volatility structure {θ, κ1, σ1, κ2, σ2}, we consider estimating an ARMA (2, 1) model for
the daily observations of log (BP):
where (ϕ1, ϕ2) are the AR coefficients, ψ is the MA coefficient, and ϵt is serially uncorrelated
with normal distribution ϵt ~ N(0, σ2
ϵ). It is well-established that an ARMA (2, 1) model can
be decomposed into sum of two independent AR (1) processes if the two roots (α1 and α2) to
the quadratic equation x2ϕ(x− 1) = 0, where ϕ(L) = 1 − ϕ1L − ϕ2L2 is the AR (2) polynomial,
are both real-valued [49]. In this case, α1 and α2 are the AR coefficients of the individual AR
(1) processes. In Appendix 6.B, we confirm that this condition indeed holds for log (BP) of
SPY. Accordingly, we have the decomposition:
where each of log(Vi)t is itself an AR (1) process:
\n\n=== OCR PAGE 188 ===\ndlog (V,(t)) = x; [-log (V()) | dr+o,dW!, i= 1,2. (6.25),

In (6.23), the log-price process has drift 1. — V(4)/2, volatility Wo for the Brownian
motion W(t)? and jumps J(t)dN(o), with Poisson arrivals N(t) ~ Poi(At) and normally
distributed jump sizes J(t) ~ N(o, o*). With a mean 9, the log variance log (V(#)) in (6.24)
has two components log(V,(t)) and log(V,(8), each following an Ornstein—Uhlenbeck (OU)
process with x; the rates of mean reversion and g; the volatility for the Brownian motions
W(t)"; in (6.25). The two-factor structure has been widely used in the literature to describe
the long-range dependence in volatility.23 For generating the leverage effect in volatility, we
require W(t)? and W(t)”; to be negatively correlated:

dw(tydW(t)! = pdt, pj) <0, i=1,2. (6.26)

Given the two-factor structure, the jumps in prices and the two correlation coefficients p;,
the model is labeled SV2FJ_2p and has 10 parameters: {1,, 8, K,, 01, Kz, 05, A, OF Py» Po}.
To obtain the parameter estimates of the SV2FJ_2p model, we first assume that pi, can be

derived from the sample mean of open-to-close returns. For the parameters of the two-
factor volatility structure {0, «,, 0,, k,, 6}, we consider estimating an ARMA (2, 1) model for

the daily observations of log (BP):

log (BP), = « + ¢, (log (BP),_, — ) (6.27)
+ o, (log (BP),_, — #) + we,_, +&,

where (#,, ),) are the AR coefficients, p is the MA coefficient, and €, is serially uncorrelated
with normal distribution €, ~ N(0, 07,). It is well-established that an ARMA (2, 1) model can
be decomposed into sum of two independent AR (1) processes if the two roots (a, and a,) to
the quadratic equation x(x" *) = 0, where @(L) = 1 - ,L — @,L? is the AR (2) polynomial,
are both real-valued [49]. In this case, a, and a, are the AR coefficients of the individual AR

(1) processes. In Appendix 6.B, we confirm that this condition indeed holds for log (BP) of
SPY. Accordingly, we have the decomposition:

log (BP), = » + log (V,), + log (V2) (6.28)

e

where each of log(V;); is itself an AR (1) process:

log (V;), =a; (log (V;),_,) + Ej, Exp ~N (0.02 ) , i=1,2. (6.29)
\n\n=== PAGE 189 ===\n(6.30)
(6.31)
Table 6.5 Estimation of ARMA (2, 1) and OU process parameters, log(BP) SPY.
Sum of two AR (1) processes
Gaussian-OU processes
2 min
5 min
10 min
2 min
5 min
10 min
μ
−9.6302
−9.7447
−9.7840 θ
−9.6302 −9.7447 −9.7840
α1
0.9870
0.9860
0.9875 κ1
0.0131
0.0141
0.0126
σϵ, 1
0.1642
0.1703
0.1625 σ1
0.1653
0.1715
0.1635
α2
0.2413
0.2007
0.2012 κ2
1.4217
1.6059
1.6035
σϵ, 2
0.3445
0.3900
0.4419 σ2
0.5986
0.7135
0.8079
Table 6.5 reports the parameters of the OU processes in SV2FJ_2ρ model, converted from the estimated ARMA (2, 1)
model for log(BP) of SPY. The parameters reported are expressed for daily interval. The discrete-time parameters of
ARMA (2, 1) model are converted into the continuous-time parameters of OU processes using the relation in (6.30). The
details of ARMA (2, 1) estimation are given in Appendix 6.B. The mean-reversion parameters κ1 and κ2 correspond to half-
life of around 52 and 0.5 days, respectively.
Note that in (6.28), we follow Alizadeh et al. [4] and do not separate μ between the two AR
(1) processes. Hence, the discrete-time (6.28) and (6.29) have their continuous-time
equivalents in (6.24) and (6.25), as an AR (1) process is the discrete-time counterpart of an
OU process. We then have the conversion between the parameters:
In Table 6.5, we give the estimated discrete-time parameters and the corresponding values
of {θ, κ1, σ1, κ2, σ2}; the continuous-time parameters in (6.30) are expressed for daily
interval.
For the jump parameters {λ, σj}, since in (6.23) the Poisson arrival N(t) is independent from
the zero-mean jump size J(t), it can be shown that:
\n\n=== OCR PAGE 189 ===\nTable 6.5 Estimation of ARMA (2, 1) and OU process parameters, log(BP) SPY.

Sum of two AR (1) processes Gaussian-OU processes

2min 5min 10min 2min 5min 10min
Hu -9.6302 -9.7447, -9.7840 9 -9.6302 -9.7447 -9.7840
a, 0.9870 0.9860 0.9875 Kk, 0.0131 0.0141 0.0126
Ger 0.1642 0.1703 0.1625 0, 0.1653 0.1715 0.1635
ay 0.2413 0.2007, 0.2012 Kk, 1.4217. 1.6059 1.6035
Oe 0 0.3445 0.3900 0.4419 6, 0.5986 0.7135 0.8079

Table 6.5 reports the parameters of the OU processes in SV2FJ_2p model, converted from the estimated ARMA (2, 1)
model for log(BP) of SPY. The parameters reported are expressed for daily interval. The discrete-time parameters of
ARMA (2, 1) model are converted into the continuous-time parameters of OU processes using the relation in (6,30). The
details of ARMA (2, 1) estimation are given in Appendix 6.B. The mean-reversion parameters x, and kg correspond to half-
life of around 52 and 0.5 days, respectively.

Note that in (6.28), we follow Alizadeh et al. [4] and do not separate j1 between the two AR
(1) processes. Hence, the discrete-time (6.28) and (6.29) have their continuous-time
equivalents in (6.24) and (6.25), as an AR (1) process is the discrete-time counterpart of an
OU process. We then have the conversion between the parameters:

(1.0,02,/ (1 -«)) = (0, e“ia?/2K;), i= 1,2. (6,30)

In Table 6.5, we give the estimated discrete-time parameters and the corresponding values
of {0, k,, O,, Ka, 02}; the continuous-time parameters in (6.30) are expressed for daily
interval.

For the jump parameters {A, 0}, since in (6.23) the Poisson arrival N(t) is independent from
the zero-mean jump size J(t), it can be shown that:

(6.32),

tlesst

E[RV, - BP,] = e( DY 69 ) = EINE (J (5) = EIN, ]6?.
\n\n=== PAGE 190 ===\n(6.32)
Table 6.6 Parameter values of SV2FJ_2ρ model, SPY.
Frequency 2 min
5 min
10 min
μr
−0.0001 −0.0001 −0.0001
θ
−9.6302 −9.7447 −9.7840
κ1
0.0131
0.0141
0.0126
σ1
0.1653
0.1715
0.1635
κ2
1.4217
1.6059
1.6035
σ2
0.5986
0.7135
0.8079
ρ1
−0.4750 −0.5000 −0.5500
ρ2
−0.1500 −0.1500 −0.1500
E[RVt − BPt] 9.1E-6
1.3E-5
1.6E-5
E[Nt]
0.08
0.08
0.08
σj
0.0107
0.0127
0.0141
E[Nt]
0.40
0.40
0.40
σj
0.0048
0.0057
0.0063
Table 6.6 collects the 10 parameter values of SV2FJ_2ρ model in (6.23) to (6.26) calibrated from our SPY data. The
parameter values are expressed for daily interval. Given the number of jumps per day E[Nt], the jump size variance
parameter σj is computed using E[RVt − BPt] = E[Nt]σ2j in (6.31); in the table the reported E[RVt − BPt] are sample
means of positive (RVt − BPt). The two correlation parameters ρi, i = 1, 2, are obtained by minimizing the loss function
loss(ρ1, ρ2) in (6.32).
Given the expected number of jumps per day E[Nt], the value of σj can thus be calculated by
setting E[RVt − BPt] equal to the sample mean of (RVt − BPt). As a sensitivity analysis, we
choose E[Nt] to be 0.08 and 0.40. Thus, when jumps are frequent, the variance of jump size
becomes small, and vice versa, as shown in Table 6.6. For the two correlation parameters
{ρ1, ρ2}, which determine the leverage effect, we define a quadratic loss function in terms of
the XCFs of log (BP) with respect to past open-to-close returns:
where XCFsim.
logBP is the average XCFs of log (BP) over 1000 simulations from SV2FJ_2ρ
model, in which {ρ1, ρ2} are selected from a grid of candidate values and the rest eight
parameters are determined as described in Section 6.4.1.24 Minimizing the loss function
(6.32) then gives the optimal values for {ρ1, ρ2}; see Appendix 6.C for details. Using this
calibration procedure, we thus successfully obtain from our SPY data the 10 parameter
values of SV2FJ_2ρ model, which are presented in Table 6.6.
Figure 6.3 shows the ACFs, in the top, and XCFs (w.r.t. open-to-close returns), in the
bottom, of log(BP) for SPY and for the fitted SV2FJ_2ρ model. The simulated ACFs and
XCFs are obtained as averages over 1000 simulations of the SV2FJ_2ρ model using
parameters listed in Table 6.6.
\n\n=== OCR PAGE 190 ===\nTable 6.6 Parameter values of SV2FJ_2p model, SPY.

Frequency 2min 5min 10min

H,. 0.0001 -0.0001 -0.0001
8 9.6302 -9.7447 -9.7840
K, 0.0131 0.0141 0.0126
0, 0.1653 0.1715 0.1635
ky 1.4217. 1.6059 1.6035
O02 0.5986 0.7135 0.8079
Pp, -0.4750 -0.5000 —0.5500
Po -0.1500 -0.1500 -0.1500
E[RV,- BP;] 9.1E-6 1.3E-5  1.6E-5
E(N;] 0.08 0.08 0.08

9; 0.0107 0.0127. 0.0141
E(N;] 0.40 0.40 0.40

0; 0.0048 0.0057 0.0063

,j
Table 6.6 collects the 10 parameter values of SV2FJ_2p model in (6.23) to (6.26) calibrated from our SPY data. The
parameter values are expressed for daily interval. Given the number of jumps per day ELN¢], the jump size variance
parameter oj is computed using E[RV; - BP;] = E[N;]o?; in (6.31); in the table the reported El
means of positive (RV; — BP;). The two correlation parameters pj, i = 1, 2, are obtained by mini
loss(p,, Pg) in (6.32).

,~ BP{] are sample
zing the loss function

Given the expected number of jumps per day E[N;], the value of o; can thus be calculated by
setting E[RV; — BP;] equal to the sample mean of (RV; - BP,). As a sensitivity analysis, we
choose E[N;] to be 0.08 and 0.40. Thus, when jumps are frequent, the variance of jump size

becomes small, and vice versa, as shown in Table 6.6. For the two correlation parameters
{P,, Po}, which determine the leverage effect, we define a quadratic loss function in terms of
the XCFs of log (BP) with respect to past open-to-close returns:

20
. si 2 (6.32),
loss (9). 2) = x (XCFicespine - XCF so) ’

lag=0

where XC sim. opp is the average XCFs of log (BP) over 1000 simulations from SV2FJ_2p
model, in which {p,, p,} are selected from a grid of candidate values and the rest eight
parameters are determined as described in Section 6.4.1.24 Minimizing the loss function
(6.32) then gives the optimal values for {p,, p.}; see Appendix 6.C for details. Using this
calibration procedure, we thus successfully obtain from our SPY data the 10 parameter
values of SV2FJ_2p model, which are presented in Table 6.6.

Figure 6.3 shows the ACFs, in the top, and XCFs (w.r.t. open-to-close returns), in the
bottom, of log(BP) for SPY and for the fitted SV2FJ_2p model. The simulated ACFs and

XCFs are obtained as averages over 1000 simulations of the SV2FJ_2p model using
parameters listed in Table 6.6.
\n\n=== PAGE 191 ===\nFIGURE 6.3 The ACFs and XCFs of log(BP) from SPY and SV2FJ_2ρ Model.
In Figure 6.3, we plot the averaged ACFs and XCFs (w.r.t. open-to-close returns) of log (BP)
over 1000 simulations of SV2FJ_2ρ model, together with their empirical counterparts of
SPY. To save space, we only present the results when BP is constructed from the 2- and 5-
min frequencies. At short lags the simulated ACFs approximate the empirical ones well, but
at longer lags the simulated ACFs are not sufficiently persistent. The fit may be improved by
estimating an ARMA (3, 2) model for log (BP) of SPY but at the cost of losing parsimony. On
the other hand, in the bottom panel simulated XCFs of log (BP) match well with the
empirical values, except at lag 1. The estimated values of ρ1, which account for most of the
leverage effect, center around −0.50 and are similar to the estimates in Eraker et al. [40].25
We therefore consider the SV2FJ_2ρ model and its parameter values a reasonable
characterization of SPY data.
6.4.2 SIMULATION RESULTS
In the simulation, the length of data is 1275 days; after the first 25 days are discarded, the
jump tests are applied to the last 250 days of simulated data. The in-sample period for the
estimation of HAR model is therefore 1000 days. For three sampling frequencies and two
jump dynamics, the SV2FJ_2ρ model is simulated 1000 times and so the test size and
power are obtained as averages over 1000 simulations. The jump tests are implemented at a
1% significance level; accordingly, the nominal size is 2.50 for a test period of 250 days. As
in previous studies, we define test size as the number of false rejections and test power as
the percentage of simulated jumps that are correctly identified.
\n\n=== OCR PAGE 191 ===\nSvar J2p —sv2Fi2p
08 —log (BP) os —log (BP)
—zero zero
06
0.4
02
°
50 «100~=S—«50.~=—S200—S—«250 50 100 150 200 250
2m 5m
02 02
* Data * Data
[- sv2FJ2p | Sv2FJ2p
o4 01
TM | A
0.4 O41 |
-20 0 20 -20 0 20

FIGURE 6.3 The ACFs and XCFs of log(BP) from SPY and SV2FJ_2p Model.

In Figure 6.3, we plot the averaged ACFs and XCFs (w.r.t. open-to-close returns) of log (BP)
over 1000 simulations of SV2FJ_2p model, together with their empirical counterparts of
SPY. To save space, we only present the results when BP is constructed from the 2- and 5-
min frequencies. At short lags the simulated ACFs approximate the empirical ones well, but
at longer lags the simulated ACFs are not sufficiently persistent. The fit may be improved by
estimating an ARMA (3, 2) model for log (BP) of SPY but at the cost of losing parsimony. On
the other hand, in the bottom panel simulated XCFs of log (BP) match well with the
empirical values, except at lag 1. The estimated values of p,, which account for most of the

leverage effect, center around —0.50 and are similar to the estimates in Eraker et al. [40].2°
We therefore consider the SV2FJ_2p model and its parameter values a reasonable
characterization of SPY data.

6.4.2 SIMULATION RESULTS

In the simulation, the length of data is 1275 days; after the first 25 days are discarded, the
jump tests are applied to the last 250 days of simulated data. The in-sample period for the
estimation of HAR model is therefore 1000 days. For three sampling frequencies and two
jump dynamics, the SV2FJ_2p model is simulated 1000 times and so the test size and
power are obtained as averages over 1000 simulations. The jump tests are implemented at a
1% significance level; accordingly, the nominal size is 2.50 for a test period of 250 days. As
in previous studies, we define test size as the number of false rejections and test power as
the percentage of simulated jumps that are correctly identified.
\n\n=== PAGE 192 ===\nWe expect to observe the following outcomes in the simulation. First, under the same
critical region, the backward-looking test should have the least overrejection, while the test
using interpolated BP should outperform the forward-looking test. Second, we shall see
that tests with Šidàk correction and with Gumbel critical values are oversized. Hence, the
shape parameter ξ of GEV distribution for the tests to have correct size should be strictly
positive. Third, between the two scenarios of jump dynamics, the tests should have less
power in detecting the smaller but frequent jumps. Fourth, a higher data frequency may
enable higher testing power.
Table 6.7 reports the simulation results on the jump tests when E[Nt] is equal to 0.08. In
the left two columns, the tests with Šidàk correction are indeed seriously oversized, and as
expected tests using forward-looking BP (middle panel) display the strongest degree of
overrejection. The size distortion is more pronounced at the lower frequencies. In the
central two columns, tests with Gumbel critical values show overrejection across three
panels. Between the two corrections, tests with Gumbel critical values have more false
detections at the higher frequencies than tests with Šidàk correction, and vice versa at the
lower frequencies. This observation is consistent with the results in Table 6.1. The right
three columns of Table 6.7 report the calibrated values of ξ at which the tests have correct
sizes. To obtain these values of ξ, we submit an array of candidate values to the decision rule
(6.22) and interpolate to identify ξ such that the test sizes are close to 2.50. The calibration
process is tabulated in Appendix 6.D. The calibrated values of ξ are all positive, consistent
with our conjecture that solving the oversized problem requires a strictly positive ξ in the
GEV distribution. Moreover, the values of ξ for the backward-looking test (between 0.0840
and 0.089) are smaller than those for the forward-looking and interpolated tests. Again,
this result is as expected. The value of ξ can thus be interpreted as a measure of deviation
from i.i.d. normal variables for the jump test statistics.
\n\n=== PAGE 193 ===\nTable 6.7 Simulation results of jump tests from SV2FJ_2ρ model, E[Nt] = 0.08.
Šidàk correction
Gumbel c.v.
Correct size ≅ 2.50
Frequency
Size
Power
Size
Power
ξ
Size
Power
Backward-looking BP
2 min
6.9920
79.32%
7.7340
79.48% 0.0890 2.4950 77.83%*
5 min
7.1980
75.61%
7.1300
75.59% 0.0870 2.4930 73.35%*
10 min
7.6700
70.07%
6.7680
69.67% 0.0840 2.5020 66.37%
Forward-looking BP
2 min
15.8080
79.67%
17.2040 79.81% 0.1602 2.5070 76.70%
5 min
18.8430
76.97%
18.6920 76.96% 0.1782 2.4920 71.95%
10 min
24.1360
72.20%
21.9780 71.92% 0.2000 2.5100 65.02%
Interpolated BP
2 min
11.7940
79.54%
12.8440 79.66% 0.1380 2.4750 76.96%
5 min
12.4430
76.41%
12.3230 76.38% 0.1410 2.4850 72.75%
10 min
13.7340
71.57%
12.4670 71.35% 0.1460 2.5050 66.61%*
Table 6.7 reports the simulation results of jump tests under the SV2FJ_2ρ model with E[Nt] = 0.08. The tests are applied
to the last 250 days in the simulated data at 1% level, giving a nominal size of 2.50. The top, middle, and bottom panels are
the results when the backward-looking, forward-looking, and interpolated BP are used to estimate spot volatility. Each
panel has results obtained with Šidàk correction, with Gumbel critical value and when empirical sizes are held at around
2.50. The ABD test corresponds to the top-left block of the table, while the LM test is in the middle-middle block. The
highest powers when empirical sizes are held at around 2.50 are marked with “*”.
Given the calibrated values of ξ, the tests are correctly sized and so direct comparison of test
powers can be made. In the last column of Table 6.7, tests with the highest power are
marked with an asterisk (*). The backward-looking test scores the highest powers (77.83%
and 73.35%) at the 2- and 5-min frequencies. At the 10-min frequency, the interpolated test
has the highest power (66.61%). The tests attain higher power at the higher frequencies,
consistent with the simulation results in the study [58]. The interpolated test achieves
powers within 1% of those of backward-looking test at the higher frequencies. Considering
that the interpolated test is implemented on a real-time basis, whereas the backward-
looking test can be done only at market's close, it is thus of value in adopting this
interpolated procedure.26
The simulation results, when E[Nt] is equal to 0.40, exhibit qualitatively similar outcomes.
To save space, we do not tabulate but list a few results. First, the tests tend to have larger
sizes and lower powers at the lower frequencies. Second, tests executed with Šidàk
correction and Gumbel critical values are oversized. Third, the calibrated values of ξ at
which the tests have correct sizes are strictly positive. Fourth, when test sizes are held at
nominal levels, the backward-looking test has the highest power at the three frequencies.
Last, the tests are less effective in spotting the frequent and small jumps; when E[Nt] is
equal to 0.40, the test powers are much lower than those in Table 6.7.
\n\n=== PAGE 194 ===\n(6.33)
Table 6.8 Simulation (SV2FJ_2ρ Model) and empirical results of BNS test, SPY.
E[Nt] = 0.08
E[Nt] = 0.40
SPY data
Size
Power
Size
Power Count Intensity
2 min
3.0800 67.43% 2.2350 44.86%
211
19.56%
5 min
3.6100 64.95% 2.6180 39.96%
116
10.75%
10 min 3.9740 58.52% 2.9340 33.05%
60
5.56%
Table 6.9 reports the simulation (under SV2FJ_2ρ model) and empirical results of BNS test for SPY data. The BNS test is
implemented with the Huang–Tauchen statistic in (6.8) at 1% significance level.
For completeness, in Table 6.8 we report the simulation results of the BNS jump test,
implemented with the Huang–Tauchen statistic in (6.7). Regardless of E[Nt], the daily BNS
test has much less spurious detection than is observed in the intraday tests. The power of
BNS test declines from around 64% when E[Nt] is 0.08 to around 39% when E[Nt] is 0.40;
in either cases, the BNS test does not show a higher power of the intraday tests, a result that
is documented in ABD (2007). Our simulation results on BNS test in Table 6.8 are also in
agreement with those in the study [16].
6.5 Empirical results
To apply our jump tests to SPY data, we reserve the first 4 years of data from 2002 to 2005
as an in-sample period for estimating the HAR model. The test period is thus from January
2, 2006, to April 30, 2010, totaling 1079 days. In light of the simulation results, we
implement only the backward-looking and interpolated tests. The calibrated values of ξ
from simulation will be used for detecting jumps in SPY data. Since there are two jumps
dynamics in the simulation of SV2FJ_2ρ model, we have two sets of ξ values and both will
be used in the empirical analysis.
Boudt et al. [28] emphasize that IVP must be accounted for when implementing jump
tests.27 To be consistent with the information sets of the tests, the IVP for the first day of
test period is given as [72]:
where t = 1, …, TIS is the in-sample period during 2002 and 2005. When performing the
interpolated test on the first day of test period, this estimated η1, j is used to remove IVP.
Figure 6.4 plots the estimated IVP for 5-min SPY returns over the test period. The first
pattern is estimated from the in-sample period during 2002 and 2005 using (6.33). The
estimation window is then moved forward day by day, up to April 30, 2010.
\n\n=== OCR PAGE 194 ===\nTable 6.8 Simulation (SV2FJ_2p Model) and empirical results of BNS test, SPY.
E[N,] = 0.08 E[N;] = 0.40 SPY data
Size Power Size Power Count Intensity

2min 3.0800 67.43% 2.2350 44.86% 211 19.56%

5min 3.6100 64.95% 2.6180 39.96% 116 10.75%

10 min 3.9740 58.52% 2.9340 33.05% 60 5.56%
Table 6.9 reports the simulation (under SV2FJ_2p model) and empirical results of BNS test for SPY data. The BNS test is
implemented with the Huang-Tauchen statistic in (6.8) at 1% significance level.
For completeness, in Table 6.8 we report the simulation results of the BNS jump test,
implemented with the Huang-Tauchen statistic in (6.7). Regardless of E[N,], the daily BNS
test has much less spurious detection than is observed in the intraday tests. The power of
BNS test declines from around 64% when E[N;] is 0.08 to around 39% when E[N;] is 0.40;
in either cases, the BNS test does not show a higher power of the intraday tests, a result that
is documented in ABD (2007). Our simulation results on BNS test in Table 6.8 are also in
agreement with those in the study [16].

6.5 Empirical results

To apply our jump tests to SPY data, we reserve the first 4 years of data from 2002 to 2005
as an in-sample period for estimating the HAR model. The test period is thus from January
2, 2006, to April 30, 2010, totaling 1079 days. In light of the simulation results, we
implement only the backward-looking and interpolated tests. The calibrated values of €
from simulation will be used for detecting jumps in SPY data. Since there are two jumps
dynamics in the simulation of SV2FJ_2p model, we have two sets of € values and both will
be used in the empirical analysis.

Boudt et al. [28] emphasize that IVP must be accounted for when implementing jump
tests.?7 To be consistent with the information sets of the tests, the IVP for the first day of
test period is given as [72]:

Ts M Ts (6.33),
mj = yey Y= 1,...,M,
=I j=l tI
where t = 1, ..., Tjg is the in-sample period during 2002 and 2005. When performing the
interpolated test on the first day of test period, this estimated n, ; is used to remove IVP.
Figure 6.4 plots the estimated IVP for 5-min SPY returns over the test period. The first

pattern is estimated from the in-sample period during 2002 and 2005 using (6.33). The
estimation window is then moved forward day by day, up to April 30, 2010.
\n\n=== PAGE 195 ===\nFIGURE 6.4 Estimated IVP over test period, SPY.
For the backward-looking test, since it is performed at the market's close, the in-sample
period is shifted 1 day forward and the IVP is updated accordingly. The procedure is then
repeated for each day in the test period. Figure 6.4 plots the estimated IVP for 5-min SPY
returns; a typical U-shape pattern evolves over the test period.
6.5.1 RESULTS ON THE BACKWARD-LOOKING TEST
In Table 6.9, we report the number of jumps given by the backward-looking test at 1% level.
The results are very consistent with the top panels of Table 6.7 from simulation. The Šidàk
correction and Gumbel critical values lead to much more jump detections than the
calibrated ξ values. The two sets of calibrated ξ values identify similar numbers of jumps in
SPY prices. For our test period of 1079 days, a 1% level has a nominal size 10.79. In
Table 6.9, across three frequencies there are significantly more jumps identified than 10.79
when the calibrated ξ values are used in the test. The daily intensity λPoi ranges from 6.12%
at the 10-min frequency to 28.73% at the 2-min frequency.28 In Figure 6.5, we plot the
identified jumps in SPY prices whenξ is calibrated with E[Nt] = 0.40. The jumps displayed
are high-frequency returns with the IVP removed.29 It can be seen that there are more
jumps detected at the higher frequencies, and jump occurrences tend to cluster after 15:00
EST at the higher frequencies.30
\n\n=== OCR PAGE 195 ===\n09:30 2006

FIGURE 6.4 Estimated IVP over test period, SPY.

For the backward-looking test, since it is performed at the market's close, the in-sample
period is shifted 1 day forward and the IVP is updated accordingly. The procedure is then
repeated for each day in the test period. Figure 6.4 plots the estimated IVP for 5-min SPY
returns; a typical U-shape pattern evolves over the test period.

6.5.1 RESULTS ON THE BACKWARD-LOOKING TEST

In Table 6.9, we report the number of jumps given by the backward-looking test at 1% level.
The results are very consistent with the top panels of Table 6.7 from simulation. The Sidak
correction and Gumbel critical values lead to much more jump detections than the
calibrated & values. The two sets of calibrated € values identify similar numbers of jumps in
SPY prices. For our test period of 1079 days, a 1% level has a nominal size 10.79. In

Table 6.9, across three frequencies there are significantly more jumps identified than 10.79
when the calibrated & values are used in the test. The daily intensity Apo; ranges from 6.12%
at the 10-min frequency to 28.73% at the 2-min frequency.8 In Figure 6.5, we plot the
identified jumps in SPY prices when is calibrated with E[N;,] = 0.40. The jumps displayed
are high-frequency returns with the IVP removed.?? It can be seen that there are more
jumps detected at the higher frequencies, and jump occurrences tend to cluster after 15:00
EST at the higher frequencies.°°
\n\n=== PAGE 196 ===\nTable 6.9 Jump detection by Backward-Looking Test.
Šidàk correction
Gumbel c.v.
ξ
Count
λPoi
ξ
Count
λPoi
2-min
–
449
41.61%
0
462
42.82%
5-min
–
197
18.26%
0
196
18.16%
10-min
–
132
12.23%
0
125
11.58%
ξ with E[Nt] = 0.08
ξ with E[Nt] = 0.40
Calib. ξ Count
λPoi
Calib. ξ Count
λPoi
2-min
0.0890
303
28.08% 0.0840
310
28.73%
5-min
0.0870
105
 9.73% 0.0780
114
10.57%
10-min 0.0840
 66
 6.12% 0.0750
 71
 6.58%
Table 6.9 reports the jump test results on SPY data at 1% level when backward-looking BP is used to estimate spot
volatility. The test period is from January 2, 2006, to April 30, 2010, a total of 1079 days.
FIGURE 6.5 Detected jumps during the test period and intraday distributions, SPY.
Table 6.10 contains the summary statistics of detected jumps. The empirical distributions of
jump returns are not Gaussian and the sample means of detected jumps are all negative. In
the right panel, the interarrival times between jumps display overdispersion at the 2- and 5-
min frequencies. A Monte Carlo Kolmogorov–Smirnov (KS) test for the interarrival times
\n\n=== OCR PAGE 196 ===\nTable 6.9 Jump detection by Backward-Looking Test.

Sidak correction Gumbel c.v.
& Count Apo; & Count Apoi
2-min - 449 41.61% (e) 462 42.82%
5-min - 197 18.26% te) 196 18.16%
1o-min  — 132 12.23% (e) 125 = 11.58%

§ with ELN;] = 0.08 § with E[N;] = 0.40

Calib.§& Count Apo Calib.§ Count —Apoj
2-min 0.0890 303 28.08% 0.0840 310 28.73%
5-min 0.0870 105 9.73% 0.0780 114 10.57%
10-min 0.0840 66 6.12% 0.0750 71 6.58%

Table 6.9 reports the jump test results on SPY data at 1% level when backward-looking BP is used to estimate spot
volatility. The test period is from January 2, 2006, to April 30, 2010, a total of 1079 days.

0.03 0.03 .
2m
0.02 0.02

0.01 0.01
° °
0.01 -0.01
0.02 0.02
a ee eo
2006 2007 2008 2009 2010 2006 2007 2008 2009 2010

°

o

FS

iy

10:00 11:00 12:00 13:00 14:00 15:00 16:00 10:00 11:00 12:00 13:00 14:00 15:00 16:00
FIGURE 6.5 Detected jumps during the test period and intraday distributions, SPY.

Table 6.10 contains the summary statistics of detected jumps. The empirical distributions of
jump returns are not Gaussian and the sample means of detected jumps are all negative. In
the right panel, the interarrival times between jumps display overdispersion at the 2- and 5-
min frequencies. A Monte Carlo Kolmogorov—Smirnov (KS) test for the interarrival times

\n\n=== PAGE 197 ===\nrejects the null of exponential distribution at 1% level at the higher frequencies. Thus, the
detected jumps in SPY do not appear to follow Poisson arrivals but display some degree of
clustering.
Table 6.10 Summary statistics and interarrival times of detected jumps, SPY.
Detected jumps
Interarrival times
Frequency 2 min
5 min
10 min 2 min
5 min
10 min
Mean
−2.2E-5 −2.7E-4 −5.0E-4 3.4577
9.4339
15.3256
SD
0.0037
0.0058
0.0079
5.1012
12.8664 14.0120
Dispersion
–
–
–
2.1765
1.8601
0.8359
Skewness
0.1935
−0.3068 −0.9144 4.3027
3.1233
1.7925
Kurtosis
3.7775
4.3036
4.6436
31.7083 16.4871
6.0830
Maximum
0.0134
0.0147
0.0146
51.2769
89.5897 65.0000
Minimum
−0.0131 −0.0220 -0.0277
0.0000
0.0000
0.0256
JB/KS Test 9.7425
9.8601
17.8852 0.1129
0.1515
0.1055
(0.0147) (0.0163) (0.0047) (5.0E-4) (0.0056) (0.3180)
Q (6.20)
–
–
–
57.1400 26.6639 17.7471
–
–
–
(<0.001) (0.1450) (0.6041)
Table 6.10 reports the summary statistics of detected jumps and their interarrival times for SPY data. The jumps are
obtained at 1% level when ξ is calibrated with E[Nt] = 0.40 in SV2FJ_2ρ model. The reported statistics are computed with
IVP removed. The interarrival times are expressed in daily unit. The dispersion is defined as var/(mean)2 for the
interarrival times. The JB test refers to the test for normality in the left panel, while the KS test is for the null of
exponential distribution in the right panel.
Our empirical results are comparable with many previous ones. In [45], the estimated daily
jump intensity 5.6% for 11-min SPY returns at 1% level is very close to our estimates 6.12%
and 6.58% for 10-min returns in Table 6.9. ABD (2007) obtains an empirical jump intensity
of 9.26% at 0.001% level for 2-min S&P 500 futures returns, which is considered by them a
very low value. Thus, the reported intensity 28% for 2-min SPY returns in Table 6.9 may be
within a reasonable interval.
6.5.2 RESULTS ON THE INTERPOLATED TEST
To implement the interpolated test, we first estimate the HAR model in (6.18) and (6.19) for
log(BP) from the in-sample period. We obtain the 1-day-ahead forecast E[BPt + 1|It] by
rolling the estimation window forward. The test results are again consistent with our
simulation. The Šidàk correction and the Gumbel critical values give much more jump
detections than those in Table 6.9. When the test is performed using calibrated ξ values,
similar or marginally more jumps are obtained than the backward-looking test, with
estimated intensity at most 3.3% higher than those of the backward-looking test. The
overall results suggest that the real-time interpolated test has slightly inferior size and
power properties to the backward-looking test.
Figure 6.5 plots the detected jumps in SPY returns, obtained when ξ is calibrated with E[Nt]
= 0.40 in SV2FJ_2ρ model. The marks are high-frequency SPY returns with IVP removed.
In the bottom panel, the plotted are frequency distributions of jump detections (counts)
within a trading day.
\n\n=== PAGE 198 ===\n6.6 Conclusion
By generalizing two intraday tests commonly used in the literature, in this study we
implement a data-driven procedure in which the jump tests are exempt from size distortion
and as a result direct comparison of test powers can be made. This improvement on the
jump tests is achieved by exploiting the GEV distribution of the maximums of the test
statistics, in which the shape parameter ξ is shown to be efficient in determining an optimal
critical region of the tests. From the simulation results, it is seen that the stronger the effect
of stochastic volatility on jump test statistics, the larger the shape parameter ξ are required
to hold the test sizes at nominal level. Therefore, the shape parameter ξ of GEV distribution
effectively measures the degree of deviation from i.i.d. Gaussian for the jump test statistics.
This observation is one of our main contributions to the existing literature.
Our simulation study suggests that a backward-looking test as in ABD (2007) obtains the
highest power when the test sizes are controlled at nominal level. On the other hand, the
interpolated test proposed in this study, which is a real-time testing procedure, can have
powers close to the backward-looking test. The values of ξ calibrated from our SV2FJ_2ρ
model, which is specified and estimated on high-frequency SPY data, are then used in the
empirical analysis. The jump test results on SPY data are consistent with our simulation
study in many aspects and thus are considered plausible. Our estimates of (detected) jumps
intensity are comparable with those reported in other studies. We also document that
detected jumps in high-frequency SPY returns display some degree of clustering in arrivals
and do not appear to have a Gaussian size distribution. These two critical observations may
have to be incorporated in further studies on relevant topics in high-frequency finance.
Acknowledgments
We thank the editor and two anonymous reviewers for comments on this chapter. The
chapter is part of my PhD thesis and I thank my supervisor Stephen Taylor for his guidance
and encouragement. I also benefit greatly from discussions with Jeff Fleming, Corsi Fulvio,
Dudley Gilder, Ilze Kalnina, Masahito Kobayashi, Suzanne Lee, Roel Oomen, Lei Sun, Chi-
Feng Tzeng, Giovanni Urga, Almut Veraart, and Martin Widdicks. Any remaining errors are
our responsibility.
\n\n=== PAGE 199 ===\nAppendix 6.A: Least-square estimation of HAR-MA (2)
model for log(BP) of SPY
In the top and bottom-left panel, we report least-square estimation of HAR model in (6.18)
for log(BP) of SPY. Most estimates of {bi}, i ≤ 4, are significantly positive; all estimates of
{bi}, i ≥ 5, are significantly negative. Hence, the signs of bi are consistent with the
autoregressive aspect and leverage effect of volatility. The adjusted R2 ranges from 86% to
80% and on average increases by 2% when lagged returns are included in HAR. The Q
(6.20) test rejects zero correlation in residuals ut with very small p values. Examination of
ACFs of HAR residuals ut finds that across three frequencies, the ACFs at lag 2 exceed the
95% upper bound of an uncorrelated process. An MA (2) model in (6.19) is thus designed
for the HAR residuals ut and the estimation result is reported in the bottom-right panel.
Across three frequencies, the estimates of ϕ1 are insignificant while ϕ2 is estimated between
0.05 and 0.07 with standard errors around 0.02. Estimates of σe are consistently smaller
than those of σu. At the 10-min frequency, the Q (6.20) statistic cannot reject zero
correlation in the HAR-MA (2) residuals at 5% level.
\n\n=== OCR PAGE 199 ===\nAppendix 6.A: Least-square estimation of HAR-MA (2)
model for log(BP) of SPY

HAR c by by bs by bs be b, bs by bio

2 min —0.4817 0.3588 = =—0.3969 = 0.0622. 0.1323. —4.7941 -10.6475 -—22.6403 -8.1348 -17.9600 -20.8579

(0.1062) (0.0257) (0.0373) (0.0365) (0.0290) (0.8255) (2.3341) (5.8768) (1.3664) (4.0129) (9.4882)
5 min —0.5378 0.3093 0.4097 0.0834 0.1425. —4.6548  —13.3684 —25.1228 -6.9036 -21.2623 21.1967
(0.1208) (0.0260) (0.0391) (0.0397) (0.0319) (0.9298) (2.6251) (6.6471) (1.5382) (4.5047) (10.5808)
10min = -0.5777 0.2628 0.3925 0.1396) 0.1461 —5.6899  -15.2989 -29.1153 -5.8881 -25.1451 —23.4155
(0.1304) (0.0258) (0.0408) (0.0437) (0.0352) (1.0191) (2.8699) (7.2799) (1.6826) (4.9190) (11.5877)
2min  Smin 10min MAQ) a o log. Q(6.20)
oy 0.3946 0.4444 0.4862 2 min 0.0696 0.3924 903.80 37.336
Adj.R? 0.8575 (0.8254 0.8004. (0.0220) (0.0062) - (0.0030)
Q(6.20) 51.621 50.098 34.782 S min 0.0614 0.4423 655.54 43.443

pvalue (5.3E-8) (1.0E-7) (6.SE-5) (0.0: (0.0070) (4.1B-4)
10 min 0.0: 0.4841 468.27 26.969
(0.0217) (0.0074) - (0.0585)

In the top and bottom-left panel, we report least-square estimation of HAR model in (6.18)
for log(BP) of SPY. Most estimates of {b;}, i < 4, are significantly positive; all estimates of

{b;}, 12 5, are significantly negative. Hence, the signs of b; are consistent with the
autoregressive aspect and leverage effect of volatility. The adjusted R* ranges from 86% to

80% and on average increases by 2% when lagged returns are included in HAR. The Q
(6.20) test rejects zero correlation in residuals u, with very small p values. Examination of

ACFs of HAR residuals u, finds that across three frequencies, the ACFs at lag 2 exceed the

95% upper bound of an uncorrelated process. An MA (2) model in (6.19) is thus designed
for the HAR residuals u, and the estimation result is reported in the bottom-right panel.

Across three frequencies, the estimates of ¢), are insignificant while ¢, is estimated between
0.05 and 0.07 with standard errors around 0.02. Estimates of o, are consistently smaller
than those of o,,. At the 10-min frequency, the Q (6.20) statistic cannot reject zero
correlation in the HAR-MA (2) residuals at 5% level.
\n\n=== PAGE 200 ===\nAppendix 6.B: Estimation of ARMA (2, 1) model for log(BP)
of SPY
In the left panel, we report the time-domain estimation of ARMA (2, 1) model in (6.27) for
log(BP) of SPY. The two roots α1 and α2 of x2ϕ(x− 1) = 0, where ϕ(L) = 1 − ϕ1L − ϕ2L2 is the
AR (2) polynomial, are calculated at around (0.98, 0.20). In the right panel, frequency-
domain estimation is done by maximizing the Whittle likelihood function:
where λ = (1, 2, …, T − 1)(2π/T) is the frequency ordinate, T = 2076, I(λ) is the sample
spectrum, and f(λ) is the theoretical spectral density:
The weights of the two processes are given by wi = vari/Σvari, i = 1, 2, where vari = σ2
ϵ, i/(1 −
α2
i) is the variance of individual process. The estimated α1 and α2 are very close to the
implied values in the left panel. The persistent AR (1) process has weight ranging from 89%
at the 2-min frequency to 84% at the 10-min frequency.
Appendix 6.C: Minimized loss function loss(ρ1, ρ2) for
SV2FJ_2ρ model, SPY
\n\n=== OCR PAGE 200 ===\nAppendix 6.B: Estimation of ARMA (2, 1) model for log(BP)
of SPY

Time-domain estimation Frequency-domain estimation

2 min 5 min 10 min 2 min Smin 10 min

u —9.6302 -9.7447 -9.7840 a, 0.9870 0.9860 0.9875
(0.1812) (0.1895) (0.2040) (0.0044) (0.0047) (0.0042)

% 1.2251 (1.1824 1.1849 oy 0.1642 0.1703 0.1625
(0.0446) (0.0412) (0.0387) (0.0125) (0.0133) (0.0133)

o, 0.2359 -0.1946 -0.1959 ay 0.2413 0.2007 0.2012
(0.0429) (0.0395) (0.0371) (0.0448) (0.0411) (0.0371)

wv 0.7005 -0.7061 -0.7449 o,2 0.3445 0.3900 0.4419
(0.0352) (0.0319) (0.0288) (0.0103) (0.0108) (0.0109)

04193 04685 05146 - - =
(0.0072) (0.0073) (0.0079) - - - -
log L. 766.48 536.26 341.28 logL. 768.71 538.25 343.53

Imp.a, 0.9858 0.9848 0.9863 w, 89.23% 86.81% 83.93%
Imp.a, 0.2393 0.1976 0.1986 w2 10.77% 13.19% 16.07%

In the left panel, we report the time-domain estimation of ARMA (2, 1) model in (6.27) for
log(BP) of SPY. The two roots a, and a, of x°(x" ') = 0, where (L) = 1- ,L - ,L? is the
AR (2) polynomial, are calculated at around (0.98, 0.20). In the right panel, frequency-
domain estimation is done by maximizing the Whittle likelihood function:

0.5), flog 2x -f(a) +1) /F A],

where A = (1, 2, ..., J - 1)(2n/T) is the frequency ordinate, T = 2076, I(A) is the sample
spectrum, and f(A) is the theoretical spectral density:

1 ia
fa=( |! -ae

o,

62

+() rae
2x “

~2
“| °

The weights of the two processes are given by w; = var;/Zvar;, i = 1, 2, where var; = 07. ;/(1 -

a?;) is the variance of individual process. The estimated a, and a, are very close to the

implied values in the left panel. The persistent AR (1) process has weight ranging from 89%
at the 2-min frequency to 84% at the 10-min frequency.

Appendix 6.C: Minimized loss function loss(p4, p2) for
SV2FJ_2p model, SPY

Frequency a 2 min) i 3 min) pr: (10 min)

A -0.450 -0.475 -0.500 -0.475 0.500 -0.525 -0.525 -0.550 -0.575

-0.100 0.0019 0.0015 0.0015 0.0022 0.0018 0.0019 0.0026 0.0024 0.0023
-0.125 0.0017 0.0015 0.0014 0.0022 0.0018 0.0019 0.0025 0.0022 0.0025
-0.150 0,016 0.0013 0.0014 0.0021 0.0017 0.0019 0.0023 0.0021 0.0022
-0.175  0,0018 0.0014 0.0016 0.0021 0.0019 0.0018 0.0023 0.0022 0.0022
—0.200 _ 0,0017 0.0015 0.0017 0.0020 0.0019 0.0019 0.0024 0.0023 0.0024

\n\n=== PAGE 201 ===\nIn this table, we report the minimized loss function loss(ρ1, ρ2) for SV2FJ_2ρ model in
(6.32). For each frequency, local minimums of loss function can be found from a grid of
candidate values on (ρ1, ρ2). The minimized loss functions become smaller at the higher
frequencies. We also calculate the minimized loss function when there is only one
correlation parameter in the SV2FJ_2ρ model, which is the one attached to the persistent
volatility component; in that case the minimized loss functions are consistently larger than
the reported values here across all three frequencies.
Appendix 6.D.1: Calibration of ξ under SV2FJ_2ρ model at 2-
min frequency, E[Nt] = 0.08
In this table, we report the calibrated values of ξ at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2ρ model with parameters given in
Table 6.6 and with E[Nt] = 0.08. The results are for 2-min sampling frequency. Results for
tests implemented with Šidàk and Bonferroni corrections are also given. The values of ξ are
obtained from an array of candidate values with interpolation; these values along with the
corresponding test size and power are highlighted in bold.
\n\n=== OCR PAGE 201 ===\nIn this table, we report the minimized loss function loss(p,, p,) for SV2FJ_2p model in

(6.32). For each frequency, local minimums of loss function can be found from a grid of
candidate values on (p,, p,). The minimized loss functions become smaller at the higher
frequencies. We also calculate the minimized loss function when there is only one
correlation parameter in the SV2FJ_2p model, which is the one attached to the persistent
volatility component; in that case the minimized loss functions are consistently larger than
the reported values here across all three frequencies.

Appendix 6.D.1: Calibration of € under SV2FJ_2p model at 2-
min frequency, E[N;] = 0.08

Backward-looking BP Forward-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size Power

Sidak 6.9920 79.32%  Sidak 15.8080 79.67%  Sidak 11.7940 79.54%
Bonferroni 6.9710 79.32% Bonferroni 15.7610 79.67% Bonferroni 11.7500 79.54%

G Size Power Gi Size Power é Size Power

0.0000 7.7340 79.48% 0.0000 17.2040 79.81% 0.0000 12.8440 79.66%
0.0100 6.9420 79.32% 0.0180 14,5680 79.55% 0.0200 10.5260 79.34%
0.0200 6.1800 79.16% 0.0360 12.1400 79.30% 0.0400 8.5610 79.03%

0.0300 5.5030 78.98% 0.0540 10.0340 78.96% 0.0600 6.8570 78.72%
0.0400 4.8840 78.82% 0.0720 8.2150 78.66% 0.0800 5.4270 78.26%
0.0500 4.2920 78.68% 0.0900 6.6660 78.36% 0.1000 4.2270 77.80%

0.0600 3.7940 78.41% 0.1080 5.3460 77.96% 0.1200 3.2500 7.31%
0.0700 3.2910 78.26% 0.1260 4.1990 77.53% 0.1380 2.4750 76.96%
0.08002. 78.07% 0.1440 3.2040 7.10% 0.1400 2.4090 76.91%

0.0890 2.4950 77.83% 0.1602 2.5070 76.70% 0.1600 1.7410 76.38%
0.0900. 2.4580 77.82% 0.1620 2.4470 76.66% 0.1800 1.2580 75.83%
0.1000 2.1000 77.62% 0.1800 1.8630 76.22% 0.2000 0.8770 75.40%

In this table, we report the calibrated values of § at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2p model with parameters given in

Table 6.6 and with E[N;] = 0.08. The results are for 2-min sampling frequency. Results for
tests implemented with Sidak and Bonferroni corrections are also given. The values of € are
obtained from an array of candidate values with interpolation; these values along with the
corresponding test size and power are highlighted in bold.

\n\n=== PAGE 202 ===\nAppendix 6.D.2: Calibration of ξ under SV2FJ_2ρ model at 2-
min frequency, E[Nt] = 0.40
In this table, we report for the 2-min frequency the calibrated values of ξ at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2ρ model has parameters
given in Table 6.6 with E[Nt] = 0.40.
Appendix 6.D.3: Calibration of ξ under SV2FJ_2ρ model at 5-
min frequency, E[Nt] = 0.08
In this table, we report the calibrated values of ξ at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2ρ model with parameters given in
Table 6.6 and E[Nt] = 0.08. The results are for 5-min sampling frequency.
\n\n=== OCR PAGE 202 ===\nAppendix 6.D.2: Calibration of € under SV2FJ_2p model at 2-
min frequency, E[N;] = 0.40

Backward-looking BP Forward-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size Power

Sidak 64540 58.47%  Sidak 14.4460 59.09% — Sidak 10.6350 58.64%
Bonferroni 6.4310 58.46% Bonferroni 14.3980 59.08% Bonferroni 10.5900 58.64%

Size Power € Size Power é Size Power

0.0000 7.1220 58.75% 0.0000 15.7120 59.35% 0.0000 11.6290 58.88%
0.0100 6.3880 58.44% 0.0160 13.5450 58.91% 0.0200 9.5390 58.35%
0.0200 5.6940 58.16% 0.0320 11.5620 58.44% 0.0400 7.7630 57.76%
0.0300 5.0280 57.83% 0.0480 9.8270 57.96% 0.0600 6.2270 S7.11%
0.0400 © 4.4570 57.5: 0.0640 8.2480 57.47% 0.0800 4.9480 56.42%
0.0500 3.9410 57.19% 0.0800 6.8620 56.94% 0.1000 3.8450 55.70%
0.0600 3.4510 56.86% 0.0960 5.6590 56.39% 0.1200 2.9580 54.96%
0.0700 3.0100 56.51% 0.1120 4.6090 55.81% 0.1300 2.5190 0%
0.0800 2.6140 56.18% 0.1280 3.7380 55.21% 0.1400 2.1880 54.21%
0.0840 2.4920 56.03% 0.1440 2.9800 54.60% 0.1600 1.5880 53.43%
0.0900 -2.2850 55.81% 0.1552 25180 S4.13% 0.1800 1.1470 52.56%
0.1000 1.9760 55.47% 0.1600 2.3440 53.94% 0.2000 0.8170 51.59%

In this table, we report for the 2-min frequency the calibrated values of € at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2p model has parameters
given in Table 6.6 with ELN;] = 0.40.

Appendix 6.D.3: Calibration of € under SV2FJ_2p model at 5-
min frequency, E[N;] = 0.08

Backward-looking BP Forward-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size Power

Sidak 7.1980 75.61%  Sidak ‘18.8430 76.97%  Sidak 12.4430 76.41%
Bonferroni 7.1780 75.60% Bonferroni 18.7980 76.97% Bonferroni 12.4070 76.41%

Size Power € Size Power € Size Power

0.0000 7.1300 75.59% 0.0000 18.6920 76.96% 0.0000 12.3230 76.38%
0.0100 6.4690 75.37% 0.0180 76.54% 0.0200 10.1750 75.94%
0.0200 5.7790 75.16% 0.0360 76.09% 0.0400 8.3690 75.51%
0.0300 5.1650 74.91% 0.0540 11.4320 75.63% 0.0600 6.8020 74.98%
0.0400 4.5700 74.69% 0.0720 9.4490 75.10% 0.0800 5.4100 74.49%
0.0500 4.0710 74.45% 0.0900 7.7750 74.63% 0.1000 4.2770 73.85%
0.0600 3.5780 74.21% 0.1080 6.3760 74.10% 0.1200 3.3330 73.30%
0.0700 3.1460 73.94% 0.1260 5.1240 73.61% 0.1400 2.5450 72.79%
0.0800 2.7530 73.57% 0.1440 4.0540 73.04% 0.1410 2.4850
0.0870 2.4930 73.35% 0.1620 3.1680 72.45% 0.1600 1.8870
0.0900 2.4000 73.30% 0.1782 2.4920 71 0.1800 1.3750
0.1000 2.1000 72.98% 0.1800 2.4300 71.90% 0.2000 0.9610

In this table, we report the calibrated values of § at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2p model with parameters given in
Table 6.6 and E[N;] = 0.08. The results are for 5-min sampling frequency.
\n\n=== PAGE 203 ===\nAppendix 6.D.4: Calibration of ξ under SV2FJ_2ρ Model at 5-
min frequency, E[Nt] = 0.40
In this table, we report for the 5-min frequency the calibrated values of ξ at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2ρ model has parameters
given in Table 6.6 with E[Nt] = 0.40.
Appendix 6.D.5: Calibration of ξ under SV2FJ_2ρ model at
10-min frequency, E[Nt] = 0.08
In this table, we report the calibrated values of ξ at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2ρ model with parameters given in
Table 6.6 and E[Nt] = 0.08. The results are for 10-min sampling frequency.
\n\n=== OCR PAGE 203 ===\nAppendix 6.D.4: Calibration of € under SV2FJ_2p Model at 5-
min frequency, E[N;] = 0.40

Backward-looking BP Forward-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size Power

Sidak 6.3760 51.28%  Sidak 16,9420 53.05%  Sidak 11.1970 52.21%
Bonferroni 6.3510 51.26% Bonferroni 16.8860 53.04% Bonferroni 11.1650 52.21%

é Size Power é Size Power € Size Power

0.0000 6.3190 51.24% 0.0000 16.7940 53.02% 0.0000 11.1010 52.19%
0.0100 5.7180 50.89% 0.0180 14.4260 52.42% 0.0200 9.1470 51.48%
0.0200 5.1250 50.52% 0.0360 12.2750 51.73% 0.0400 7.5060 50.74%
0.0300 4.5770 50.14% 0.0540 10.2480 51.05% 0.0600 6.0250 49.98%
0.0400 4.1020 49.72% 0.0720 8.4780 50.32% 0.0800 4.8170 49.17%
0.0500 3.6430 49.29% 0.0900 6.9760 49.62% 0.1000 3.7560 48.28%
0.0600 3.1890 48.84% 0.1080 5.6290 48.82% 0.1200 2.8780 47.34%
0.0700. 2.8120 48.39% 0.1260 4.4910 47.99% 0.1300 2.5210 46.83%
0.0780 2.4900 48.01% 0.1440 3.5200 47.11% 0.1400 2.1940 46.34%
0.0800 2.4290 47.92% 0.1620 2.7830 46.24% 0.1600 1.6390 45.35%
0.0900 2.1220 47.48% 0.1692 2.4930 45.90% 0.1800 1.2130 44.32%
0.1000 1.8480 47.03% 0.1800 2.0850 45.34% 0.2000 0.8690 43.21%

In this table, we report for the 5-min frequency the calibrated values of € at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2p model has parameters
given in Table 6.6 with E[N;] = 0.40.

Appendix 6.D.5: Calibration of € under SV2FJ_2p model at
10-min frequency, E[N,] = 0.08

Backward-looking BP Forwant-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size

Sidak 7.6700 70.07%  Sidak 24.1360 72.20% Sidak = 13.7340.
Bonferroni 7.6450 70.06% Bonferroni 24,0670 72.19% Bonferroni 13.6900

é Size Power é Size Power é Size

0.0000 6.7680 69.67% 0.0000 21.9780 71.92% 0.0000 12.4670 71.35%
0.0100 6.0960 69.34% 0.0200 18.7160 71.42% 0.0200 10.4630 70.77%
0.0200 5.4770 68.97% 0.0400 15.7640 70.90% 0.0400 8.5930 70.10%
0.0300 4.8830 68.58% 0.0600 13.0950 70.30% 0.0600 6.9890 69.60%
0.0400 4.3680 68.21% 0.0800 10.8260 69.70% 0.0800 5.6590 68.90%
0.0500 3.8570 67.84% 0.1000 8.7400 68.97% 0.1000 4.4870 68.14%
0.0600 3.4130 67.41% 0.1200 7.0170 68.24% 0.1200 3.5180 67.53%

0.0700 —-2.9930 67.03% 0.1400 5.5820 67.52% 2.7230 66.78%
0.0800 66.59% 0.1600 4.3930 66.78% 2.5050 66.61%
0.0840 66.37% 0.1800 3.3520 2.0560 65.99%
0.0900 66.12% 0.2000 0.1800 1.5560 65.12%
0.1000 65.68% 0.2020 0.2000 1.1480 64.05%

In this table, we report the calibrated values of § at which the jump tests have empirical size
close to 2.50 for data simulated from the SV2FJ_2p model with parameters given in
Table 6.6 and E[N;] = 0.08. The results are for 10-min sampling frequency.
\n\n=== PAGE 204 ===\nAppendix 6.D.6: Calibration of ξ under SV2FJ_2ρ model at
10-min frequency, E[Nt] = 0.40
In this table, we report for the 10-min frequency the calibrated values of ξ at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2ρ model has parameters
given in Table 6.6 with E[Nt] = 0.40.
References
1. Abdi, H. 2007, The Bonferroni and Šidák corrections for multiple comparisons, in Neil
Salkind (Ed.), Encyclopedia of Measurement and Statistics. SAGE Publications.
2. Aït-Sahalia, Y., and J. Jacod. 2009, Testing for jumps in a discretely observed process,
The Annals of Statistics 37, 184–222.
3. Aldous, D. 1989, Probability Approximations via the Poisson Clumping Heuristic.
Springer.
4. Alizadeh, S., M.W. Brandt, and F.X. Diebold. 2002, Range-based estimation of
stochastic volatility models, Journal of Finance 57, 1047–1091.
5. Andersen, T.G., T. Bollerslev, F.X. Diebold, and P. Labys. 2000, Great realization, Risk,
March 2000, 105–108.
6. Andersen, T.G., T. Bollerslev, F.X. Diebold, and H. Ebens. 2001, The distribution of
realized stock return volatility, Journal of Financial Economics 61, 43–76.
7. Andersen, T.G., T. Bollerslev, and D. Dobrev. 2007, No-arbitrage semi-martingale
restrictions for continuous-time volatility models subject to leverage effects, jumps and
i.i.d. noise: theory and testable distributional implications, Journal of Econometrics
138, 125–180.
8. Areal, N., and S.J. Taylor. 2002, The realized volatility of FTSE-100 future prices,
Journal of Futures Markets 22, 627–648.
9. Bajgrowicz, P., and O. Scaillet. 2011, Jumps in High-Frequency Data: Spurious
Detections, Dynamics and News, University of Geneva and Swiss Finance Institute
preprint.
10. Bakshi, G., C. Cao, and Z. Chen. 1997, Empirical performance of alternative option
pricing models, Journal of Finance 52, 2003– 2049.
\n\n=== OCR PAGE 204 ===\nAppendix 6.D.6: Calibration of € under SV2FJ_2p model at
10-min frequency, E[N;] = 0.40

Backward-looking BP Forward-looking BP Interpolated BP

Correction Size Power Correction Size Power Correction Size Power

Sidak 6.7250 42.39%  Sidak 21.1080 46.15%  Sidak 11.7420 44.70%
Bonferroni 6.6990 42.38% Bonferroni 21.0440 46.13% Bonferroni 11.7040 44.69%

Size Power é Size Power € Size Power

0.0000 5.9950 41.84%  0,0000 19.2810 45.66% 0.0000 10.6020 44.23%

0.0100 5.3680 41.37% 0.0200 16.3930 44.81% 0.0200 8.9360 43.35%
0.0200 4.8010 40.87% 0.0400 13.7940 43.90% 0.0400 7.3390 42.43%
0.0300 4.3600 40.40% 0.0600 11.4700 43.03% 0.0600 5.9430 41.54%
0.0400 3.9190 39.90% 0.0800 9.4700 42.07% — 0.0800 40.57%
0.0500 3.4690 39.45% 0.1000 7.6860 41.08% 0.1000 3.7250 39.54%
0.0600 3.0490 38.91% 0.1200 6.1390 40.14% 0.1200 2.9260 38.47%
0.0700 38.36% 0.1400 4.9060 39.03% 0.1320 2.4890 37.80%
0.0750 38.07% 0.1600 3.8290 37.95% 0.1400 2.2240 37.32%
0.0800 37.81% 0.1800 36.79% 0.1600 1.7030 36.16%
0.0900 37.33% 0.1920 36.08% 0.1800 1.2800 34.99%
0.1000 36.79% 0.2000 35.61% 0.2000 0.9350 33.81%

In this table, we report for the 10-min frequency the calibrated values of € at which the jump
tests have empirical sizes close to 2.50. The simulated SV2FJ_2p model has parameters
given in Table 6.6 with E[N;] = 0.40.

References

1. Abdi, H. 2007, The Bonferroni and Sidak corrections for multiple comparisons, in Neil
Salkind (Ed.), Encyclopedia of Measurement and Statistics. SAGE Publications.

2. Ait-Sahalia, Y., and J. Jacod. 2009, Testing for jumps in a discretely observed process,
The Annals of Statistics 37, 184-222.

3. Aldous, D. 1989, Probability Approximations via the Poisson Clumping Heuristic.
Springer.

4. Alizadeh, S., M.W. Brandt, and F.X. Diebold. 2002, Range-based estimation of
stochastic volatility models, Journal of Finance 57, 1047-1091.

5. Andersen, T.G., T. Bollerslev, F.X. Diebold, and P. Labys. 2000, Great realization, Risk,
March 2000, 105-108.

6. Andersen, T.G., T. Bollerslev, F.X. Diebold, and H. Ebens. 2001, The distribution of
realized stock return volatility, Journal of Financial Economics 61, 43-76.

7. Andersen, T.G., T. Bollerslev, and D. Dobrev. 2007, No-arbitrage semi-martingale
restrictions for continuous-time volatility models subject to leverage effects, jumps and
i.i.d. noise: theory and testable distributional implications, Journal of Econometrics
138, 125-180.

8. Areal, N., and S.J. Taylor. 2002, The realized volatility of FTSE-100 future prices,
Journal of Futures Markets 22, 627-648.

g. Bajgrowicz, P., and O. Scaillet. 2011, Jumps in High-Frequency Data: Spurious
Detections, Dynamics and News, University of Geneva and Swiss Finance Institute
preprint.

10. Bakshi, G., C. Cao, and Z. Chen. 1997, Empirical performance of alternative option
pricing models, Journal of Finance 52, 2003- 2049.

\n\n=== PAGE 205 ===\n11. Bandi, F.M., and J.R. Russell. 2006, Separating microstructure noise from volatility,
Journal of Financial Economics 79, 655–692.
12. Bandi, F.M., and R. Renò. 2012b, Price and Volatility Co-jumps, Johns Hopkins
University and Università di Siena preprint.
13. Barndorff-Nielsen, O.E., and N. Shephard. 2001, Non-Gaussian Ornstein–Uhlenbeck
based models and some of their uses in financial economics, Journal of the Royal
Statistical Society B63, 167–241.
14. Barndorff-Nielsen, O.E., and N. Shephard. 2002, Econometric analysis of realized
volatility and its use in estimating stochastic volatility models, Journal of the Royal
Statistical Society B64, 253–280.
15. Barndorff-Nielsen, O.E., and N. Shephard. 2004, Power and bi-power variation with
stochastic volatility and jumps, Journal of Financial Econometrics 2: 1–48.
16. Barndorff-Nielsen, O.E., and N. Shephard. 2006, Econometrics of testing for jumps in
financial economics using bi-power variation, Journal of Financial Econometrics 4, 1–
30.
17. Barndorff-Nielsen, O.E., and N. Shephard. 2006, Impact of jumps on returns and
realized variances: econometric analysis of time-deformed Lévy processes, Journal of
Econometrics 131, 217–252.
18. Barndorff-Nielsen, O.E., and N. Shephard. 2007, Variation, jumps, market frictions and
high frequency data in financial econometrics, in R. Blundell, P. Torsten, & W.K. Newey
(Eds.), Advances in Economics and Econometrics: Theory and Applications, Ninth
World Congress (Ch. 10, Vol. 3, pp. 328–372). Cambridge University Press.
19. Barndorff-Nielsen, O.E., S.E. Graversen, J. Jacod, and N. Shephard. 2006, Limit
theorems for bi-power variation in financial econometrics, Econometric Theory 22,
677–719.
20. Barndorff-Nielsen, O.E., P.R. Hansen, A. Lunde, and N. Shephard. 2008, Designing
realized kernels to measure the ex post variation of equity prices in the presence of
noise, Econometrica 76, 1481–1536.
21. Barndorff-Nielsen, O.E., P.R. Hansen, A. Lunde, and N. Shephard. 2009, Realized
kernels in practice: trades and quotes, Econometrics Journal 12, C1–C32.
22. Bates, D.S. 1996, Jumps and stochastic volatility: exchange rate processes implicit in
Deutsche mark options, Review of Financial Studies 9, 69–107.
23. Benjamini, Y., and Y. Hochberg. 1995, Controlling the false discovery rate: a practical
and powerful approach to multiple testing, Journal of the Royal Statistical Society B57,
289–300.
24. Bollerslev T., G. Tauchen, and T.H. Law. 2008, Risk, jumps and diversification, Journal
of Econometrics 144, 234–256.
25. Bollerslev T., U. Kretschmer, C. Pigorsch, and G. Tauchen. 2009, A discrete-time model
for daily S&P 500 returns and realized variations: jumps and leverage effects, Journal of
Econometrics 150, 151–166.
26. Bollerslev T., V. Todorov, and S. Z. Li. 2013, Jump tails, extreme dependencies, and the
distribution of stock returns, Journal of Econometrics 172, 307–324.
27. Bos, C.S., P. Janus, and S.J. Koopman. 2012, Spot variance path estimation and its
application to high frequency jump testing, Journal of Financial Econometrics 10, 354–
389.
28. Boudt, K., C. Croux, and S. Laurent. 2011, Robust estimation of intraweek periodicity in
volatility and jump detection, Journal of Empirical Finance 18, 353–367.
29. Boudt, K., J. Cornelissen, C. Croux, and S. Laurent. 2011, Non-parametric tests for
intraday jumps: impact of periodicity and microstructure noise, in L. Bauwens, C.
Hafner, and S. Laurent (Eds.), Volatility Models and Their Applications. Wiley-
Interscience.
\n\n=== PAGE 206 ===\n30. Breidt, F.J., and R.A. Davis. 1998, Extremes of stochastic volatility models, The Annals
of Applied Probability 8, 664–675.
31. Christensen, K., R.C.A. Oomen, and M. Podolskij. 2010, Realised quantile-based
estimation of the integrated variance, Journal of Econometrics 159, 74–98.
32. Coles, S. 2001, An Introduction to Statistical Modeling of Extreme Values. Springer.
33. Corsi, F. 2009, A simple approximate long-memory model of realized volatility, Journal
of Financial Econometrics 7, 174–196.
34. Corsi, F., D. Pirino, and R. Renò. 2010, Threshold bi-power variation and the impact of
jumps on volatility forecasting, Journal of Econometrics 159, 276–288.
35. Cramér, H. 1946, Mathematical Methods of Statistics. Princeton University Press.
36. Das, S.R., and R.K. Sundaram. 1999, Of smiles and smirks: a term structure perspective,
Journal of Financial and Quantitative Analysis 34, 211–240.
37. Delbaen, F., and W. Schachermayer. 1994, A general version of the fundamental
theorem of asset pricing, Mathematische Annalen 300, 463–520.
38. Duffie, D., J. Pan, and K. Singleton. 2000, Transform analysis and asset pricing for
affine jump-diffusions, Econometrica 68, 1343–1376.
39. Dumitru, A.-M., and G. Urga. 2012, Identifying jumps in financial assets: a comparison
between nonparametric jump tests, Journal of Business and Economic Statistics 30,
242–255.
40. Eraker, B., M. Johannes, and N. Polson. 2003, The impact of jumps in volatility and
returns, Journal of Finance 58, 1269–1300.
41. Evans, K.P. 2011, Intraday jumps and US macroeconomic news announcements,
Journal of Banking & Finance 35, 2511–2527.
42. Fleming, J., and B.S. Paye. 2011, High-frequency returns, jumps and the mixture of
normal hypothesis, Journal of Econometrics 160, 119–128.
43. Galambos, J. 1978, The Asymptotic Theory of Extreme Order Statistics. John Wiley &
Sons, Inc.
44. Gallant, A.R., C.-T. Hsu, and G. Tauchen. 1999, Using daily range data to calibrate
volatility diffusions and extract the forward integrated variance, The Review of
Economics and Statistics 81, 617– 631.
45. Gilder, D., M.B. Shackleton, and S.J. Taylor. 2013, Cojumps in Stock Prices: Empirical
Evidence, Aston Business School and Lancaster University Management School
preprint.
46. Hall, P. 1979, On the rate of convergence of normal extremes, Journal of Applied
Probability 16, 433–439.
47. Hansen, P.R., and A. Lunde. 2006, Realized variance and market micro-structure noise,
Journal of Business & Economic Statistics 24, 127–161.
48. Hansen, P.R., Z. Huang, and H.H. Shek. 2012, Realized GARCH: a joint model for
returns and realized measures of volatility, Journal of Applied Econometrics 27, 877–
906.
49. Harvey, A.C. 1993, Time Series Models. Pearson Education.
50. Hasbrouck, J. 2003, Intraday price formation in U.S. equity index markets, Journal of
Finance 58, 2375–2400.
51. Huang, X., and G. Tauchen. 2005, The relative contribution of jumps to total price
variance, Journal of Financial Econometrics 3, 456–499.
52. Jacod, J., Y. Li, P.A. Mykland, M. Podolskij, and M. Vetter. 2009, Microstructure noise
in the continuous case: the pre-averaging approach, Stochastic Processes and Their
Applications 119, 2249–2276.
53. Jacod, J., C. Klüppelberg, and G. Müller. 2012, Functional relationships between price
and volatility jumps and their consequences for discretely observed data, Journal of
Applied Probability 49, 901–914.
\n\n=== PAGE 207 ===\n54. Jacod, J., and V. Todorov. 2010, Do price and volatility jump together? The Annals of
Applied Probability 20, 1425–1469.
55. Kalnina, I. 2011, Subsampling high frequency data, Journal of Econometrics 161, 262–
283.
56. Kobayashi, M. 2009, Testing for jumps in the stochastic volatility models, Mathematics
and Computers in Simulation 79, 2597–2608.
57. Lahaye, J., S. Laurent, and C.J. Neely. 2011, Jumps, cojumps and macro
announcements, Journal of Applied Econometrics 26, 893–921.
58. Lee, S.S., and P.A. Mykland. 2008, Jumps in financial markets: a new nonparametric
test and jump dynamics, The Review of Financial Studies 21, 2535–2563.
59. Lee, S.S. 2011, Jumps and information flow in financial markets, Review of Financial
Studies 25, 439–479.
60. Liu, C., and J.M. Maheu. 2009, Forecasting realized volatility: a Bayesian model-
averaging approach, Journal of Applied Econometrics 24, 709–733.
61. Mykland, P.A., and L. Zhang. 2009, Inference for continuous semi-martingales observed
at high frequency, Econometrica 77, 1403–1445.
62. Mancini, C. 2004, Estimation of the characteristics of the jumps of a general Poisson-
diffusion model, Scandinavian Actuarial Journal 1, 42–52.
63. Mancini, C. 2009, Non-parametric threshold estimation for models with stochastic
diffusion coefficient and jumps, Scandinavian Journal of Statistics 36, 270–296.
64. Patton, A.J., and K. Sheppard. 2011, Good Volatility, Bad Volatility: Signed Jumps and
the Persistence of Volatility, Oxford-Man Institute of Quantitative Finance preprint.
65. Peters, R.T., and R.G. de Vilder. 2006, Testing the continuous semi-martingale
hypothesis for the S&P 500, Journal of Business and Economic Statistics 24, 444–454.
66. Platen, E., and N. Bruti-Liberati. 2010, Numerical Solution of Stochastic Differential
Equations with Jumps in Finance. Springer.
67. Podolskij, M., and D. Ziggel. 2010, New tests for jumps in semi-martingale models,
Statistical Inference for Stochastic Processes 13, 15–41.
68. Pong S., M.B. Shackleton, S.J. Taylor, and X. Xu. 2004, Forecasting currency volatility: a
comparison of implied volatilities and ARFIMA models, Journal of Banking and
Finance 28, 2541–2563.
69. Rasmussen, T.B. 2009, Jump Testing and the Speed of Market Adjustment, Aarhus
University School of Economics and Management preprint.
70. Rogers, L.C.G. 1997, Arbitrage with fractional Brownian motion, Mathematical Finance
7, 95–105.
71. Shreve, S.E. 2004, Stochastic Calculus for Finance II: Continuous-Time Models.
Springer.
72. Taylor, S.J., and X. Xu. 1997, The incremental volatility information in one million
foreign exchange quotations, Journal of Empirical Finance 4, 317–340.
73. Taylor, S.J. 2005, Asset Price Dynamics, Volatility, and Prediction, Princeton
University Press.
74. Taylor, S.J. 2010, An Econometric Defence of Pure-Jump Price Dynamics, Lancaster
University Management School preprint.
75. Theodosiou, M., and F. Zikes. 2011, A Comprehensive Comparison of Alternative Tests
for Jumps in Asset Prices, Central Bank of Cyprus and Imperial College London
preprint.
76. Todorov, V., 2011, Econometric analysis of jump-driven stochastic volatility models,
Journal of Econometrics 160, 12–21.
77. Vasudeva, R., and A.Y. Moridani. 2010, Limit distributions of the extremes of a random
number of random variables in a stationary Gaussian sequence, ProbStat Forum 3, 78–
90.
\n\n=== PAGE 208 ===\n78. Veraart, A.E.D. 2010, Inference for the jump part of quadratic variation of Itô semi-
martingales, Econometric Theory 26, 331–368.
79. Zhang, L., P.A. Mykland, and Y. Aït-Sahalia. 2005, A tale of two time scales: determining
integrated volatility with noisy high-frequency data, Journal of the American Statistical
Association 100, 1394–1411.
Notes
1 [48, 40].
2 [10, 22, 36].
3 Barndorff-Nielsen and Shephard [16, 51 and 2] find that more than 10% of days in their
samples are with jumps, whereas in [43] only a couple of jumps are expected to occur per
annum.
4 [24, 45].
5 [12; 53; 54; 56].
6 See the discussion in [74].
7 The assumption may also contradict with the jump-driven volatility models in References
[13, 76].
8 Both [39, 69] document this result.
9 Breidt and Davis [30] show that the extreme value distribution has the same scaling and a
slightly larger centering constant under a standard stochastic volatility model than for
i.i.d. Gaussian variables.
10 [37, 70].
11 A process is a local martingale if there is an increasing sequence of stopping times such
that the stopped process is a martingale [66].
12 Unlike the conventional long-span asymptotic theorems, the convergence here is in-fill
asymptotic.
13 Around 20% at 5% level when there is a single jump per day accounting for 5% of total
price variation.
14 Jump tests constructed using realized measures include, among others, the threshold-
based estimator of [62, 63], the ratio of power variations in [2], the threshold-attached
BP in [34], the quantile-based estimator of [31], and the bootstrap technique in [67].
15 One relevant study is that of Bajgrowicz and Scaillet [9], who treat the problem as
controlling the false discovery rate in multiple testing suggested by Benjamini and
Hochberg [23].
16 See also [35, 43, 46, 77].
17 [45, 48, 64].
\n\n=== PAGE 209 ===\n18 The three approaches are the two-scaled realized variance of the study [79], the realized
kernel estimator of the study [20], and the preaveraging estimator of the study [52].
19 This ratio is estimated using 5-minute returns. At 2- and 10-minute frequencies the ratios
are 4.8 and 4.3.
20 The assumption that et ~ N(0, σ2
e) is justified by the observation that log(BP) is close to
normal. We adjust for Jensen's Inequality as in [68] when converting E[log(BP)t|It − 1] to
E[BPt|It − 1].
21 In [26], the same point is taken and high-frequency jumps in prices are first identified
before jump tails behavior is estimated.
22 Our approach is related to Breidt and Davis [30], who show that for maximums from a
stochastic volatility model, the Gumbel limit can be derived but with a slightly different
centering constant.
23 These studies include [4; 44; 68].
24 The leverage effect is driven by correlated diffusive movements and so jumps dynamics
are irrelevant; we use E[Nt] = 0.40 in constructing the loss function.
25 The two AR (1) processes are not equally weighted; the persistent process, with α1 above
0.986, accounts for nearly 90% of the total weight; see Appendix 6.B.
26 Timely knowledge on jump occurrences may be relevant in obtaining the conditional
probability of jump arrival in the next interval, particularly when jumps in prices present
some degree of clustering.
27 Coles [32] also notes that extreme value theory is valid only for stationary processes and
so the nonstationary intraday volatility patterns must be addressed before applying
extreme value methods.
28 As our SV2FJ_2ρ model underestimates volatility persistence at long lags, the calibrated
ξ values may still be biased downward and so there are, at most, some spurious
detections.
29 To save space, we present only the results at the 2- and 5-minute frequencies.
30 This is different from Boudt et al. [28, 29], who find that jump detections most often
occur at 14:00 EST.
\n\n=== PAGE 210 ===\nChapter Seven
Hawkes Processes and Their Applications to
High-Frequency Data Modeling
Baron Law and Frederi G. Viens
Purdue University, West Lafayette, IN, USA
7.1 Introduction
This short chapter introduces and surveys an emerging class of
stochastic point processes used in modeling the evolution of high-
frequency (HF) data on stock markets at a high level of quantitative
detail.
The information contained in a stock market’s Limit Order Books
(LOB) is a multivariate time series that records the order arrival
times and volumes at each price level of thousands of stocks trading
on the exchange. An LOB exhibits a number of distinctive
characteristics [1–3], including
1. irregular time interval between arrivals,
2. discrete state space of price ticks and volume lot sizes,
3. intraday seasonality (more activities around market open and
close),
4. arrival clustering,
5. self-excitation from its own history,
6. cross (mutual) excitation from the history of other assets, and
7. long memory of excitation effect.
Consequently, classical time series models with fixed time intervals
such as ARIMA and GARCH are not suitable to model HF financial
data. A standard approach commonly used in practice is to resample
the data in 5-min intervals [4, 5], thereby avoiding the time scale for
\n\n=== PAGE 211 ===\nliquid stocks where many of the characteristics listed earlier can be
observed, but this may amount to discarding more than 99% of the
data for such stocks. On the contrary, Poisson processes, which are
widely used in the market microstructure literature [6, 7], fail to
depict the aforementioned prevalent features of HF data.
This survey paper, on the current research in HF financial data
modeling, concentrates on the use of so-called Hawkes processes, a
family of point processes designed to model self- and cross-
excitation. In Section 7.2, we offer an informal introduction to point
processes while a more technical review can be found in Appendix
7.A. All the presented material about point processes is thoroughly
covered in major textbooks including our references [8–12]. Sections
7.3 and 7.4 introduce Hawkes processes and their statistical
inference; a brief history thereof is provided in Appendix 7.B. Section
7.5 presents the applications of Hawkes processes to HF data
modeling.
7.2 Point processes
This section provides an informal introduction to point processes. A
more rigorous treatment using random measures and martingale
theory can be found in Appendix 7.A.
A point process is a random countable set of points {x1, x2, ...} on a
set X. The {xn}’s are X-valued random variables; they can have a
highly nontrivial dependence structure. For example, {xn}’s can be
locations of earthquakes on 
, and if self-excitation (property 5
above) is built into the model, which is the case with Hawkes
processes as we will discuss, then {xn}’s can exhibit random patterns
of aftershocks surrounding major earthquakes.
Let N(A) be the number of points inside a region A⊆ X. If N(A) is
known for all subsets A of X, this essentially determines the locations
of all the points and this representation is the principal tool to
describe a point process. A point process N is called simple if each
location has at most one point.
\n\n=== OCR PAGE 211 ===\nliquid stocks where many of the characteristics listed earlier can be
observed, but this may amount to discarding more than 99% of the
data for such stocks. On the contrary, Poisson processes, which are
widely used in the market microstructure literature [6, 7], fail to
depict the aforementioned prevalent features of HF data.

This survey paper, on the current research in HF financial data
modeling, concentrates on the use of so-called Hawkes processes, a
family of point processes designed to model self- and cross-
excitation. In Section 7.2, we offer an informal introduction to point
processes while a more technical review can be found in Appendix
7.A. All the presented material about point processes is thoroughly
covered in major textbooks including our references [8—12]. Sections
7.3 and 7.4 introduce Hawkes processes and their statistical
inference; a brief history thereof is provided in Appendix 7.B. Section
7.5 presents the applications of Hawkes processes to HF data
modeling.

7.2 Point processes

This section provides an informal introduction to point processes. A
more rigorous treatment using random measures and martingale
theory can be found in Appendix 7.A.

A point process is a random countable set of points {x,, x5, ...} on a
set X. The {x,,}’s are X-valued random variables; they can have a
highly nontrivial dependence structure. For example, {x,}’s can be

locations of earthquakes on X = R’, and if self-excitation (property 5
above) is built into the model, which is the case with Hawkes
processes as we will discuss, then {x,,}’s can exhibit random patterns

of aftershocks surrounding major earthquakes.

Let N(A) be the number of points inside a region A C X. If N(A) is
known for all subsets A of X, this essentially determines the locations
of all the points and this representation is the principal tool to
describe a point process. A point process N is called simple if each
location has at most one point.
\n\n=== PAGE 212 ===\nWhen X is the positive half-line 
, this typically represents the time
axis and the points are regarded as the times of event occurrences. In
this case, N(t) = N((0, t]) denotes the number of occurrences at or
before time t. For our purpose of modeling HF data, we will mainly
deal with point processes on 
.
When an event happens at 
, it may carry an additional
information yn (its mark). For instance, in finance, each order arrival
is associated with an order quantity (volume); in seismology, each
earthquake is reported with a magnitude. A point process with marks
is called a marked point process (MPP). Let Y be the mark space (i.e.,
yn ∈ Y) and A⊆Y; then N(t, A) = N((0, t] × A) denotes the number
of events that happened at or before time t such that the marks fall
within the set A. The ground process Ng(t) = N((0, t] × Y) is the
count of all events in (0, t], regardless of the marks.1 The marks of an
MPP are called unpredictable if yn is independent of {(ti, yi)}i < n and
they are called independent if yn is independent of {(ti, yi)}i ≠ n.2 An
MPP N is called simple if Ng is simple.
If a point process has multiple occurrences in the same location, we
can treat it as a simple MPP with the mark being the number of
points in each location. Hence, without loss of generality, most of the
results to be presented will be based on simple point processes.
N is called a multivariate point process if the mark space Y = {1, …,
d}. In this case, Ni(•) = N(• × {i}) is called the marginal process for
the points of type i. A simple d-variate point process is different from
a d-dimensional (
-valued) simple point process as the former
cannot have any common jump times. In addition, if the multivariate
point process also carries some extra information wn ∈ W at each
point, the mark space will become Y = {1, …, d} × W. In this case, the
marginal processes are Ni(t, A) = N((0, t] × {i} × A) and the marginal
ground processes are Ni(t) = N((0, t] × {i} × Y).
Many point processes can be modeled in terms of their stochastic
intensities3 λ(t), which can be defined informally as the expected
number of arrivals per unit of time, at time t, conditioned on all the
information just before time t, that is,
\n\n=== OCR PAGE 212 ===\nWhen X is the positive half-line Ry, this typically represents the time
axis and the points are regarded as the times of event occurrences. In
this case, N(t) = N((o, t]) denotes the number of occurrences at or
before time t. For our purpose of modeling HF data, we will mainly
deal with point processes on “+.

When an event happens at ‘n © Ry it may carry an additional
information y,, (its mark). For instance, in finance, each order arrival
is associated with an order quantity (volume); in seismology, each
earthquake is reported with a magnitude. A point process with marks
is called a marked point process (MPP). Let Y be the mark space (i.e.,
Yn € Y) and ACY; then N(t, A) = N((0, t] x A) denotes the number
of events that happened at or before time t such that the marks fall
within the set A. The ground process N,(t) = N((0, t] x Y) is the

count of all events in (0, t], regardless of the marks.* The marks of an
MPP are called unpredictable if y,, is independent of {(t;, y;)}; <n and
they are called independent if y,, is independent of {(t;, y;)}; + n-2 AN
MPP Nis called simple if N, is simple.

If a point process has multiple occurrences in the same location, we
can treat it as a simple MPP with the mark being the number of
points in each location. Hence, without loss of generality, most of the
results to be presented will be based on simple point processes.

Nis called a multivariate point process if the mark space Y = {1, ...,
d}. In this case, N,(+) = N(+ x {i}) is called the marginal process for
the points of type i. A simple d-variate point process is different from
a d-dimensional (R‘-valued) simple point process as the former
cannot have any common jump times. In addition, if the multivariate
point process also carries some extra information w,, € Wat each
point, the mark space will become Y = {1, ..., d} x W. In this case, the
marginal processes are N;,(t, A) = N((0, t] x {i} x_A) and the marginal
ground processes are N;,(t) = N((0, t] x {i} x Y).

Many point processes can be modeled in terms of their stochastic
intensities? A(t), which can be defined informally as the expected
number of arrivals per unit of time, at time t, conditioned on all the
information just before time t, that is,
\n\n=== PAGE 213 ===\n(7.1)
(7.2)
where 
, which represents the information contained in all the
events happened up to and including time t, denotes the filtration of
N. For a multivariate point process, λi(t) is the intensity of the
marginal process Ni(t).
The most well-known point processes are Poisson processes, where
the intensities are deterministic functions of time. When one
generalizes the intensities to become stochastic processes on their
own, they play a role similar to stochastic volatility in diffusions. One
important use of stochastic intensity is to allow it to change
according to how events unfold over time. The Hawkes processes are
directly exploring this feature for the purpose of modeling self-
excitation, as we are about to see.
7.3 Hawkes processes
A Hawkes process [13] is a point process where its stochastic
intensity has an autoregressive form. For a nonlinear multivariate
marked Hawkes process, the intensity λ(t) = (λ1(t), …, λd(t)) is given
by4,5
where wn ∈ {1, …, d} denotes the type of tn and Φi is known as rate
function. Consider the special case
\n\n=== OCR PAGE 213 ===\nE(N((t — h, t]) | Fy») (7.1)
h
where /*:, which represents the information contained in all the

events happened up to and including time t, denotes the filtration of
N. For a multivariate point process, A,(t) is the intensity of the

marginal process N,(t).

A(t) = lim
h-0+

The most well-known point processes are Poisson processes, where
the intensities are deterministic functions of time. When one
generalizes the intensities to become stochastic processes on their
own, they play a role similar to stochastic volatility in diffusions. One
important use of stochastic intensity is to allow it to change
according to how events unfold over time. The Hawkes processes are
directly exploring this feature for the purpose of modeling self-
excitation, as we are about to see.

7.3 Hawkes processes

A Hawkes process [13] is a point process where its stochastic
intensity has an autoregressive form. For a nonlinear multivariate
marked Hawkes process, the intensity A(f) = (A,(0), ..., Ag(t)) is given

by45
d (7.2)
A(t) = ®; y/ yy(t — s, y)Nj(ds x dy), t
j=l (—00,t)xY

=, YX Yiw,(t —tY,).¢
t,<t

®,:RXR,—>R,, ¥:R,XYR,
N,: BIR, x Y) > N

where w,, € {1, ..., d} denotes the type of t,, and ®; is known as rate
function. Consider the special case
\n\n=== PAGE 214 ===\n(7.3)
(7.4)
that is, Φi(x, t) = μi(t) + x. Such a Hawkes process determined by
(7.3) is called linear, and μi(t) is called the base or background rate.
The function γij is called (marked) decay/exciting/fertility kernel and
often γij(t, y) takes the separable form γij(t)gij(y), where gij is called
mark impact kernel. Popular choices of decay kernel γij(t) include
exponential 
 [13], power law 
 [14], or Laguerre-
type polynomial 
 [15].
If the decay function is exponential with βij = βi, the intensity λ(t)
and the vector (N(t), λ(t)) are both Markov processes6 [16, 17].
Moreover, provided that μi(t) = μi, then (λ1(t), …, λd(t)) satisfies the
system of stochastic differential equations
This specification has the simple interpretation that the events of Nj,
which happened just before time t, increase the intensity λi(t) by αij ≥
0 and thus trigger further events. Yet if the intensity λi(t) is higher
than μi, the first term becomes negative (βi > 0) and draws the
intensity back to the equilibrium level μi. In other words, the
intensity λi(t) is a mean-reverting process driven by its own point
process. The Markov property and this intuitive interpretation may
explain why the exponential decay kernels are so widely used.
\n\n=== OCR PAGE 214 ===\n(7.3),
10-0 Ban y(t — s,y)Nj(ds x dy) = u(t)

00) XY

t,<t
Hi R,—R,, 7: R,xY—R,,
N;: BIR, x Y) > N

that is, O,(x, t) = u,(t) + x. Such a Hawkes process determined by
(7.3) is called linear, and p1,(f) is called the base or background rate.
The function y,; is called (marked) decay/exciting/fertility kernel and
often y;(t, y) takes the separable form y;(t)gj(y), where gj is called
mark impact kernel. Popular choices of decay k kernel y,,(¢) include

exponential “i “3h power law MC +?

type polynomial re 9 Mute Pt [15].
If the decay function is exponential with B,; = B;, the intensity A(t)

and the vector (N(#), A(t)) are both Markov processes® [16, 17].
Moreover, provided that u,(f) = p;, then (A,(0), ..., Ag(t)) satisfies the
system of stochastic differential equations

“ [14], or Laguerre-

é (7.4)
dst) = Bu; — AO)dt + Y) aydN(0)

j=1
This specification has the simple interpretation that the events of N;,
which happened just before time t, increase the intensity A,(t) by a;
o and thus trigger further events. Yet if the intensity A,(t) is higher
than ;, the first term becomes negative (8; > 0) and draws the
intensity back to the equilibrium level 1;. In other words, the
intensity ),(t) is a mean-reverting process driven by its own point

process. The Markov property and this intuitive interpretation may
explain why the exponential decay kernels are so widely used.

j=
\n\n=== PAGE 215 ===\nFor linear Hawkes processes, μi(t), γij(t), and gij(t) must be
nonnegative for all t, in order to ensure the positivity of λi(t). As a
result, unlike nonlinear Hawkes processes, linear Hawkes processes
cannot model inhibitory effect (negative excitation). Nonetheless, the
linear Hawkes processes are easier to handle, their properties are
better understood and, most importantly, they have a branching
structure representation, which is extremely useful in simulation,
estimation, and interpretation of the models.
7.3.1 BRANCHING STRUCTURE REPRESENTATION
Linear Hawkes processes have a very elegant branching structure
representation [18]. We describe here the version for the
multivariate Hawkes processes with unpredictable marks [19].
There are d types of immigrants arriving according to Poisson
processes with rates μ1, …, μd. Each individual (descendant or
immigrant) will carry an unpredictable mark when born or arrived.
An individual of type j born at time tn with mark yn will give birth to
an individual of type i according to a nonhomogeneous Poisson
process with rate γij(t − tn, yn). All the nonhomogeneous Poisson
processes are independent of one another.
Let Ni(t) be the total number of individuals of type i born/arrived at
or before time t under the above scenario, then N(t) = (N1(t), …,
Nd(t)) will follow the linear marked Hawkes process (7.3). This
representation forms the basis of the Expectation Maximization
(EM) algorithm in Section 7.4.2 and we will also see how it is used to
measure the endogeneity of a point process in Section 7.5.4.
7.3.2 STATIONARITY
Considering a Hawkes process N with intensity (7.2) such that Φi(x,
t) = Φi(x), N has an unique stationary version7 if either of the
following conditions is satisfied [18, 20]:
1. Φi(x) is ki-Lipschitz8 and the spectral radius9 ρ(A) < 1 for the d ×
d matrix  A = [ki∫∞
0|γij(t)|dt]i, j.
\n\n=== PAGE 216 ===\n2. Φi(x) is Lipschitz,
.
Technically speaking, N may have other nonstationary versions
together with the stationary one; however, the nonstationary version
will converge weakly to the stationary version when t → ∞ (see [21]
for exact meaning). Since the Hawkes process starts at − ∞, N((0, t])
will have the stationary distribution for all t > 0.
For the case of an exponential decay kernel 
, we have a simpler
result. Let 
, then N has a unique
stationary version under either of the following conditions [22]:
1. Φi(x) = μi + x, αij ≥ 0, βij, μi > 0, ρ(A) < 1 (linear Hawkes process).
2. 
 (T-
Hawkes process).
3. 
 (E-
Hawkes process).
For the univariate linear case with μ = 0, if there exists r, R > 0, c ∈
(0, 1/2) such that ∫∞
0γ(t)dt = 1, 
, limt → ∞t1 + cγ(t) =
r, Brémaud and Massoulié [23] show that there exists a unique
stationary nontrivial Hawkes process having such an intensity and he
calls it the critical Hawkes process or Hawkes process without
ancestors (μ = 0).10 11
7.3.3 CONVERGENCE
In this section, we state the various results about the convergence of
Hawkes processes. A properly scaled linear Hawkes process will
converge weakly to a Brownian diffusion when the spectral radius of
decay functions’ L1-norm is less than 1 [24]. When the spectral radius
is close to 1 in a certain sense, it converges to the integrated Cox–
Ingersoll–Ross (CIR) process [25]. For the nonlinear Hawkes
processes, we have only the result for the univariate case and the
sufficient conditions depend on the Lipschitz constant of Φ [26].
\n\n=== OCR PAGE 216 ===\n2. (x) is Lipschitz,
@,(x) <M, [* Iv()|dt < 00 and I tly@|dt < 00

Technically speaking, N may have other nonstationary versions
together with the stationary one; however, the nonstationary version
will converge weakly to the stationary version when t — o (see [21]
for exact meaning). Since the Hawkes process starts at — ~, N((0, t])
will have the stationary distribution for all t > o.

Bit :
For the case of an grponential decay kernel “© “” , we havea simpler
= —fiyt _
result. Let 4 = Lo ae dt], = la/ Bil... then N has a unique
stationary version under either of the following conditions [22]:
1. D(X) = Hy +X, ay 2 0, By, i; > 0, p(A) < 1 (linear Hawkes process).
2. @,(x) = max(y; +x, €;), a; € R, Bis Hi > 0, €; > 0, p(A) < 1 (T-
Hawkes process).
3. (x) = min(y; + exp(x), Mj), a ER, Bj > 0, M; > nH; > OE
Hawkes process).

For the univariate linear case with 1 = 0, if there exists r, R > 0,c €
(0, 1/2) such that fy(édt = 1, SUP:20 tey(t) < R lim, _, ot? * yO) =
r, Brémaud and Massoulié [23] show that there exists a unique
stationary nontrivial Hawkes process having such an intensity and he
calls it the critical Hawkes process or Hawkes process without
ancestors (ut = 0).2° 44

7.3.3 CONVERGENCE

In this section, we state the various results about the convergence of
Hawkes processes. A properly scaled linear Hawkes process will
converge weakly to a Brownian diffusion when the spectral radius of
decay functions’ L'-norm is less than 1 [24]. When the spectral radius
is close to 1 in a certain sense, it converges to the integrated Cox—
Ingersoll—Ross (CIR) process [25]. For the nonlinear Hawkes
processes, we have only the result for the univariate case and the
sufficient conditions depend on the Lipschitz constant of ® [26].
\n\n=== PAGE 217 ===\n(7.5)
(7.6)
(7.7)
(7.8)
7.3.3.1 Law of large numbers for multivariate linear Hawkes
processes
Assuming the model (7.3) without marks, if the spectral radius ρ(A)
< 1, where A = [∫∞
0γij(t)dt]i, j, then [24]
where μ = (μ1, …, μd). When d = 1 and we take t = 1, it implies
7.3.3.2 Functional central limit theorem for multivariate linear
Hawkes processes
Assuming the model (7.3) without marks, N = (N1, …, Nd), if the
spectral radius ρ(A) < 1, where A = [∫∞
0γij(t)dt]i, j and
, then [24]
12
13
7.3.3.3 Functional central limit theorem for univariate nonlinear
Hawkes processes
Assuming the model (7.2) without marks and d = 1, if γ(t) is
decreasing, ∫∞
0tγ(t)dt < ∞, Φ(x, t) = Φ(x) is increasing and k-
Lipschitz, ∫∞
0kγ(t)dt < 1, then [26]
\n\n=== OCR PAGE 217 ===\n7.3.3.1 Law of large numbers for multivariate linear Hawkes
processes

Assuming the model (7.3) without marks, if the spectral radius p(A)

<1, where A = [J™,y;(t)d¢];, ;, then [24]
.s./L?
N(nt) tl, arta as/Lt io. (7.5)
te[0,1] n

where LL = ([1,, .-., Hg). When d = 1 and we take t = 1, it implies

N(T) as/P? ia (7.6)
T T-00 1— [S° y(t

7.3.3.2 Functional central limit theorem for multivariate linear
Hawkes processes
Assuming the model (7.3) without marks, N = (N,, ..., Ng), if the
spectral radius p(A) < 1, where A = [S*ovy@Odtl;, j and
I Viy,(dt < © Vij then [24]

weak

a/n (N(en)/n (ly Ay!) (1, A)! !/2Wee)y!? (2D,

noo

(7.8)
xX = diag((J, — Ay 2, W is standard d-dimensional Brownian Motion

13
7.3.3.3 Functional central limit theorem for univariate nonlinear
Hawkes processes

Assuming the model (7.2) without marks and d = 1, if y(d) is
decreasing, [*,ty(t)dt < ©, O(x, t) = (x) is increasing and k-
Lipschitz, f° ky(t)dt < 1, then [26]
\n\n=== PAGE 218 ===\n(7.9)
(7.10)
(7.11)
(7.12)
(7.13)
(7.14)
7.3.3.4 Convergence of nearly unstable univariate linear Hawkes
processes
Considering the linear model (7.3) without marks and d = 1, N(T)/T
converges to μ/(1 − ∫∞
0γ(t)dt), when ∫∞
0γ(t)dt < 1 by (7.6), while it
explodes when ∫∞
0γ(t)dt = 1. However, Jaisson and Rosenbaum [25]
find that the properly scaled Hawkes process converges to the
integrated CIR process when one has a sequence of decay kernel γ(n)
(t) whose integral converges to 1 at the speed of n− 1 (see 7.13). More
precisely, let
where f⊗k denotes the k-fold self-convolution of f. If the sequence of
Hawkes process N(n) has intensity λ(n) satisfying (7.11–7.14), then the
\n\n=== OCR PAGE 218 ===\nVn (N(en)/n — 0v) + o Wie) (7.9)

o? = E((N((0, 1]) — v)?) +2 ¥ E((N({0, 1]) — v)
n=1

x (N([n, n+ 1]) — v), v = E(N([O, 1]))

(7.10)

7.3.3.4 Convergence of nearly unstable univariate linear Hawkes
processes

Considering the linear model (7.3) without marks and d = 1, N(T)/T
converges to /(1 — S*,y(Hdt), when f*,y(t)dt < 1 by (7.6), while it
explodes when f®*,y(t)dt = 1. However, Jaisson and Rosenbaum [25]
find that the properly scaled Hawkes process converges to the
integrated CIR process when one has a sequence of decay kernel y™

(t) whose integral converges to 1 at the speed of n™ 1 (see 7.13). More
precisely, let

AMO) = +/ yY(t—s)dN(s), pw>0, yOO=a”y(t) (7.11)
(0.1)
co © (7.12)
y:R,—R,, [ y(t)dt = 1, | ty(t)dt = m < &, 7
0 0
[ ly'(Dldt < co, sup |y'(t)| < co
0 t€[0,00)
a” €f0.1), lima” =1, limn(l-a”)=c>0 (7.13)
noo noo
(7.14)
— ny” (nt)
y(t) = YX yO@ ny, pt) = » [p@o| < Mn vt
= I w(t)dt

where f®* denotes the k-fold self-convolution of f, If the sequence of
Hawkes process N™ has intensity \ satisfying (7.11-7.14), then the
\n\n=== PAGE 219 ===\n(7.15)
(7.16)
(7.17)
scaled intensity converges to the CIR process and the scaled Hawkes
process converges to the integrated CIR process [25] as follows:
7.4 Statistical inference of Hawkes
processes
7.4.1 SIMULATION
In this section, we give an overview of the algorithms that simulate
Hawkes processes. Assume that we know all the parameters in the
functional form of μ(t) and γ(t, y), our goal is to simulate the points
(t1, y1), (t2, y2), ... on the interval [0,T].
If the marks distribution depends only on tn, we can simply generate
yn conditioned on the generated tn. Next, tn + 1 can be generated from
the intensity λ(t) for t > tn, which depends on {(t1, y1), …, (tn, yn)}. If
the distribution of yn also depends on {(tn − 1, yn − 1), (tn − 2, yn − 2),
...}, the algorithms can be modified accordingly.
7.4.1.1 Inverse CDF transform
The first simulation algorithm for Hawkes processes appears in
Ozaki [29]. Suppose that the intensity is governed by the univariate
Hawkes model in (7.3). Let tn be the arrival time and τn = tn − tn − 1
be the interarrival time. By (7.63), λ(t) = hn(t − tn − 1) for t ∈ (tn − 1,
tn], where hn(t) = gn(t)/(1 − Gn(t−)) and g, G are the conditional pdf,
\n\n=== OCR PAGE 219 ===\nscaled intensity converges to the CIR process and the scaled Hawkes
process converges to the integrated CIR process [25] as follows:

weak

da- a) A (ne) X(e) (7.15)

n>0o

N'(ne eak ° .
a— a X2@ sek X(s)ds (7.16)
n n>oo 0
(7.17),

Cc
dX, = “(uw —X,)dt + Vv Raw, Xp =0
m m

7.4 Statistical inference of Hawkes
processes

7.4.1 SIMULATION

In this section, we give an overview of the algorithms that simulate
Hawkes processes. Assume that we know all the parameters in the
functional form of y(¢) and y(t, y), our goal is to simulate the points
(t,, Y1), (to, Yo), -.. on the interval [0,7].

If the marks distribution depends only on t,, we can simply generate
Yn conditioned on the generated t,. Next, t, , , can be generated from
the intensity A(f) for t > t,, which depends on {(t,, y,), .-.. (ty, Und}. If
the distribution of y, also depends on {(t,_ 1, Yn - 1), (th— 2. Un- 2),
...}, the algorithms can be modified accordingly.

7.4.1.1 Inverse CDF transform

The first simulation algorithm for Hawkes processes appears in
Ozaki [29]. Suppose that the intensity is governed by the univariate
Hawkes model in (7.3). Let t,, be the arrival time and t,, = t, - t,_,
be the interarrival time. By (7.63), A) =h,(t- t, _,) fort € (t,_,,

t,], where h,(t) = g,(0)/( - G,(t-)) and g, G are the conditional pdf,
\n\n=== PAGE 220 ===\n(7.18)
cdf of τn given 
. If Gn(t) is continuous, hn(t) is simply the hazard
function, and it can be shown that
Given tn − 1, we can generate tn = tn − 1 + τn by inverse cdf transform
. However, the inversion needs to be
done numerically, so this method is largely superceded by Ogata’s
modified thinning that we now discuss.
7.4.1.2 Ogata’s modified thinning
Ogata [30] introduces the modified thinning method that does not
require numerical inversion. The algorithm is based on the following
theorem. Let N = (N1, …, Nd) be a multivariate point process with
intensity (λ1, …, λd) such that 
 (λ*(t) is an
exogenously chosen deterministic rate function) and N* is the
univariate nonhomogeneous Poisson process with intensity λ*(t). If
each point tn in N* is given a mark yn such that
, then (N*1, …, Nd*) has the
same distribution as (N1, …, Nd).
The following algorithm generates a d-dimensional multivariate
Hawkes process such that λi(t) is decreasing between points and |
λi(t) − λi(t−)| ≤ αi ∀t.
Ogata’s Modified Thinning [30]
1. n = 1, t0 = 0.
2. Generate 
.
3. Let tn = tn − 1 + τn.
4. Generate Un ~ Unif(0, 1).
\n\n=== OCR PAGE 220 ===\ncdf of t, given FTE G,( is continuous, h,,(t) is simply the hazard
function, and it can be shown that

(7.18)
Tr ty tTp
G,(t,,) = 1 — exp (-/ hy(ods) = 1-—exp (- / iiss)
0 trot
Given t,, _ ,, we can generate t,, = t,, _ , + T, by inverse cdf transform
=C if
7, = G,'(U), U ~ Unif(O, 1) Fowever, the inversion needs to be

done numerically, so this method is largely superceded by Ogata’s
modified thinning that we now discuss.

7.4.1.2 Ogata’s modified thinning

Ogata [30] introduces the modified thinning method that does not
require numerical inversion. The algorithm is based on the following
theorem. Let N = (N,, ..., Ng) be a multivariate point process with

d r as
intensity (A,, ..., Ag) such that Lies Ai) < A*(H) Ve as. (A*(6) is an
exogenously chosen deterministic rate function) and N* is the

univariate nonhomogeneous Poisson process with intensity A*(t). If
each point t, in N* is given a mark y,, such that

P(y, = i) = At,)/A"(t,), i= 1,-5.4, then (N*,, ..., Nq*) has the
same distribution as (N,, ..., Ng).

The following algorithm generates a d-dimensional multivariate
Hawkes process such that A,(t) is decreasing between points and |
A(t) - A\(t)| < a; ve.

Ogata’s Modified Thinning [30]

1.n=1,ty=0.

~ Exp(4*) for some 4* > YA Odts1 )+a;)

2. Generate *n
3. Lett, =tyh_-1+Tp.

4. Generate U,, ~ Unif(o, 1).
\n\n=== PAGE 221 ===\n5. If 
return tn and the point is of type k, else discard tn (but keep the
value for use in next-generation steps 2 and 3).
6. n = n + 1, go to step 2.
7.4.1.3 Simulation by branching structure
This method generates points using the branching structure
representation of linear marked Hawkes processes. Type-j
immigrants arrive according to a nonhomogeneous Poisson process
with rate μj(t). Next, the type-j parent arriving at tn with mark yn
produces type-i descendants according to a nonhomogeneous
Poisson process with rate γij(t − tn, yn) and the generation is repeated
for each descendant until all of them exceed the predefined time T.
Since all the nonhomogeneous Poisson processes are independent,
the generations can be done in parallel.
Simulation by the Branching Structure [31]
1. Generate nonhomogeneous Poisson processes with intensities
μi(t), i = 1, …, d on [0, T].
2. For each points tn, generate yn|tn.
3. Suppose that tn is of type j and generates type-i descendants
according to nonhomogeneous Poisson process with intensity γij(t
− tn, yn) on [tn, T], i = 1, …, d.
4. Repeat steps 2 and 3 for all descendants.
The nonhomogeneous Poisson process with intensity μ(t) on [0, T]
can be generated using Lewis’ thinning algorithm [32]:
1. Generate N ~ Poisson(μ*) for some μ* ≥ maxt ∈ (0, T]μ(t).
2. Generate Un ~ Unif(0, 1), n = 1, …, N.
3. Tn = U(n)T, n = 1, …, N ({U(n)} is the order statistics of {Un}).
4. Generate Vn ~ Unif(0, 1), n = 1, …, N.
\n\n=== OCR PAGE 221 ===\n5. Un © (Len Ailty)/AS, Diy Ailt,)/A5] for some k € {1,....d}

return t,, and the point is of type k, else discard t,, (but keep the
value for use in next-generation steps 2 and 3).

6.n=n +1, go to step 2.

7.4.1.3 Simulation by branching structure

This method generates points using the branching structure
representation of linear marked Hawkes processes. Type-j
immigrants arrive according to a nonhomogeneous Poisson process
with rate 11(t). Next, the type-j parent arriving at ¢, with mark y,,
produces type-i descendants according to a nonhomogeneous
Poisson process with rate y;(t — t,,, y,) and the generation is repeated
for each descendant until all of them exceed the predefined time T.
Since all the nonhomogeneous Poisson processes are independent,
the generations can be done in parallel.

Simulation by the Branching Structure [31]

Generate nonhomogeneous Poisson processes with intensities
u(t), 7= 1, ...,d on [0, T]

r

y

For each points t,,, generate y,,|t,.

Suppose that ¢, is of type j and generates type-i descendants
according to nonhomogeneous Poisson process with intensity y,,(¢
— ty, Y,) on [t,, T],i=1,...,d.

4. Repeat steps 2 and 3 for all descendants.

Ye

The nonhomogeneous Poisson process with intensity p(t) on [0, T]
can be generated using Lewis’ thinning algorithm [32]:

1. Generate N ~ Poisson(*) for some p* > max; ¢ (0, TIH(t).

2. Generate U,, ~ Unif(o, 1), n = 1, ..., N.

3. Ty = UT, n = 1, ..., N {Up} is the order statistics of {U,,}).
4. Generate V,, ~ Unif(o, 1), n = 1,..., N.
\n\n=== PAGE 222 ===\n(7.19)
(7.20)
5. Return Tn if Vn ≤ (μ(Tn)/μ*), n = 1, …, N, otherwise discard Tn.
7.4.2 ESTIMATION
Suppose that we observe a point process on (0, T] and collect the
event times and marks {(t1, y1), …, (tN, yN)}, and now we would like
to estimate the functions μ(t) and γ(t, y) in the intensity λ(t), which
drives the process N(t). We will summarize the various methods
appearing in the literature, but so far the focus is on unmarked
processes. In the special case where the marks are independent and
identically distributed, the mark distribution can be estimated
separately from the point process.
If we assume that μ(t) and γ(t) have some parametric
representations, we can use Maximum Likelihood Estimation (MLE),
EM, or Generalized Method of Moments (GMMs) to estimate the
parameters. Otherwise, we need to rely on some advanced
nonparametric techniques to estimate the whole function curves.
7.4.2.1 Maximum likelihood estimation
The log-likelihood of a Hawkes process is given by [29]
In the case of multivariate linear Hawkes process, it becomes
The parameters θ can be estimated by maximizing the log-likelihood.
However, the numerical optimization is problematic as the log-
\n\n=== OCR PAGE 222 ===\n5. Return T,, if V,, < (u(T,,)/p*), n = 1, ..., N, otherwise discard T,,.

7.4.2 ESTIMATION

Suppose that we observe a point process on (0, T] and collect the
event times and marks {(t,, y,), ..., (tw, yy)}, and now we would like
to estimate the functions (f) and y(t, y) in the intensity A(t), which
drives the process M(t). We will summarize the various methods
appearing in the literature, but so far the focus is on unmarked
processes. In the special case where the marks are independent and
identically distributed, the mark distribution can be estimated
separately from the point process.

If we assume that p(t) and y(t) have some parametric
representations, we can use Maximum Likelihood Estimation (MLE),
EM, or Generalized Method of Moments (GMMs) to estimate the
parameters. Otherwise, we need to rely on some advanced
nonparametric techniques to estimate the whole function curves.
7.4.2.1 Maximum likelihood estimation

The log-likelihood of a Hawkes process is given by [29]

d

r rT (7.19)
log(L(@) = 1 (= | aj(t:O)de + / log(A,(t; 0))dN,(0)
0 0

i=l

In the case of multivariate linear Hawkes process, it becomes

(7.20)

T d N oT d
togtL(oy) =~ | H(t; 0) ) dt — / Vi (t— t,20) ) dt

n=l 4% 'n

N
+ } log ( Hy (tp) + Dy Yoogarg ln — fm’ »)

n=l tin <ty

im Sy

The parameters 6 can be estimated by maximizing the log-likelihood.
However, the numerical optimization is problematic as the log-
\n\n=== PAGE 223 ===\n(7.21)
(7.22)
(7.23)
likelihood function is usually quite flat (see [33, Figure 2,3]) and may
have a lot of local maxima (see, [33, Figure 4]).
7.4.2.2 Expectation maximization
For linear Hawkes process, the estimation can also be done via EM
[34, 35] as in [33, 36–39). EM is a variant of MLE where part of the
data is missing. In the branching structure representation, the
missing data is the parents that produce the descendants. Let zn
denote the index of the parent of tn and 
 represents the type of the
parent of tn. If zn = m and 
, that means tn is produced by the
type j point tm. When zn is 0, tn is an immigrant. Also, we define w0 =
0, γi, 0(t) = μi(t), and t0 = 0 to simplify the expression. Suppose that
{tn, wn, zn} are known, since each generation is an independent
Poisson process, the complete data log-likelihood is
The EM algorithm can be implemented as follows:
\n\n=== OCR PAGE 223 ===\nlikelihood function is usually quite flat (see [33, Figure 2,3]) and may
have a lot of local maxima (see, [33, Figure 4]).

7.4.2.2 Expectation maximization

For linear Hawkes process, the estimation can also be done via EM
[34, 35] as in [33, 36-39). EM is a variant of MLE where part of the
data is missing. In the branching structure representation, the
missing data is the parents that produce the descendants. Let z,,

denote the index of the parent of t,, and '’. represents the type of the

parent of t,. If z, =m and 'Y:, —/, that means t, is produced by the
typej point t,,. When z, is 0, t, is an immigrant. Also, we define wy =
O, Yi, o(t) = H,(O), and ty = O to simplify the expression. Suppose that
{ty, Wy, Zn} are known, since each generation is an independent
Poisson process, the complete data log-likelihood is

. (7.21)
log(L(@)) =} y {- [ove Yi, (t — ty3 O)dt
ty

n=0 i=

$Y 18s, ln = fos OE, = MUG, = a

tn >t,

(7.22)
Q(o|6) = EM doe LO t wp)

= » x {- [tw Vinog(t = ty3O)dt + LY LOLs, (tm = fn 9D)

n=0 i=1 in ty, >ty

pez, =nl{(t.w))l@,, = iy}

Yin, Atm — 43 9 U0w,, = i) (7-23)

PO ey = AUCs WTO", = 1) —
1=0 Viw; (t,, — 4 0)

The EM algorithm can be implemented as follows:
\n\n=== PAGE 224 ===\n(7.24)
(7.25)
(7.26)
(7.27)
(7.28)
1. k = 0 and choose an initial guess θ(0).
2. E-step: compute 
.
3. M-step: compute θ(k + 1) = argmaxθ Q(θ|θ(k)).
4. k = k + 1, repeat E-step and M-step until θ(k) converges (e.g., ||θ(k
+ 1) − θ(k)|| < ϵ).
In general, the optimization in M-step needs to be solved
numerically but when the decay kernel has the exponential form
αijβijexp ( − βijt), Olson and Carley [39] suggest a closed form
approximate iteration.
\n\n=== OCR PAGE 224 ===\n1. k = 0 and choose an initial guess 0.
2. E-step: compute 00/0) = E* log(LO))I{(t,. wi),
3. M-step: compute 0+» = argmaxy Q(6|0™).

4.k =k +1, repeat E-step and M-step until 0“ converges (e.g., ||0“
+) _ Q®|| <€).

In general, the optimization in M-step needs to be solved
numerically but when the decay kernel has the exponential form
a8 yexp (- Bit), Olson and Carley [39] suggest a closed form

approximate iteration.

De POR, = OG we) 10", = 8) (7.24)
T

Wid =
at - Di 1 Lie =ntl PE, = (tw) UW, = iw, =f) (7-25)
4 y, low, =)
o (7.26)
De Di PO En = Mes HY TOr, = i, =)

pit) = rs
Dyin = IPO Cy = AUCs We) Uy = bs ey, =A)

pz, =nl{(t.w,)}) 160, = iw, =A) (7.27)
(k) alk) (k)

a. 8 exp(—BE (tm — ty) VW = iW, =I)
H+ Dt al”) BY exp(— P(t, — t))

Pg, = Ol{(h. 10", = 1) (7.28)
10 =i)

m

~ p® m—1 al pi (k)
Fe Oe Bisw, XPCHB itm = 1)
\n\n=== PAGE 225 ===\nIn addition, the summation ∑m − 1
l = 1αi, wl
(k)β(k)
i, wlexp ( − β(k)
i, wl(tm
− tl)) can be truncated after 
 has decayed to a small
value. The speed of EM is reported to be 10–100 times faster than
MLE and, more importantly, MLE does not converge within 500
iterations in practically all test cases while EM does [39].
7.4.2.3 Generalized method of moments
Another method for statistical estimation apart from MLE is the
Generalized Method of Moments14 [40]. The idea is to find the
parameters that minimize the difference between theoretically
moments (see Appendix 7.A) in terms of the unknown parameters
and the empirical moments computed directly from the data. If we
have more moments than the number of parameters, the method
involves solving a weighted least squares problem.
Da Fonseca and Zaatour [41] obtain the analytic moment expressions
by restricting the process to be univariate with exponential kernel
and making use of the Markov property in this special case. The
authors claim that this method is extremely fast but no speed
comparison result is provided.
7.4.2.4 Nonparametric estimation
Without assuming any parametric form for μ(t) nor γ(t), some
nonparametric methods are developed recently to estimate the whole
base rate and decay kernel functions. Similar to parametric
estimation, penalized MLE or GMM is used to find the function with
desirable characteristics (e.g., smooth functions, sparse coefficients).
Nonetheless, the nonparametric method, which involves finding the
unknown functions in infinite-dimensional spaces, requires
extensive computational effort and the underlying statistical
construction is usually much more involved than the parametric
counterpart.
To the best of our knowledge, the first attempt in nonparameteric
estimation of Hawkes process is by Gusto and Schbath [42] in 2005.
The authors express the kernel function of the multivariate Hawkes
process, using B-splines [43] with equally spaced knots. The log-
likelihood function involving the basis coefficients is then maximized
\n\n=== OCR PAGE 225 ===\nIn addition, the summation 2" ~ 1 _ ,a;, w}B®, wEeXP (- Bp, wjttn

- (k)
— t;)) can be truncated after eXP(— Bis, (tm — 1) has decayed to a small
value. The speed of EM is reported to be 10-100 times faster than
MLE and, more importantly, MLE does not converge within 500
iterations in practically all test cases while EM does [39].

7.4.2.3 Generalized method of moments

Another method for statistical estimation apart from MLE is the
Generalized Method of Moments*4 [40]. The idea is to find the
parameters that minimize the difference between theoretically
moments (see Appendix 7.A) in terms of the unknown parameters
and the empirical moments computed directly from the data. If we
have more moments than the number of parameters, the method
involves solving a weighted least squares problem.

Da Fonseca and Zaatour [41] obtain the analytic moment expressions
by restricting the process to be univariate with exponential kernel
and making use of the Markov property in this special case. The
authors claim that this method is extremely fast but no speed
comparison result is provided.

7.4.2.4 Nonparametric estimation

Without assuming any parametric form for y:(f) nor y(t), some
nonparametric methods are developed recently to estimate the whole
base rate and decay kernel functions. Similar to parametric
estimation, penalized MLE or GMM is used to find the function with
desirable characteristics (e.g., smooth functions, sparse coefficients).
Nonetheless, the nonparametric method, which involves finding the
unknown functions in infinite-dimensional spaces, requires
extensive computational effort and the underlying statistical
construction is usually much more involved than the parametric
counterpart.

To the best of our knowledge, the first attempt in nonparameteric
estimation of Hawkes process is by Gusto and Schbath [42] in 2005.
The authors express the kernel function of the multivariate Hawkes
process, using B-splines [43] with equally spaced knots. The log-
likelihood function involving the basis coefficients is then maximized
\n\n=== PAGE 226 ===\nnumerically and the optimal order for the B-splines basis, as well as
number of knots, is determined using AIC criteria [44].
Instead of B-splines, Reynaud-Bouret and Schbath [45] find the
function within the space of piecewise constant functions that
minimizes the empirical L2-norm between the true and estimated
kernel functions. The method is later extended to multivariate cases
[46] with a Lasso-type penalty [47] in the minimization objective.
Instead of EM, Zhou et al. [48] use Minorize-Maximization (MM)
algorithm [49], in which EM is a special case. In the E-step of MM
algorithm, Q(•|θ(k)) is any lower bound of the objective function
log (L(•)) such that Q(θ(k)|θ(k)) = log (L(θ(k))). It is then iteratively
maximized in the M-steps until convergence. In [48], the kernel
functions are expressed using a finite number of basis functions that
are estimated nonparametrically in M-step by solving the Euler–
Lagrange equation.
Another approach is to use moment matching to find the kernel
function as in [50–52). Bacry and Muzy [52] derive the conditional
moment density 
 of multivariate marked
Hawkes process as the solution of Wiener–Hopf equation [53]
involving μi, γij(t), gij(y) for the case that the mark impact kernel is
piecewise constant. The conditional moment density can be
estimated by any kernel density estimation technique and the
Wiener–Hopf equation can be solved numerically via the Nyström
method [54].
7.4.3 HYPOTHESIS TESTING
7.4.3.1 Random time change
The classical method to test the goodness of fit of a point process
model on 
 is Ogata’s residual analysis [14]. Ogata calls
 the residual process15 and according to the random
time change theorem (see Appendix 7.A), the residual process should
be close to a standard Poisson process if the estimated intensity 
is close to the true intensity λ(t). The hypothesis that 
 is a
standard Poisson process can be tested by the following methods:
\n\n=== OCR PAGE 226 ===\nnumerically and the optimal order for the B-splines basis, as well as
number of knots, is determined using AIC criteria [44].

Instead of B-splines, Reynaud-Bouret and Schbath [45] find the
function within the space of piecewise constant functions that
minimizes the empirical L?-norm between the true and estimated
kernel functions. The method is later extended to multivariate cases
[46] with a Lasso-type penalty [47] in the minimization objective.

Instead of EM, Zhou et al. [48] use Minorize-Maximization (MM)
algorithm [49], in which EM is a special case. In the E-step of MM
algorithm, Q(+|6) is any lower bound of the objective function
log (L(+)) such that Q(0|6) = log (L(0)). It is then iteratively
maximized in the M-steps until convergence. In [48], the kernel
functions are expressed using a finite number of basis functions that
are estimated nonparametrically in M-step by solving the Euler—
Lagrange equation.

Another approach is to use moment matching to find the kernel
function as in [50-52). Bacry and Muzy [52] derive the conditional
moment density E(AN;(t)|dN(O) = 1, 4Y) of multivariate marked
Hawkes process as the solution of Wiener—Hopf equation [53]
involving 1;, yg, 9y(Y) for the case that the mark impact kernel is
piecewise constant. The conditional moment density can be
estimated by any kernel density estimation technique and the
Wiener—Hopf equation can be solved numerically via the Nystrom
method [54].

7.4.3 HYPOTHESIS TESTING

7.4.3.1 Random time change

The classical method to test the goodness of fit of a point process
model on ® is Ogata’s residual analysis [14]. Ogata calls

z ty, 3 . _
ff, = J 0 4(8)45} the residual process?» and according to the random
time change theorem (see Appendix 7.A), the residual process should
be close to a standard Poisson process if the estimated intensity A(t)

is close to the true intensity A(t). The hypothesis that {4,} is a
standard Poisson process can be tested by the following methods:
\n\n=== PAGE 227 ===\n1. QQ Plot [56] of 
 vs Exp(1).
2. Kolmogorov–Smirnov Test [57–59] to test 
 Exp(1).
3. Ljung–Box Test [60] to test the lack of serial correlation of 
.
7.4.3.2 Approximate thinning
Another method to test goodness of fit is by thinning, which does not
require integration of the intensity function. It is useful if the
intensity function is estimated nonparametrically. However, the
thinned residual process is only approximately a Poisson process.
By Ogata’s modified thinning [30], we know that if there exists b > 0
such that b ≤ λ(t)∀t and we keep point tn with probability b/λ(tn), the
thinned point process is a homogeneous Poisson process with rate b.
However, the infimum b of the intensity function is often close to 0,
making the number of points in the thinned process very small and
the test to have little power. A remedy is to use approximate thinning
[61] as follows: choose an integer k ≪ N and select one point from
{t1, .., tN} with probability of selecting tn proportional to λ(tn)− 1.
Repeat the selection (without replacement) until k points are
selected. The resulting k points will be approximately a
homogeneous Poisson process.
7.5 Applications of Hawkes processes
After the groundwork of basic theory and statistical inference for
Hawkes processes, we now unleash their power to model HF data.
First, the readers are reminded how diverse the notion of stock
trading frequency can be. According to the Trade And Quote
database, between 9:30 a.m. and 4:00 p.m. on May 2, 2014, there
were 11 million quote changes (limit + cancellation + market orders)
and 0.3 million trades (market orders) for SPDR S&P 500 ETF
(SPY). In other words, on average, there are 460 quote changes and
13 trades per second. If we take a snapshot every 5 min as in [4, 5],
we will use only 0.03% of trade data and 0.0007% of quote data. In
comparison, Pathfinder Bancorp has only 306 quote changes and 11
trades on the the same day, which means that there is a 35-min lag
between trades on average and thus the 5-min snapshots will just
\n\n=== OCR PAGE 227 ===\n1. QQ Plot [56] of {@n = t, — tn-1} vs Exp().
2. Kolmogorov—Smirnov Test [57-59] to test T,~ Exp(1).
3. Ljung—Box Test [60] to test the lack of serial correlation of {7}.

7.4.3.2 Approximate thinning

Another method to test goodness of fit is by thinning, which does not
require integration of the intensity function. It is useful if the
intensity function is estimated nonparametrically. However, the
thinned residual process is only approximately a Poisson process.

By Ogata’s modified thinning [30], we know that if there exists b > 0
such that b < A(t)vt and we keep point t, with probability b/A(t,), the
thinned point process is a homogeneous Poisson process with rate b.
However, the infimum b of the intensity function is often close to 0,
making the number of points in the thinned process very small and
the test to have little power. A remedy is to use approximate thinning
[61] as follows: choose an integer k < N and select one point from
{t,, .., ty} with probability of selecting t,, proportional to A(t,)~ *.
Repeat the selection (without replacement) until k points are
selected. The resulting k points will be approximately a
homogeneous Poisson process.

7.5 Applications of Hawkes processes

After the groundwork of basic theory and statistical inference for
Hawkes processes, we now unleash their power to model HF data.
First, the readers are reminded how diverse the notion of stock
trading frequency can be. According to the Trade And Quote
database, between 9:30 a.m. and 4:00 p.m. on May 2, 2014, there
were 11 million quote changes (limit + cancellation + market orders)
and 0.3 million trades (market orders) for SPDR S&P 500 ETF
(SPY). In other words, on average, there are 460 quote changes and
13 trades per second. If we take a snapshot every 5 min as in [4, 5],
we will use only 0.03% of trade data and 0.0007% of quote data. In
comparison, Pathfinder Bancorp has only 306 quote changes and 11
trades on the the same day, which means that there is a 35-min lag
between trades on average and thus the 5-min snapshots will just
\n\n=== PAGE 228 ===\ngive a series of repeated information. Regardless of the sampling
frequency, we are likely to get some misleading result if we analyze
the asynchronous data from a portfolio of liquid and illiquid stocks
using models with fixed intervals.
The construction of multivariate point processes shows that each
variate can have a completely different arrival intensity λi(t).
Nonetheless, the multivariate Hawkes process can still model the
dependence structure easily via the γij(t)’s, which are estimated by
duly considering all the asynchronous data in the highest frequency
without any resampling.
Order arrivals and price changes are unarguably two of the most
important elements in HF trading. Using Hawkes processes, we can
estimate their conditional distributions based on all the historical HF
asynchronous data, enabling us to give a more accurate real-time
prediction of future event occurrences. In the following subsections,
we are going to highlight some of the literature that takes advantage
of Hawkes processes to model HF data.
7.5.1 MODELING ORDER ARRIVALS
Bowsher [22]16 is the first to use Hawkes processes to model order
arrivals. He uses nonlinear Hawkes processes to allow for inhibitory
effect and he considers two rate functions Φi(x, t) = μi(t) + exp (x)
and Φi(x, t) = max (μi(t) + x, ϵi), ϵi > 0, where both of them guarantee
that the stochastic intensity will be strictly positive at all times. For
the deterministic base rate μi(t), he exploits a piecewise linear
function with knots at 9:30, 10:00, 11:00, … ,16:00 while the decay
kernel is the exponential function without marks. In addition, an
extra term is included to represent the spillover effects from the
previous trading day.
Bowsher uses MLE to estimate the parameters for the bivariate point
process of trade and quote of General Motor, trading on NYSE
between July 5, 2000, and August 29, 2000. The model is found to
be decent according to the goodness-of-fit test using random time
change.
\n\n=== PAGE 229 ===\nInstead of modeling arrivals of all trades and quotes, Large [62] uses
Hawkes processes to model only the arrivals of aggressive orders,
which are market orders depleting the queue and limit orders falling
inside the bid-ask spread, in order to study the resiliency of the LOB.
An LOB is called resilient if it reverts to its generic shape promptly
after large trades. The idea is that when a large trade causes the bid-
ask spread to widen, the arrival intensity of aggressive limit orders in
a resilient LOB will surge so that the gap will be filled very quickly. In
order words, the cross-excitation effect γij(t) from aggressive market
orders to aggressive limit orders should be reasonably large in a
resilient LOB.
In addition to market orders and limit orders, Large also includes the
cancellations of limit orders as well as limit orders falling outside the
best quotes. Therefore, he builds a 10-variate linear marked Hawkes
process with exponential decay and mark impact kernel to fit the HF
data of Barclays (BARC), trading on LSE between January 2 and 31,
2002. The result shows that the widening of bid-ask spread indeed
pumps up the intensities of aggressive limit orders, causing the gap
to be filled very quickly and hence making the LOB resilient.
More examples of applications of Hawkes processes to order arrivals
include the following papers: Muni Toke and Pomponio [63] use a
similar approach as Large [62] to model trades-through, namely
market orders that deplete the best queues and consume at least one
share in the second best. Muni Toke [64] designs a more realistic
market simulator, using Hawkes processes with exponential kernel
for order arrivals. Hewlett [65] models the arrival of market orders
with Hawkes processes for single period market making. Finally,
Alfonsi and Blanc [66], Jaisson [67] tackle the problem of optimal
execution with market orders following multivariate Hawkes
processes.
7.5.2 MODELING PRICE JUMPS
7.5.2.1 Single asset
Traditionally, the events of price jumps are modeled by Poisson
processes, which suffer from the drawbacks mentioned in the
Introduction section. Again, Hawkes processes can be applied to
\n\n=== PAGE 230 ===\n(7.29)
(7.30)
(7.31)
model price jumps, which often delineate clustering, self- and cross-
excitation behaviors.
Bacry et al. [68] use Hawkes processes to model the price jumps,
resulting in a model that can reproduce the microstructure noise
[69], Epps effect [70], and jump clustering, while maintaining the
coarse scale limit of Brownian diffusion. In their model, the trade
price X(t) has the dynamics
where N(t) = (N1(t), N2(t)) is a bivariate linear Hawkes process with
exponential decay kernel. N1(t), N2(t) represents the total number of
upward and downward jumps, respectively. The authors make
additional assumptions that the Hawkes process N has only cross-
excitation and coefficients are symmetric to simplify computation.
According to the model, when X jumps up (down), λ2 (resp. λ1)
increases, causing the probability of jumping down (resp. up) to
increase. Such a cross-linkage generates the effect of microstructure
noise where the trade price is bouncing between best bid and best
ask.
Because of the bid-ask bounce, it is well-known that the realized
variance (annualized) increases when the sampling frequency
increases [71].
\n\n=== OCR PAGE 230 ===\nmodel price jumps, which often delineate clustering, self- and cross-
excitation behaviors.

Bacry et al. [68] use Hawkes processes to model the price jumps,
resulting in a model that can reproduce the microstructure noise
[69], Epps effect [70], and jump clustering, while maintaining the
coarse scale limit of Brownian diffusion. In their model, the trade
price X(t) has the dynamics

X(t) = N,(t) — N3(t) (7.29)

where N(t) = (N, (0), N,(t)) is a bivariate linear Hawkes process with
exponential decay kernel. N,(z), N,(¢) represents the total number of

upward and downward jumps, respectively. The authors make
additional assumptions that the Hawkes process N has only cross-
excitation and coefficients are symmetric to simplify computation.

ain= nr [ Y(t—s)\dN(t),  Ag(t) = (7.30)
J (0,t)

+ | y(t—s)dN,(), y(t) = ae
(0,t)

According to the model, when X jumps up (down), A, (resp. A,)
increases, causing the probability of jumping down (resp. up) to
increase. Such a cross-linkage generates the effect of microstructure
noise where the trade price is bouncing between best bid and best
ask.

Because of the bid-ask bounce, it is well-known that the realized
variance (annualized) increases when the sampling frequency
increases [71].

T/t
Vi) =E (3 Yixn + Vz) - xin)

n=0

(7.31)
\n\n=== PAGE 231 ===\n(7.32)
Such an effect can be easily demonstrated by computing the expected
realized variance (7.32) of the jump model (7.29) and the result with
μ = 0.16, α = 0.024, β = 0.11 as shown in Figure 7.1. The authors
apply the model to Euro-Bund futures and find a very good fit
between the observed and theoretical realized variances under this
highly simplified model.
FIGURE 7.1 Volatility signature plot of hawkes jump model.
Let Y(t) = X(nt), then Y(t) is a coarse scale version of X(t). For
example, if t in X is in microsecond and n = 60, 000, then t in Y will
be in minute. When we look at the trade price in a low-frequency
setting, Bacry et al. [24] show that the macroscopic Hawkes jump
model goes back to the classical model of Brownian motion due to
\n\n=== OCR PAGE 231 ===\n_ 2k ( 1 +(1- 1 ys) (7.32)
1—a/p \(+a/py (l+a/py/ (a+f)r

Such an effect can be easily demonstrated by computing the expected
realized variance (7.32) of the jump model (7.29) and the result with
Lt = 0.16, a = 0.024, B = 0.11 as shown in Figure 7.1. The authors
apply the model to Euro-Bund futures and find a very good fit
between the observed and theoretical realized variances under this
highly simplified model.

. 04
>
Ee
6
>
% 0.35
S
"I
2
7
i)
19)
& 03
aa)
0 50 100 150 200

Sampling interval Tt
FIGURE 7.1 Volatility signature plot of hawkes jump model.

Let Y(O = X(nt), then Y(t) is a coarse scale version of X(t). For
example, if t in X is in microsecond and n = 60, 000, then t in Y will
be in minute. When we look at the trade price in a low-frequency
setting, Bacry et al. [24] show that the macroscopic Hawkes jump
model goes back to the classical model of Brownian motion due to
\n\n=== PAGE 232 ===\n(7.33)
(7.34)
(7.35)
(7.36)
(7.37)
(7.38)
the functional central limit theorem for linear Hawkes process (7.7).
Assuming that ∫∞
0γ(t)dt < 1, then
It is interesting to see how the macroscopic variance σ2 is related to
the microscopic base rate μ and cross-excitation γ(t). As ∫∞
0γ(t)dt
approaches 1, the variance goes to ∞.
Jaisson and Rosenbaum [25] extend the model of Bacry et al. [68] to
the case of nearly unstable Hawkes process, where ∫∞
0γ(t)dt ≃ 1, by
constructing a sequence of kernel functions whose integrals converge
to 1 at the speed of n− 1. They show that the properly scaled price
process converges to Brownian diffusion with Heston stochastic
volatility [72]. The full result is stated below.
\n\n=== OCR PAGE 232 ===\nthe functional central limit theorem for linear Hawkes process (7.7).
Assuming that f®,y(t)dt < 1, then

X(ne) weak oW), 02 = - 2u — (7.33)
vn = ff yd + fo? y(odty?

It is interesting to see how the macroscopic variance o? is related to
the microscopic base rate 1 and cross-excitation y(t). As f°,y(t)dt
approaches 1, the variance goes to ©.

Jaisson and Rosenbaum [25] extend the model of Bacry et al. [68] to
the case of nearly unstable Hawkes process, where f*,y(t)dt = 1, by
constructing a sequence of kernel functions whose integrals converge
to 1 at the speed of n- '. They show that the properly scaled price
process converges to Brownian diffusion with Heston stochastic
volatility [72]. The full result is stated below.

XW) = MPH) — NW (7.34)

Aro=us [ Pe- oan [ y= s)dN3"(s) (7-35)

onus ff 10 oan [ y(t s\dN!(s) (7.36)

(/ Modes | 1S) <1, V(t) = ay, (t) (7.37)
0 0

y2R,—R,, (7.38)
/ (1, (t) + yo(t))dt = 1, [ t(y,(t) + y2(t))dt = m <
0 0
\n\n=== PAGE 233 ===\n(7.39)
(7.40)
(7.41)
(7.42)
(7.43)
(7.44)
Under the conditions of (7.34–7.41),
Conditions (7.34–7.36) are just a bivariate Hawkes model (with both
self- and cross-excitation) but now we have a different γ(n)
i(t) for
each n that is used to scale the time. The rest are the regularity
conditions similar to the univariate nearly unstable Hawkes process
(7.17) and the most important one is (7.40), which states that α(n)
converges to 1 at the speed of n− 1. However, the interesting result is
that instead of converging to an integrated CIR, the price dynamics
formed by the difference between two Hawkes processes converges
to a stochastic volatility model.
7.5.2.2 Two assets
To model the Epps effect, Bacry et al. [68] consider the two-asset
case with prices (X1(t), X2(t)) given by
\n\n=== OCR PAGE 233 ===\n/ ly{(|dt < co, — sup |y/(t)| < o%
0

tE[0,c0)

a” €f0.1), lima” =1, limn(l-a”)=c>0
noo

noo

— @k ny (nt)
yon = »Y (” + i”) (), pt) = yw

k=l “ I wrn(tydt’

|p (t)| < M Vn Vt
Under the conditions of (7.34-7.41),

X (ne) weak
————.

n noo

Y(e)

V,
= ——_—aw}, %=0
1=f.° In@ - y2(0ldt

t

dV, = dt+—dW?, V,=0
m

m

c (% v) VV,
—_ — — "t
c

(7.39)

(7.40),

(7.41)

(7.42)

(7.43)

(7.44)

Conditions (7.34—7.36) are just a bivariate Hawkes model (with both
self- and cross-excitation) but now we have a different yw for

each n that is used to scale the time. The rest are the regularity

conditions similar to the univariate nearly unstable Hawkes process
(7.17) and the most important one is (7.40), which states that a™
converges to 1 at the speed of n- *. However, the interesting result is
that instead of converging to an integrated CIR, the price dynamics
formed by the difference between two Hawkes processes converges

to a stochastic volatility model.

7.5.2.2 Two assets

To model the Epps effect, Bacry et al. [68] consider the two-asset

case with prices (X,(t), X,(t)) given by
\n\n=== PAGE 234 ===\n(7.45)
(7.46)
(7.47)
(7.48)
(7.49)
(N1(t), …, N4(t)) is a 4-variate Hawkes process with exponential
kernel, where βij = β. The coupling of excitation effects is constrained
to have the form
In this case, there is a closed form representation for the realized
correlation, which vanishes when the sampling interval goes to 0
(Epps effect).
If we assume μ1 = μ2, μ3 = μ4, α12 = α34 = 0, (∫∞
0γ13(t)dt)(∫∞
0γ31(t)dt)
< 1, then the macroscopic bivariate asset prices converges to
correlated Brownian diffusion [24]
This convergence result gives us an explicit formula to estimate the
macroscopic correlation from the asynchronous HF data.
\n\n=== OCR PAGE 234 ===\nX(t) = N,(t)-—N3(t), X(t) = N3(t) -— Ny() (7.45)

- ; (7.46)
A(t) = mu; + > / a, exp(—A(t — s))dNj(s), t= 1,....4
jal 4 (0.t) .

(N,(), ..., N,(O) is a 4-variate Hawkes process with exponential
kernel, where B,; = B. The coupling of excitation effects is constrained
to have the form
0 a a, 0 (7.47)
a O 0 a,
a;, O OO ayy
a3, O34 0

a=

In this case, there is a closed form representation for the realized
correlation, which vanishes when the sampling interval goes to 0
(Epps effect).

If we assume Hy = Hos Mg = Hy Ayo = Agy = O, (J oVig®d(SoVsi dt)
< 1, then the macroscopic bivariate asset prices converges to
correlated Brownian diffusion [24]

(7.48)

7
va( VW) + Vv or moot)

1 ) = VV Jo as (DatW, (+) + ¥/VW2(*)
Yin \Xalne) J nose =o” nado” vsi(Odt)°/?

y=", + (/ rs(oat) H3, Vz = 3 + (/ rant) My (7-49)
0 0

(W,, W,) is standard 2-dimensional Brownian motion

This convergence result gives us an explicit formula to estimate the
macroscopic correlation from the asynchronous HF data.
\n\n=== PAGE 235 ===\n(7.50)
(7.51)
(7.52)
(7.53)
(7.54)
As a final remark, under this jump representation, the observed
trade price is not some hidden continuous fair value process plus
some microstructure noise as in [73]. It is the result of the trading
interactions between buyers (N1, N3) and sellers (N2, N4) on a fixed
price grid. There is no such thing as HF volatility or correlation since
prices are not diffusions but pure jump processes in the HF scale.
Volatility and correlation are only meaningful when we look at the
coarse scale diffusion approximation, but those low-frequency
representation parameters can be computed directly from the HF
jump model characteristics.
7.5.3 MODELING JUMP-DIFFUSION
Duffie et al. [74, 75] propose the affine jump-diffusion X(t), which
has the following structure.17
The jump intensity λ(t) of N(t) is an affine function of X(t), which
depends on the Brownian motion W(t) and the jump process N(t),
with jump size ζ drawn from a fixed distribution. When k0 = βθ, k1 =
−β, h0 = h1 = 0, a0 = 0, a1 = 1, ζ = α, we can see that λ(t) = X(t) and
dλ(t) = β(θ − λ(t))dt + αdN(t). Hence in this case, N(t) is the Hawkes
process with exponential kernel.
Zhu [76] derives some convergence results when ζ is a constant and
the diffusion part is a CIR process.
Aït-Sahalia et al. [77] model the contagion of financial crisis with the
Hawkes jump-diffusion where the price dynamic Xi(t) is given by
\n\n=== OCR PAGE 235 ===\nAs a final remark, under this jump representation, the observed
trade price is not some hidden continuous fair value process plus
some microstructure noise as in [73]. It is the result of the trading
interactions between buyers (N,, N,) and sellers (N,, N,) on a fixed
price grid. There is no such thing as HF volatility or correlation since
prices are not diffusions but pure jump processes in the HF scale.
Volatility and correlation are only meaningful when we look at the
coarse scale diffusion approximation, but those low-frequency
representation parameters can be computed directly from the HF
jump model characteristics.

7.5.3 MODELING JUMP-DIFFUSION
Duffie et al. [74, 75] propose the affine jump-diffusion X(t), which
has the following structure.*7

dX(t) = (ko(t) + ky ()X(O)dt + (ho(t) + hy ()X())dW(t) + CdN(t) (7.50)

A(t) = ag(t) + ay (t)X(t) (7.51)

The jump intensity A(t) of N(f) is an affine function of X(t), which
depends on the Brownian motion W(t) and the jump process N(t),
with jump size ¢ drawn from a fixed distribution. When k, = B80, k, =
-B, hy = h, = 0, ay = 0, a, = 1, = a, we can see that A(t) = X(t) and
dX(t) = B(8 - A(t) dt + adN(t). Hence in this case, N(t) is the Hawkes
process with exponential kernel.

Zhu [76] derives some convergence results when Z is a constant and
the diffusion part is a CIR process.

dX(t) = B(u — X(t))dt + o-/X(t)dW(t) + adN(t) (7.52)

A(t) = dg + a, X(t) (7.53)

Ait-Sahalia et al. [77] model the contagion of financial crisis with the
Hawkes jump-diffusion where the price dynamic X,(¢) is given by

dX;(t) = pdt + V/V()dW2 (t) + Z()dN;(t) (7.54)
\n\n=== PAGE 236 ===\n(7.55)
(7.56)
(7.57)
The diffusion part is the Heston stochastic volatility model and the
jump part is a multivariate Hawkes process modeling the clustering
and propagation of jumps among multiple assets. Zi(t) corresponds
to the jump size and direction.
7.5.4 MEASURING ENDOGENEITY (REFLEXIVITY)
In terms of the Hawkes branching structure representation of events
arrivals, Filimonov et al. [78, 79] portray immigrants as exogenous
news whereas the descendants are endogenous incidents. In the
context of price movements in the stock market, immigrants are the
price discovery due to orders from informed traders, who react to
external information, whereas the descendants are the destabilizing
ripples created by noise traders, who engage in herding [80],
momentum trading [81], parasite trading [82], etc.
Under the univariate linear Hawkes model with exponential decay
kernel and constant base rate, the expected number of direct
descendants per individual (branching coefficient) is given by
For a given immigrant, the expected number of descendants in all
generations is n + n2 + n3 + ⋅⋅⋅ = n/(1 − n) if n < 1, so the ratio of
descendants (nonimmigrants) versus total population is
Therefore, the branching coefficient n characterizes the amount of
endogenous feedback activities while the base rate μ measures the
arrival rate of exogenous information.
Using E-mini S&P futures as proxy, Filimonov and Sornette [78] find
that the level of endogeneity (reflexivity18) n in the US market has
gone from 0.3 in 1998 to 0.7 in 2007. Moreover, in the flash crash of
May 6, 2010, n reached a peak of 0.95.
\n\n=== OCR PAGE 236 ===\ndV,(t) = K,(0; — V,(t))dt + nV (DdWy (2) (7.55)

The diffusion part is the Heston stochastic volatility model and the
jump part is a multivariate Hawkes process modeling the clustering
and propagation of jumps among multiple assets. Z,(t) corresponds
to the jump size and direction.

7.5.4 MEASURING ENDOGENEITY (REFLEXIVITY)

In terms of the Hawkes branching structure representation of events
arrivals, Filimonov et al. [78, 79] portray immigrants as exogenous
news whereas the descendants are endogenous incidents. In the
context of price movements in the stock market, immigrants are the
price discovery due to orders from informed traders, who react to
external information, whereas the descendants are the destabilizing
ripples created by noise traders, who engage in herding [80],
momentum trading [81], parasite trading [82], etc.

Under the univariate linear Hawkes model with exponential decay
kernel and constant base rate, the expected number of direct
descendants per individual (branching coefficient) is given by

co ce
n= / y(s)ds = [ ae*ds = a/p (7-56)
0 0
For a given immigrant, the expected number of descendants in all
generations isn +n? +n34+-+:=n/(1-1n) ifn <1, so the ratio of

descendants (nonimmigrants) versus total population is

Descendants n/(1—n) h (7.57)
Descendants + immigrant n/(1—n)+1

Therefore, the branching coefficient n characterizes the amount of
endogenous feedback activities while the base rate 1, measures the
arrival rate of exogenous information.

Using E-mini S&P futures as proxy, Filimonov and Sornette [78] find
that the level of endogeneity (reflexivity!8) n in the US market has
gone from 0.3 in 1998 to 0.7 in 2007. Moreover, in the flash crash of
May 6, 2010, n reached a peak of 0.95.
\n\n=== PAGE 237 ===\nNonetheless, using the power law decay kernel, Hardiman et al. [84]
challenge the result of Filimonov and Sornette [78] by reporting that
the branching ratio n has always been close to 1 since 1998 and that
the market could be a critical Hawkes process [23], but Filimonov
and Sornette [85] refute that the power law kernel is sensitive to
outliers in addition to other counter arguments. Later, Hardiman
and Bouchaud [86] devised a nonparametric estimation of the
branching ratio in terms of moments, but the result depends heavily
on the window size used in the empirical moment computation.
Appendix 7.A: Point Processes
7.A.1 DEFINITION
Let X (state space) be a locally compact Hausdorff second countable
topological space,19 
 be the Borel sets on X and  be the collection
of bounded (relatively compact) Borel sets on X. A Borel measure μ
on (
 is called locally finite if 
. Let 
20 be
the set of (positive) locally finite Borel counting (integer-valued)
measure on 
 and 
21 be the σ-algebra of 
 generated
by the set of evaluation functionals 
, where
ΦB(μ) = μ(B) and 
.
A point process N on X is defined as a measurable mapping from a
probability space 
 to 
; thus, a point process is
formally a measure-valued random element. However, for any point
process N, there exist random variables
 such that
, where δx is the Dirac measure 
[8, p. 20]. If we think of bi as the number of points at xi, we can see
that the point process N is indeed the random counting measure
showing the total number of points in any given region and this
matches our intuition that a point process is a random set of points
{xi} on X.
The point process N is called simple if 
22;
that is, each location has at most one point. In this case, N(•) = ∑n
i =
\n\n=== OCR PAGE 237 ===\nNonetheless, using the power law decay kernel, Hardiman et al. [84]
challenge the result of Filimonov and Sornette [78] by reporting that
the branching ratio n has always been close to 1 since 1998 and that
the market could be a critical Hawkes process [23], but Filimonov
and Sornette [85] refute that the power law kernel is sensitive to
outliers in addition to other counter arguments. Later, Hardiman
and Bouchaud [86] devised a nonparametric estimation of the
branching ratio in terms of moments, but the result depends heavily
on the window size used in the empirical moment computation.

Appendix 7.A: Point Processes
7.A.1 DEFINITION

Let X (state space) be a locally compact Hausdorff second countable
topological space,° 3x be the Borel sets on X and B be the collection
of bounded (relatively compact) Borel sets on X. A Borel measure
on (X, By) is called locally finite if #(B) < co VB € B, Let 2(X)2° be
the set of (positive) locally finite Borel counting (integer-valued)
measure on (X, By) and NV (X)2! be the o-algebra of 2t(X) generated
by the set of evaluation functionals {Pp : W(X) — N| BE B), where
p(w) = u(B) and N = (0, 1, 2, ...}.

A point process N on X is defined as a measurable mapping from a
probability space (@, F, P) to (2(X), N’(X)); thus, a point process is
formally a measure-valued random element. However, for any point
process N, there exist random variables

bE Z, = {1,2,..}, x,EX,nEZ,=Z,U{} ouch that

NO) = Yi-1 5:5), where 8, is the Dirac measure (6.(A) = 1(x € A))
[8, p. 20]. If we think of b; as the number of points at x,, we can see
that the point process N is indeed the random counting measure
showing the total number of points in any given region and this

matches our intuition that a point process is a random set of points
{x,} on X.

The point process N is called simple if P(N({x}) > 1) = 0 Vx € X22;
that is, each location has at most one point. In this case, N(«) = >"; -
\n\n=== PAGE 238 ===\n(7.58)
(7.59)
(7.60)
1δxi(•).
Suppose that X is also a topological vector space (e.g., 
), the shift
operator 
 is defined as St(A) = A + t = {(s + t) ∈ X|s ∈
A}. A point process N is called stationary if the shifted process
N○St
23 has the same distribution as N ∀t ∈ X.
7.A.2 MOMENTS
Let 
, the kth moment measure24 
 of a point
process N is defined as
The first moment measure is also called mean (intensity) measure
and denoted as M(•). The covariance measure is defined as
The second and higher moment measures have concentration along
diagonals, so we also have the kth factorial moment measure.
The name factorial comes from the fact that
. Obviously, M(A)
= M(1)(A) and for k = 2, we have M2(A1, A2) = M(2)(A1, A2) +
M(A1∩A2).
If 
 and N is stationary, it can be shown that M(A) = λ|A|,
where 
 is Lebesgue measure. That implies the
mean measure M of a stationary point process is absolutely
continuous with respect to Lebesgue measure with constant density
M((0, 1]m). If the covariance factorial moment measure C(2) is also
\n\n=== OCR PAGE 238 ===\n15,,(¢).

Suppose that X is also a topological vector space (e.g., R’”), the shift
operator S, : By — By is defined as S{A) =A+t={(s+D € X|s e€
A}. A point process N is called stationary if the shifted process
NoS,?* has the same distribution as N vt € X.

7.A.2 MOMENTS
k . R@k
Let k © Z,, the k* moment measure*4 *: BY — 10, ©] ofa point
process N is defined as
(7.58)

M*(Aj,...,A,) = E(N(A)).--N(A,)) = (2 YD Goce Ar X= «4,)

The first moment measure is also called mean (intensity) measure
and denoted as M(e). The covariance measure is defined as

C?(A,, Ax) = Cov(N(A,), N(A3)) = M?(Ay,A) — M(A,)M(A3) (7.59)

The second and higher moment measures have concentration along
diagonals, so we also have the k"” factorial moment measure.

(7.60)
Ma A= Y  DAraonat 40)

xy Fo A

The name factorial comes from the fact that

M(A, ...,A) = E(N(A)(N(A) — 1)...(N(A) — k + D), Obviously, M(A)
= M((A) and for k = 2, we have M?(A,, A.) = M®)(A,, A.) +
M(A,NA,).

If X = R” and Nis stationary, it can be shown that M(A) = A\A|,
where 4 = M((O, 1]") and | + | is Lebesgue measure. That implies the
mean measure M of a stationary point process is absolutely
continuous with respect to Lebesgue measure with constant density
M((0, 1]”). If the covariance factorial moment measure C® is also
\n\n=== PAGE 239 ===\n(7.61)
absolutely continuous, we denote its density function as c(2)(x, y).
Since N is stationary, c(2)(x, y) = c(2)(y − x) and c(2)(•) is called
reduced covariance density. The covariance measure C2 is usually not
absolutely continuous but for simple point process N on 
, the
quantity below is still called (reduced) covariance density and is
useful in estimation:
7.A.3 MARKED POINT PROCESSES
When an event happens, it may carry an additional information
(mark). For instance, each order arrival is associated with an order
quantity (volume) and each earthquake is reported with a
magnitude. A point process with marks is called marked point
process.
Let Y (mark space) be a locally compact Hausdorff second countable
space, 
 be a measurable space and ν (mark distribution) be a
probability measure on 
. A marked point process (MPP) N is a
measurable mapping 
 such that the
ground measure Ng(•) = N(• × Y) is a point process (i.e., locally
finite).25 Hence a marked point process is nothing but a point
process on a product space, but usually we treat the location x and
mark y differently and we have a few more definitions.
N is called a multivariate point process if Y = {1, …, d}. In this case,
Ni(•) = N(• × {i}) is called the marginal process of type i points. An
MPP N is called simple if Ng is simple.26 The marks of an MPP are
called unpredictable if yn is independent of {(xi, yi)}i < n and they are
called independent if yn is independent of {(xi, yi)}i ≠ n.27
\n\n=== OCR PAGE 239 ===\nabsolutely continuous, we denote its density function as c®)(x, y).
Since Nis stationary, c®(x, y) = c2(y — x) and c®)() is called
reduced covariance density. The covariance measure C? is usually not
absolutely continuous but for simple point process N on Ry, the
quantity below is still called (reduced) covariance density and is
useful in estimation:

(de) = E(N(x + de)N(a)) [dee — 0? = A6(dx) +. (de) (7.61)

(/ d(x)dx = ')

7.4.3 MARKED POINT PROCESSES

When an event happens, it may carry an additional information
(mark). For instance, each order arrival is associated with an order
quantity (volume) and each earthquake is reported with a
magnitude. A point process with marks is called marked point
process.

Let Y (mark space) be a locally compact Hausdorff second countable
space, (Y, By) be a measurable space and v (mark distribution) be a
probability measure on (Y; By), A marked point process (MPP) Nisa
measurable mapping N : 2 — (2(X x Y), N(X x Y)) such that the
ground measure N,(+) = N(+ x Y) is a point process (i.e., locally

finite).25 Hence a marked point process is nothing but a point
process on a product space, but usually we treat the location x and
mark y differently and we have a few more definitions.

Nis called a multivariate point process if Y = {1, ..., d}. In this case,
N,(+) = N(+ x {i} is called the marginal process of type 7 points. An

MPP Nis called simple if Nj is simple.2° The marks of an MPP are
called unpredictable if y,, is independent of {(x;, y;)}; < , and they are
called independent if y,, is independent of {(x;, yj)}; + n-27
\n\n=== PAGE 240 ===\n(7.62)
7.A.4 STOCHASTIC INTENSITY
In this section, 
28 and Nt = N((0, t]). Let 
 be a
filtered complete probability space. A stochastic process
 is called -predictable if it is measurable with
respect to the predictable σ-algebra
. If Zt is adapted and left-
continuous, then Zt is predictable [10, p. 9]. In practice, all the
predictable processes we use are in this category. Also, if Zt is
predictable, then 
; in other words, the value of the
predictable process Zt at time t is “known” just before time t.
We assume that the filtration 
 satisfies the usual condition
(complete and right-continuous) and {Nt} is adapted and simple. A
stochastic process 
 is called a -compensator of a
point process N if At is increasing, right-continuous, -predictable,
A0 = 0 a.s. and (Nt − At) is a -local martingale. If At = ∫t
0λsds a.s., λt
is nonnegative and -predictable, then λt is called the stochastic or
conditional -intensity of N.293031 A defining properties of λt is that
When s → t, this becomes 
. We can see that the
stochastic intensity λt is the instantaneous rate of arrival conditioned
on all information just before time t. For a multivariate point
process, λi(t) is the intensity of the marginal process Ni(t).
From the definition, we notice that intensity exists if and only if the
compensator is absolutely continuous. In fact, the compensator of a
point process can be expressed in terms of the conditional
interarrival time 
 if the conditional distribution has
support over 
. Under this condition, the intensity exists if and only
if the conditional interarrival time is absolutely continuous. In this
case, the intensity is given by [9, p. 70]
\n\n=== OCR PAGE 240 ===\n7.4.4 STOCHASTIC INTENSITY

In this section, ¥ = 8428 and N, = N((0, t]). Let Q: F; F;,P) bea
filtered complete probability space. A stochastic process

Z:R, XQ — Ris called F-predictable if it is measurable with
respect to the predictable o-algebra

P =o({(s,t]XAl0 <5 <1t,A € F.}) 1fZ, is adapted and left-
continuous, then Z, is predictable [10, p. 9]. In practice, all the
predictable processes we use are in this category. Also, if Z; is
predictable, then Z, © F-; in other words, the value of the
predictable process Z;, at time t is “known” just before time t.

We assume that the filtration {7;} satisfies the usual condition
(complete and right-continuous) and {N;} is adapted and simple. A
stochastic process A:R, XQ — Ry, is called a F-compensator of a
point process N if A; is increasing, right-continuous, F-predictable,
Ao = 0 a.s. and (N; — A;) is a F-local martingale. If A; = J‘pA,ds a.s., A
is nonnegative and F-predictable, then A, is called the stochastic or
conditional F-intensity of N.293°31 A defining properties of ), is that

t
6
E (/ i,dulF =E(N,-N,|F) as. Ws<t (7-62)

When s — t, this becomes 4,4¢ = E(V(dt)|F,-), We can see that the
stochastic intensity A, is the instantaneous rate of arrival conditioned
on all information just before time ¢. For a multivariate point
process, A,(¢) is the intensity of the marginal process N,(t).

From the definition, we notice that intensity exists if and only if the
compensator is absolutely continuous. In fact, the compensator of a
point process can be expressed in terms of the conditional
interarrival time “« ~ ‘»-1)I*%,., if the conditional distribution has
support over +, Under this condition, the intensity exists if and only
if the conditional interarrival time is absolutely continuous. In this
case, the intensity is given by [9, p. 70]
\n\n=== PAGE 241 ===\n(7.63)
(7.64)
Once we know the intensity, we know the conditional distributions of
all interarrival times and hence the complete distribution of the point
process [11, p. 233].
 is called a compensator of the MPP N if A(•,
B) is a compensator of 
 and A(t, •) is a measure on
. If A(t, B) = ∫t
0∫Bλ(s)ν(s, dy)ds a.s. where λ(t) is
nonnegative and predictable, then λ(t) is called the stochastic
intensity of the MPP N and 
 is called the
conditional mark distribution.
7.A.5 RANDOM TIME CHANGE
If the filtration is usual, a point process N on 
 is simple and
adapted, its intensity λ(t) exists, and ∫∞
0λ(s)ds = ∞ a.s., then
 is a standard Poisson process (rate = 1). The above
theorem is called random time change theorem [88, 89] and is
extremely useful in testing the goodness of fit of a stochastic
intensity model.
Appendix 7.B: A Brief History of Hawkes
processes
Hawkes processes are proposed by Hawkes [13] in 1971 to model
contagious processes like epidemics, neuron firing, and particle
emission, where the occurrences of events trigger further events.
Although the intensity of Cox processes [90], introduced in 1955, are
stochastic, they are determined before the events are unfolded.32 In
order to portray the excitation behavior in contagious processes,
Hawkes extends the model in such a way that the intensity is a
predictable stochastic process with an intuitive autoregressive form,
which allows it to adapt to events that happen over time.
\n\n=== OCR PAGE 241 ===\nAy = Ay(t = ty) if EE yas tl (7.63)

g,,(t)
h,(t) = ——., (t, -t, F,
an( ) 1— G,(t-) (t, nv ty,

~G (7.64)

7 n
Once we know the intensity, we know the conditional distributions of
all interarrival times and hence the complete distribution of the point
process [11, p. 233].

A:R, x ByXQ— Ry is called a compensator of the MPP Nif A(s,
B) is a compensator of N(e x B) VB € By and A(t, ¢) is a measure on
(Y, By) Wee Ry. If A(t, B) = f'oSpa(s)v(s, dy)ds a.s. where A(t) is
nonnegative and predictable, then A(t) is called the stochastic

. . v(t, dy) = Py, € dy|F,-);

intensity of the MPP N and ‘\"" a ‘,’ is called the
conditional mark distribution.

7.A.5 RANDOM TIME CHANGE

If the filtration is usual, a point process N on 8 is simple and
adapted, its intensity A(t) exists, and J*,A(s)ds = ~ a.s., then

~ rt,
{t, = J 0 A(s)dS} ig a standard Poisson process (rate = 1). The above
theorem is called random time change theorem [88, 89] and is
extremely useful in testing the goodness of fit of a stochastic
intensity model.

Appendix 7.B: A Brief History of Hawkes
processes

Hawkes processes are proposed by Hawkes [13] in 1971 to model
contagious processes like epidemics, neuron firing, and particle
emission, where the occurrences of events trigger further events.
Although the intensity of Cox processes [90], introduced in 1955, are
stochastic, they are determined before the events are unfolded.3? In
order to portray the excitation behavior in contagious processes,
Hawkes extends the model in such a way that the intensity is a
predictable stochastic process with an intuitive autoregressive form,
which allows it to adapt to events that happen over time.
\n\n=== PAGE 242 ===\nAfter Hawkes’ seminal paper, there are a number of theoretical
developments including the branching structure representation by
Hawkes and Oakes [18] in 1974, Markov property for intensity with
exponential decay kernel by Oakes [16] in 1975, MLE for Hawkes
processes by Ozaki [29] in 1979, Ogata’s modified thinning
simulation algorithm [30] in 1981, nonlinear Hawkes processes by
Brmaud and Massoulié [20] in 1996, nonparametric estimation by
Gusto and Schbath [42] in 2005, EM for Hawkes processes by Veen
and Schoenberg [33] in 2008, and functional central limit theorem
for Hawkes processes by Bacry et al. [24], Jaisson and Rosenbaum
[25], Zhu [26] in 2013–2015.
Although the first application of Hawkes processes to earthquake
occurrences appeared in 1982 [15], it was not until 1988 [14] that
Hawkes processes received much attention. Since then, the
versatility of Hawkes model is leveraged in seismology [14, 91],
finance (risk and credit default modeling) [92, 93], social networks
[94, 95], neuroscience [96, 97], etc. (see [98, 99] for more
applications).
The use of Hawkes processes in HF financial data modeling starts
with Bowsher [22] in 200733 and then Large [62] in the same year.
Both authors exploit Hawkes processes so as to describe the
interactions among different types of order arrivals. Later, Bacry
et al. [68] and Aït-Sahalia et al. [77] employ Hawkes processes to
reproduce jump clustering in pure jump and jump diffusion
representation of stock prices in 2013 and 2015, respectively. On the
other hand, an interesting idea from Filimonov and Sornette [78]
appears in 2012, which utilizes the branching coefficient of linear
Hawkes model to measure the level of endogenous activities in the
US stock market, although the debate about the validity of the result
is still ongoing [84–86].
References
1. J.-P. Bouchaud, J. D. Farmer, and F. Lillo. How markets slowly
digest changes in supply and demand. Handbook of Financial
Markets: Dynamics and Evolution, 1:57, 2009.
\n\n=== PAGE 243 ===\n2. R. Cont. Statistical modeling of high-frequency financial data.
Signal Processing Magazine, IEEE, 28(5):16–25, 2011.
3. A. Chakraborti, I. M. Toke, M. Patriarca, and F. Abergel.
Econophysics review: I. empirical facts. Quantitative Finance,
11(7):991–1012, 2011.
4. T. Andersen, T. Bollerslev, F. Diebold, and P. Labys. The
distribution of realized exchange rate volatility. Journal of the
American Statistical Association, 96(453):42–55, 2001.
5. O. E. Barndorff-Nielsen. Econometric analysis of realized
volatility and its use in estimating stochastic volatility models.
Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 64(2):253–280, 2002.
6. M. Garman. Market microstructure. Journal of Financial
Economics, 3(3):257–275, 1976.
7. T. Ho and H. Stoll. Optimal dealer pricing under transactions and
return uncertainty. Journal of Financial Economics, 9(1):47–73,
1981.
8. O. Kallenberg. Random measures. Akademie-Verlag, 1983.
9. A. Karr. Point processes and their statistical inference. CRC
press, 1991.
10. P. Brémaud. Point processes and queues. Springer, 1981.
11. D. Daley and D. Vere-Jones. An introduction to the theory of
point processes: Volume I: Elementary theory and methods.
Springer, 2003.
12. D. Daley and D. Vere-Jones. An introduction to the theory of
point processes: Volume II: General theory and structure.
Springer, 2007.
13. A. G. Hawkes. Spectra of some self-exciting and mutually exciting
point processes. Biometrika, 58(1):83–90, 1971.
14. Y. Ogata. Statistical models for earthquake occurrences and
residual analysis for point processes. Journal of the American
Statistical Association, 83(401):9–27, 1988.
15. Y. Ogata and H. Akaike. On linear intensity models for mixed
doubly stochastic poisson and self-exciting point processes.
Journal of the Royal Statistical Society. Series B
(Methodological), 102–107, 1982.
16. D. Oakes. The Markovian self-exciting process. Journal of
Applied Probability, 69–77, 1975.
\n\n=== PAGE 244 ===\n17. J. Da Fonseca and R. Zaatour. Clustering and mean reversion in a
Hawkes microstructure model. Journal of Futures Markets,
2014.
18. A. G. Hawkes and D. Oakes. A cluster process representation of a
self-exciting process. Journal of Applied Probability, 493–503,
1974.
19. J. Møller and J. G. Rasmussen. Perfect simulation of Hawkes
processes. Advances in Applied Probability, 629–646, 2005.
20. P. Brémaud and L. Massoulié. Stability of nonlinear Hawkes
processes. The Annals of Probability, 1563–1588, 1996.
21. P. Brémaud, G. Nappo, and G. Torrisi. Rate of convergence to
equilibrium of marked Hawkes processes. Journal of Applied
Probability, 39(1):123–136, 2002.
22. C. G. Bowsher. Modelling security market events in continuous
time: Intensity based, multivariate point process models. Journal
of Econometrics, 141(2):876–912, 2007.
23. P. Brémaud and L. Massoulié. Hawkes branching point processes
without ancestors. Journal of Applied Probability, 38(1):122–
135, 2001.
24. E. Bacry, S. Delattre, M. Hoffmann, and J. Muzy. Some limit
theorems for Hawkes processes and application to financial
statistics. Stochastic Processes and Their Applications, 2013.
25. T. Jaisson and M. Rosenbaum. Limit theorems for nearly
unstable Hawkes processes. The Annals of Applied Probability,
25(2):600–631, 2015.
26. L. Zhu. Central limit theorem for nonlinear Hawkes processes.
Journal of Applied Probability, 50(3):760–771, 2013.
27. P. Billingsley. Convergence of probability measures. John Wiley
& Sons, 2009.
28. J. Jacod and A. N. Shiryaev. Limit theorems for stochastic
processes. Springer-Verlag, Berlin, 1987.
29. T. Ozaki. Maximum likelihood estimation of Hawkes’ self-exciting
point processes. Annals of the Institute of Statistical
Mathematics, 31(1):145–155, 1979.
30. Y. Ogata. On Lewis’ simulation method for point processes.
Information Theory, IEEE Transactions on, 27(1):23–31, 1981.
31. J. Zhuang, Y. Ogata, and D. Vere-Jones. Analyzing earthquake
clustering features by using stochastic reconstruction. Journal of
Geophysical Research, 109(B5), 2004.
\n\n=== PAGE 245 ===\n32. P. A. Lewis and G. S. Shedler. Simulation of nonhomogeneous
poisson processes by thinning. Naval Research Logistics
Quarterly, 26(3):403–413, 1979.
33. A. Veen and F. P. Schoenberg. Estimation of space--time
branching process models in seismology using an EM--type
algorithm. Journal of the American Statistical Association,
103(482):614–624, 2008.
34. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood from incomplete data via the EM algorithm. Journal of
the Royal Statistical Society. Series B (Methodological), 1–38,
1977.
35. G. Mclachlan and T. Krishnan. The EM algorithm and
extensions. John Wiley & Sons, Inc., 2007.
36. D. Marsan and O. Lengline. Extending earthquakes’ reach
through cascading. Science, 319(5866):1076–1079, 2008.
37. P. Halpin and P. Boeck. Modelling dyadic interaction with
Hawkes processes. Psychometrika, 78(4):793–814, 2013.
38. P. F. Halpin. A scalable EM algorithm for Hawkes processes.
New developments in quantitative psychology. Springer, 2013.
39. J. F. Olson and K. M. Carley. Exact and approximate EM
estimation of mutually exciting Hawkes processes. Statistical
Inference for Stochastic Processes, 16(1):63–80, 2013.
40. L. P. Hansen. Large sample properties of generalized method of
moments estimators. Econometrica, 1029–1054, 1982.
41. J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration,
application to trade clustering, and diffusive limit. Journal of
Futures Markets, 2013.
42. G. Gusto and S. Schbath. Fado: A statistical method to detect
favored or avoided distances between occurrences of motifs using
the Hawkes’ model. Statistical Applications in Genetics and
Molecular Biology, 4(1):1119, 2005.
43. C. De Boor. A practical guide to splines. Springer-Verlag, New
York, 1978.
44. H. Akaike. Information theory and an extension of the maximum
likelihood principle. In Second international symposium on
information theory, pp. 267–281. Akademinai Kiado, 1973.
45. P. Reynaud-Bouret and S. Schbath. Adaptive estimation for
Hawkes processes; application to genome analysis. The Annals of
Statistics, 38(5):2781–2822, 2010.
\n\n=== PAGE 246 ===\n46. N. R. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and
probabilistic inequalities for multivariate point processes.
Bernoulli, 21(1):83–143, 2015. ISSN 1350-7265.
47. R. Tibshirani. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B
(Methodological), 267–288, 1996.
48. K. Zhou, H. Zha, and L. Song. Learning triggering kernels for
multi-dimensional Hawkes processes. In Proceedings of the 30th
International Conference on Machine Learning (ICML-13),
1301–1309, 2013.
49. D. R. Hunter and K. Lange. A tutorial on MM algorithms. The
American Statistician, 58(1):30–37, 2004.
50. E. Bacry, K. Dayri, and J. F. Muzy. Non-parametric kernel
estimation for symmetric Hawkes processes. Application to high
frequency financial data. The European Physical Journal B,
85(5), 2012.
51. E. Bacry and J.-F. Muzy. Hawkes model for price and trades high-
frequency dynamics. Quantitative Finance, 14(7):1147–1166,
2014.
52. E. Bacry and J.-F. Muzy. Second order statistics characterization
of Hawkes processes and non-parametric estimation. arXiv
preprint arXiv:1401.0903, 2014.
53. B. Noble. Methods based on the Wiener-Hopf technique.
Pergamon Press, New York, 1958.
54. G. Chandlen and I. G. Graham. The convergence of Nyström
methods for Wiener–Hopf equations. Numerische Mathematik,
52(3):345–364, 1987.
55. A. Baddeley, R. Turner, J. Møller, and M. Hazelton. Residual
analysis for spatial point processes (with discussion). Journal of
the Royal Statistical Society: Series B (Statistical Methodology),
67(5):617–666, 2005.
56. M. Wilk and R. Gnanadesikan. Probability plotting methods for
the analysis of data. Biometrika, 55(1):1–17, 1968.
57. A. N. Kolmogorov. Sulla determinazione empirica di una legge di
distribuzione. Giornale dell'Istituto Italiano degli Attuari,
4(1):83–91, 1933.
58. N. V. Smirnov. On the estimation of the discrepancy between
empirical curves of distribution for two independent samples.
Bulletin of Mathematics University of Moscow, 2(2), 1939.
\n\n=== PAGE 247 ===\n59. N. Smirnov. Table for estimating the goodness of fit of empirical
distributions. The Annals of Mathematical Statistics, 279–281,
1948.
60. G. M. Ljung and G. E. Box. On a measure of lack of fit in time
series models. Biometrika, 65(2):297–303, 1978.
61. F. P. Schoenberg. Multidimensional residual analysis of point
process models for earthquake occurrences. Journal of the
American Statistical Association, 98(464), 2003.
62. J. Large. Measuring the resiliency of an electronic limit order
book. Journal of Financial Markets, 10(1):1–25, 2007.
63. I. Muni Toke and F. Pomponio. Modelling trades-through in a
limited order book using Hawkes processes. Economics: The
Open-Access, Open-Assessment E-Journal, 6(2012-22), 2012.
64. I. Muni Toke. ``Market Making'' in an order book model and its
impact on the spread. In Econophysics of order-driven markets,
pages 49–64. Springer, 2011.
65. P. Hewlett. Clustering of order arrivals, price impact and trade
path optimisation. In Workshop on Financial Modeling with
Jump processes, Ecole Polytechnique, pages 6–8, 2006.
66. A. Alfonsi and P. Blanc. Dynamic optimal execution in a mixed-
market-impact Hawkes price model. arXiv preprint
arXiv:1404.0648, 2014.
67. T. Jaisson. Market impact as anticipation of the order flow
imbalance. Quantitative Finance, 15(7):1123–1135, 2015.
68. E. Bacry, S. Delattre, M. Hoffmann, and J.-F. Muzy. Modelling
microstructure noise with mutually exciting point processes.
Quantitative Finance, 13(1):65–77, 2013.
69. Y. Aït-Sahalia, P. A. Mykland, and L. Zhang. How often to sample
a continuous-time process in the presence of market
microstructure noise. Review of Financial Studies, 18(2):351–
416, 2005.
70. T. W. Epps. Comovements in stock prices in the very short run.
Journal of the American Statistical Association, 74(366a):291–
298, 1979.
71. T. Andersen, T. Bollerslev, F. Diebold, and P. Labys. Great
realizations. Risk Magazine, 2000.
72. S. L. Heston. A closed-form solution for options with stochastic
volatility with applications to bond and currency options. Review
of Financial Studies, 6(2):327–343, 1993.
\n\n=== PAGE 248 ===\n73. F. M. Bandi and J. R. Russell. Separating microstructure noise
from volatility. Journal of Financial Economics, 79(3):655–692,
2006.
74. D. Duffie and R. Kan. A yield-factor model of interest rates.
Mathematical Finance, 6(4):379–406, 1996.
75. D. Duffie, J. Pan, and K. Singleton. Transform analysis and asset
pricing for affine jump-diffusions. Econometrica, 68(6):1343–
1376, 2000.
76. L. Zhu. Limit theorems for a Cox–Ingersoll–Ross process with
Hawkes jumps. Journal of Applied Probability, 51(3):699–712,
2014.
77. Y. Aït-Sahalia, J. Cacho-Diaz, and R. J. A. Laeven. Modeling
financial contagion using mutually exciting jump processes.
Journal of Financial Economics, 117(3):585–606, 2015.
78. V. Filimonov and D. Sornette. Quantifying reflexivity in financial
markets: Toward a prediction of flash crashes. Physical Review
E, 85(5):056108, 2012.
79. V. Filimonov, D. Bicchetti, N. Maystre, and D. Sornette.
Quantification of the high level of endogeneity and of structural
regime shifts in commodity markets. Journal of International
Money and Finance, 2013.
80. R. J. Shiller. Irrational exuberance. Princeton University Press,
2005.
81. J. B. De Long, A. Shleifer, L. H. Summers, and R. J. Waldmann.
Positive feedback investment strategies and destabilizing rational
speculation. Journal of Finance, 379–395, 1990.
82. L. Harris. Order exposure and parasitic traders. Unpublished
manuscript, 1997.
83. G. Soros. The alchemy of finance. John Wiley Sons, 2003.
84. S. J. Hardiman, N. Bercot, and J.-P. Bouchaud. Critical reflexivity
in financial markets: A Hawkes process analysis. The European
Physical Journal B, 86(10):1–9, 2013.
85. V. Filimonov and D. Sornette. Apparent criticality and calibration
issues in the Hawkes self-excited point process model:
application to high-frequency financial data. Quantitative
Finance, 15(8):1293–1314, 2015.
86. S. J. Hardiman and J.-P. Bouchaud. Branching-ratio
approximation for the self-exciting Hawkes process. Physical
Review E, 90(6):062807, 2014.
\n\n=== PAGE 249 ===\n87. F. Papangelou. The conditional intensity of general point
processes and an application to line processes. Probability
Theory and Related Fields, 28(3):207–226, 1974.
88. P. A. Meyer. Démonstration simplifiée d’un théorème de knight.
In Séminaire de probabilités v université de strasbourg, pages
191–195. Springer, 1971.
89. F. Papangelou. Integrability of expected increments of point
processes and a related random change of scale. Transactions of
the American Mathematical Society, 165:483–506, 1972.
90. D. R. Cox. Some statistical methods connected with series of
events. Journal of the Royal Statistical Society. Series B
(Methodological), 17(2):129–164, 1955.
91. Y. Ogata. Space-time point-process models for earthquake
occurrences. Annals of the Institute of Statistical Mathematics,
50(2):379–402, 1998.
92. V. Chavez-Demoulin, A. Davison, and A. Mcneil. Estimating
value-at-risk: A point process approach. Quantitative Finance,
5(2):227–234, 2005.
93. E. Errais, K. Giesecke, and L. R. Goldberg. Affine point processes
and portfolio credit risk. SIAM Journal on Financial
Mathematics, 1(1):642–665, 2010.
94. R. Crane and D. Sornette. Robust dynamic classes revealed by
measuring the response function of a social system. Proceedings
of the National Academy of Sciences, 105(41):15649–15653,
2008.
95. C. Blundell, J. Beck, and K. A. Heller. Modelling reciprocating
relationships with Hawkes processes. In Advances in Neural
Information Processing Systems, pages 2609–2617, 2012.
96. M. Krumin, I. Reutsky, and S. Shoham. Correlation-based
analysis and generation of multiple spike trains using Hawkes
models with an exogenous input. Frontiers in Computational
Neuroscience, 4, 2010.
97. V. Pernice, B. Staude, S. Cardanobile, and S. Rotter. How
structure determines correlations in neuronal networks. PLoS
Computational Biology, 7(5):e1002059, 2011.
98. T. Liniger. Multivariate Hawkes processes. PhD thesis, Swiss
Federal Institute of Technology, 2009.
99. L. Zhu. Nonlinear Hawkes processes. PhD thesis, New York
University, 2013.
\n\n=== PAGE 250 ===\nNotes
1 We may refer to the ground process of an MPP without the
subscript “g” if the meaning is clear from the input parameter.
2 Notice that if marks are independent, future location tn + 1 cannot
depend on previous mark yn.
3 See Appendix 7.A for formal definition and existence condition of
stochastic intensity.
4 Notice that some authors use γji, so that the first index is the source
type and the second index is the destination type.
5 Hawkes process only specifies the intensity without any restriction
on the mark distribution.
6 N itself is not a Markov process as its intensity at time t depends on
its full history before time t.
7 See Appendix 7.A for definition of stationarity of point processes.
8 
.
9 ρ(A) = max i{|πi|}, {πi} are eigenvalues of A.
10 A sequence of random variables 
 if
11 A sequence of random variables 
 if limn → ∞E(|Xn −
X|2) = 0.
12 A sequence of probability measure Pn converges weakly to P if
 for all bounded continuous function f. A
sequence of stochastic process Xn: Ω → D[0, 1] converges weakly
\n\n=== OCR PAGE 250 ===\nNotes

+ We may refer to the ground process of an MPP without the
subscript “g” if the meaning is clear from the input parameter.

? Notice that if marks are independent, future location t,, , , cannot
depend on previous mark y,,.

3 See Appendix 7.A for formal definition and existence condition of
stochastic intensity.

4 Notice that some authors use y;;, so that the first index is the source
type and the second index is the destination type.

5 Hawkes process only specifies the intensity without any restriction
on the mark distribution.

6 N itself is not a Markov process as its intensity at time t depends on
its full history before time t.

7 See Appendix 7.A for definition of stationarity of point processes.

f :R — Ris called k-Lipschitz (k>0) if [f(x) — FQ) < ke —
8y|Vx,yER

9 p(A) = max ;{|7,|}, {7,3 are eigenvalues of A.
as.

n

4° A sequence of random variables x, n—00 x if
P(lim,_.., X, =X) = 1.
Lv
+ A sequence of random variables Xn noo x iflim, —, .(|Xn -

X|?) =0.
+2 A sequence of probability measure P,, converges weakly to P if

i} QfAP, > J o/4P for all bounded continuous function fA
sequence of stochastic process X,,: © — D[o, 1] converges weakly
\n\n=== PAGE 251 ===\n(in distribution) to X if the law of Xn(Pn○X− 1
n) converges weakly
to law of X(P○X− 1) in the sense of probability measure, D[0,1] is
the Skorokhod space of càdlàg (right continuous with left limits)
functions (see [27, 28]).
13 
.
14 Although GMM is consistent under some mild regularity
conditions, unlike MLE, it is not asymptotic efficient among the
class of consistent estimators.
15 The terminology is not standard. Baddeley et al. [55] refer
 as residual to extend the concept to higher
dimension.
16 Although Bowsher’s paper was published in 2007, the first draft
appeared in 2002.
17 We show only the one-dimensional case for simplicity.
18 Filimonov and Sornette [78] borrow this term from Soros [83].
19 Some textbooks use complete separable metric space, but locally
compact Hausdorff second countable space has a complete
separable metrization and all the results here do not depend on
any particular choice of metric [8, p. 11]. In most cases, 
.
20 On locally compact Hausdorff second countable space, all locally
finite Borel measures are Radon measures.
21 
 is the same as the Borel σ-algebra generated by the vague
topology of 
 [8, p. 32].
22 X is Hausdorff, so all singletons are closed and thus measurable.
23 
; that
is, N is shifted t unit to the left when 
.
\n\n=== OCR PAGE 251 ===\n(in distribution) to X if the law of X,(P,°X~ 1,,) converges weakly

to law of X(PoX~ *) in the sense of probability measure, D[o,1] is
the Skorokhod space of cadlag (right continuous with left limits)
functions (see [27, 28]).

Vv ERY, diag) = [alae ai = Vir ag =OW FI,
44 Although GMM is consistent under some mild regularity

conditions, unlike MLE, it is not asymptotic efficient among the
class of consistent estimators.

+5 The terminology is not standard. Baddeley et al. [55] refer
rt, 3 “
{N(,) — fo" A(s)dS} ag residual to extend the concept to higher
dimension.

16 Although Bowsher’s paper was published in 2007, the first draft
appeared in 2002.

+2 We show only the one-dimensional case for simplicity.
18 Filimonov and Sornette [78] borrow this term from Soros [83].

+9 Some textbooks use complete separable metric space, but locally
compact Hausdorff second countable space has a complete
separable metrization and all the results here do not depend on
any particular choice of metric [8, p. 11]. In most cases, X = R”.

2° On locally compact Hausdorff second countable space, all locally
finite Borel measures are Radon measures.

21 N'(X) is the same as the Borel o-algebra generated by the vague
topology of 2(X) [8, p. 32].
22 X is Hausdorff, so all singletons are closed and thus measurable.

23 NoS, : Q —+ (N(X), N(X)), (NoS,)(@))(A) = (N(@))(S,(A)); that
is, N is shifted t unit to the left when X = R.
\n\n=== PAGE 252 ===\n24 The notations of moment, covariance, factorial moment, and
reduced moment vary between authors.
25 From this definition, Poisson random measure N on 
 is not MPP
on 
 as 
26 Any point process can be treated as simple MPP, with the mark
being the number of points at xi.
27 Notice that if marks are independent, future location xn + 1 cannot
depend on previous mark yn.
28 The stochastic intensity of point process on 
 is extended to
higher dimension in [87].
29 Stochastic intensity is unique up to modification [10, p. 31].
30 Notice that stochastic intensity depends on the underlying
filtration, so some text use the notation 
 but we will simply
use λ(t) and call it stochastic intensity or intensity when there is
no confusion about the filtration.
31 if At is absolutely continuous with respect to Lebesgue measure
and λt is the Radon–Nikodym derivative (may not be predictable)
then 
 is a version of the stochastic intensity. Some
authors require only the intensity to be adapted, but using the
conditional expectation, one can always find a predictable version
of intensity provided that the intensity has finite first moment.
32 For processes on 
, it means that intensity of Cox process is 
measurable.
33 Although Bowsher’s paper was published in 2007, the first draft
appeared in 2002.
\n\n=== OCR PAGE 252 ===\n24 The notations of moment, covariance, factorial moment, and
reduced moment vary between authors.

25 From this definition, Poisson random measure N on R? is not MPP
onR x Ras MA XR) = 0.

26 Any point process can be treated as simple MPP, with the mark
being the number of points at x;.

27 Notice that if marks are independent, future location x,, , , cannot
depend on previous mark y,,.

R

28 The stochastic intensity of point process on + is extended to

higher dimension in [87].
29 Stochastic intensity is unique up to modification [10, p. 31].

3° Notice that stochastic intensity depends on the underlying
filtration, so some text use the notation 4(‘I¥;) but we will simply
use A(t) and call it stochastic intensity or intensity when there is
no confusion about the filtration.

31 if A; is absolutely continuous with respect to Lebesgue measure
and A, is the Radon—Nikodym derivative (may not be predictable)
then E(/,|F,-) is a version of the stochastic intensity. Some
authors require only the intensity to be adapted, but using the
conditional expectation, one can always find a predictable version
of intensity provided that the intensity has finite first moment.

32 For processes on R,, it means that intensity of Cox process is Fo
measurable.

33 Although Bowsher’s paper was published in 2007, the first draft
appeared in 2002.
\n\n=== PAGE 253 ===\nChapter Eight
Multifractal Random Walk Driven by a
Hermite Process:
Alexis Fauth1,2 and Ciprian A. Tudor3,4,*
1SAMM, Université de Paris 1 Panthéon-Sorbonne, 90, rue de
Tolbiac, 75634, Paris, France
2Invivoo, 13 rue de l’Abreuvoir, 92400 Courbevoie, France
3Laboratoire Paul Painlevé, Université de Lille 1, F-59655
Villeneuve d’Ascq, France
4Department of Mathematics, Academy of Economical Studies,
Bucharest, Romania
8.1 Introduction
The1 so-called random multifractal processes (or simply
multifractals) have been used to model natural and man-made
phenomena in a variety of fields, such as hydrodynamics, genetics,
Internet traffic analysis, or stock prices. Our work concerns the
applications of multifractals to this last field, mathematical finance.
Starting with the seminal work of Mandelbrot about cotton price
[22], several studies of financial stock prices times series have
allowed to exhibit some particularities of their fluctuations. Without
making a comprehensive list, we can mention the appearance in
empirical data of the following properties: non-Gaussian
distributions due to now well-known fat tails of financial returns, the
so-called volatility clustering that means that the volatility
fluctuations are of intermittent and of correlated nature, scaling
invariance, long-run correlation in volatility, leverage effect, and so
on (see, e.g., [17, 18, 34] for an extensive review). Thus, constructing
theoretical models for financial returns that include all the properties
listed before appears as a very interesting challenge.
The multifractal models are introduced to capture the
aforementioned stylized facts but multifractality started to emerge as
\n\n=== PAGE 254 ===\n(8.1)
(8.2)
an additional stylized fact since the nineties. The multifractality can
be understood as an invariance scaling property. At any scales, we
observe the same properties. The concept of multifractality
represents an extension of the notion of self-similarity. A process
(Xt)t ≥ 0 is multifractal if it satisfies the following scaling property:
at any scale l and for any q ≥ 0. The exponent ξ(q) is usually called
the multifractal spectrum of the process X.
When the exponent ξ(q) is linear in q, that is, ξ(q) = qH, the process
is referred to as a monofractal process. The self-similar processes
(e.g., fractional Brownian motions) are a particular case of
monofractal processes. When ξ(q) is a nonlinear function of q, it is
referred to as a multifractal process, or sometimes, as a process
displaying multiscaling or intermittency.
In high-frequency data, one observes a nontrivial multifractal scaling
in the sense that the higher moments of the return process satisfy the
property (8.1) where the index ξ(q) is not a constant and it is not a
linear function of q. The multifractal random walk (MRW)
constructed in our chapter has this multiscaling character.
The MRW (see, e.g., [8], and see [1]) is defined as
where Q is a suitable fractal noise and Y is a self-similar process with
stationary increments, independent of Q. In his initial form [6], Y is
chosen as a standard Brownian motion. More recent approaches ([1,
15]) used as an integrator in (8.2) the fractional Brownian motion
(fBm in the sequel) and this choice is more appropriate to obtain
long-memory properties for the financial log-returns.
In addition to multifractality, this model is able to capture several
properties observed on the empirical data, clustering,
autocorrelation in log-returns, and squared log-returns. This process
is also able to reproduce the leverage effect when the fractal noise Q
\n\n=== OCR PAGE 254 ===\nan additional stylized fact since the nineties. The multifractality can
be understood as an invariance scaling property. At any scales, we
observe the same properties. The concept of multifractality
represents an extension of the notion of self-similarity. A process
(X,); 5 9 is multifractal if it satisfies the following scaling property:

E Xi) ~ x,|* = KE (8.1)

at any scale / and for any q = 0. The exponent &(q) is usually called
the multifractal spectrum of the process X.

When the exponent &(q) is linear in q, that is, &(q) = qH, the process
is referred to as a monofractal process. The self-similar processes
(e.g., fractional Brownian motions) are a particular case of
monofractal processes. When &(q) is a nonlinear function of q, it is
referred to as a multifractal process, or sometimes, as a process
displaying multiscaling or intermittency.

In high-frequency data, one observes a nontrivial multifractal scaling
in the sense that the higher moments of the return process satisfy the
property (8.1) where the index &(q) is not a constant and it is not a
linear function of g. The multifractal random walk (MRW)
constructed in our chapter has this multiscaling character.

The MRW (see, e.g., [8], and see [1]) is defined as

Z(t) -/ Q(u)dY(u) (8.2)
0

where Q is a suitable fractal noise and Yis a self-similar process with
stationary increments, independent of Q. In his initial form [6], Yis
chosen as a standard Brownian motion. More recent approaches ([1,
15]) used as an integrator in (8.2) the fractional Brownian motion
(fBm in the sequel) and this choice is more appropriate to obtain
long-memory properties for the financial log-returns.

In addition to multifractality, this model is able to capture several
properties observed on the empirical data, clustering,
autocorrelation in log-returns, and squared log-returns. This process
is also able to reproduce the leverage effect when the fractal noise Q
\n\n=== PAGE 255 ===\nand the integrator Y in (8.2) are dependent processes, which is not
the considered case here. Such an example can be found in [15].
The purpose of this work is to propose a generalization of the model
(8.2) that allows more flexibility for the log-return. Actually the
integrator Y in (8.2), who is usually chosen to be a standard
Brownian motion, will be replaced by a larger class of long-memory
processes, called the Hermite processes that includes the fractional
Brownian motion and Rosenblatt process.
The Hermite process of order k ≥ 1 is a self-similar process with
stationary increments and exhibits long memory. When k = 1, it
reduces to the fractional Brownian motion but is not a Gaussian
process for k ≥ 2. The Hermite process of order 2 is called the
Rosenblatt process.
We will show that this new model, with the fBm replaced by a
Hermite process (this will be called a Hermite multifractal random
walk and abbreviated HMRW) of order k in (8.2) keeps the
multifractal character. Indeed, we prove in Section 8.3 that equation
(8.1) is satisfied by the HMRW. This is because the integrator in
(8.2) is self-similar and it has stationary increments. Moreover, the
HMRW is also able to capture other empirical stylized facts, such as
the decay of the autocorrelation of the return process. This property
is induced by the long-range dependence of the Hermite process. All
these facts, together with the non-Gaussian nature of the object
(8.2), make the HMRW a natural candidate to model high-frequency
financial data.
One of the main challenges that appear when working with such a
process as an integrator in (8.2) is how to construct the stochastic
integral with respect to it. Since the Hermite process is not a
semimartingale, and it is not Gaussian, no classical stochastic
integral can be defined with respect to it. Therefore, we will restrict
to the case when Y (which will be the Hermite process) and the
fractal noise Q is (8.2) are independent. In this case, the stochastic
integral in (8.2) can be viewed as a Wiener integral, which is easier to
define and handle. The model still keeps very well many properties of
the empirical data, as the initial MRW, including long-memory in
log-returns.
\n\n=== PAGE 256 ===\nWe have organized our chapter as follows. In Section 8.2, we recall
the definition and the main properties of the Hermite process and we
explain how the Wiener integrals with respect to it are constructed.
In Section 8.3, we define and analyze the HMRW, while in Section
8.4, we describe how our object can be simulated and how they fit to
real-world data.
8.2 Preliminaries
In this paragraph, we first describe the integrator in the integral in
(8.2) and then we explain how the Wiener integral with respect to a
Hermite process is constructed. More details can be found in [26]. In
the last part of this section, we present the definition and the basic
properties of the fractal process Q that appears in the expression
(8.2).
8.2.1 FRACTIONAL BROWNIAN MOTION AND HERMITE
PROCESSES
Let us first introduce the integrator in the integral in (8.2). The
fractional Brownian motion (BH
t)t ≥ 0 with Hurst parameter H ∈ (0,
1) is a centered Gaussian process starting from 0 with covariance
function
BH is a H-self-similar process with stationary increments. Actually, it
is the only Gaussian process H-self-similar with stationary
increments.
The fractional Brownian process (BH
t)t ∈ [0, 1] with Hurst parameter
H ∈ (0, 1) can be written as a Wiener integral with respect to the
Wiener process as
\n\n=== OCR PAGE 256 ===\nWe have organized our chapter as follows. In Section 8.2, we recall
the definition and the main properties of the Hermite process and we
explain how the Wiener integrals with respect to it are constructed.
In Section 8.3, we define and analyze the HMRW, while in Section
8.4, we describe how our object can be simulated and how they fit to
real-world data.

8.2 Preliminaries

In this paragraph, we first describe the integrator in the integral in
(8.2) and then we explain how the Wiener integral with respect to a
Hermite process is constructed. More details can be found in [26]. In
the last part of this section, we present the definition and the basic
properties of the fractal process Q that appears in the expression

(8.2).

8.2.1 FRACTIONAL BROWNIAN MOTION AND HERMITE
PROCESSES

Let us first introduce the integrator in the integral in (8.2). The
fractional Brownian motion (B”,),, . with Hurst parameter H «€ (0,

1) is a centered Gaussian process starting from o with covariance
function

R"(t,s):= ; (t 2H 4 2H _ t—s|7"), s,t>0.

B" is a H-self-similar process with stationary increments. Actually, it
is the only Gaussian process H-self-similar with stationary
increments.

The fractional Brownian process (B”,); < [o, 1) With Hurst parameter

H €é (0, 1) can be written as a Wiener integral with respect to the
Wiener process as
\n\n=== PAGE 257 ===\n(8.3)
where (Wt, t ≥ 0) is a standard Wiener process, the kernel KH(t, s)
has the expression (when 
) cHs1/2 − H∫t
s(u − s)H − 3/2uH − 1/2du
for t > s (and it vanishes if s ≥ t), cH is an explicit positive constant.
The above integral is a Wiener integral with respect to the standard
Wiener process W. For t > s, the kernel’s derivative is
. Fortunately, we will not need to use
these expressions explicitly, since they will be involved below only in
integrals whose expressions are known.
We will denote by (Zk
H(t))t ≥ 0 the qth Hermite process with self-
similarity parameter H ∈ (1/2, 1). Here q ≥ 1 is an integer. The
Hermite process can be defined as a multiple integral with respect to
the standard Wiener process W; details concerning the definition of
multiple stochastic integrals can be found in Chapter 1 in [25]. These
multiple integrals can be viewed as iterated Itô integrals. Concretely,
we have the following definition.
Definition 8.1 Let 
 be a Wiener process. The Hermite
process (Zk
H(t))t ≥ 0 of order k and with self-similarity index
 is defined as
where x+ = max (x, 0). The above integral is a multiple integral of
order k with respect to the Brownian motion B and the constant
c(H, k) is a normalizing constant that ensures that 
.
Remark 8.1 Throughout, a random variable that has the same law
as Zk
H(1) will be called as Hermite random variable.
The most studied Hermite process is, of course, the fractional
Brownian motion (which is obtained in (8.3) for k = 1) due to its
\n\n=== OCR PAGE 257 ===\nt
at = | K"(t,s)\dW,, t>0
0

where (W,, t 2 0) is a standard Wiener process, the kernel K"(t, s)

1
has the expression (when H> 2) cys? ~*ft(u — s)#- 3/2u- V2du
for t > s (and it vanishes if s = f), cy is an explicit positive constant.

The above integral is a Wiener integral with respect to the standard
Wiener process W. For t > s, the kernel’s derivative is

ox" _ S\1/2-Hyy _ .H-3/2

a 5) = CHG) (t—s) . Fortunately, we will not need to use
these expressions explicitly, since they will be involved below only in
integrals whose expressions are known.

We will denote by (Z*;,(t)); 9 the qth Hermite process with self-
similarity parameter H & (1/2, 1). Here q = 1is an integer. The
Hermite process can be defined as a multiple integral with respect to
the standard Wiener process W; details concerning the definition of
multiple stochastic integrals can be found in Chapter 1 in [25]. These
multiple integrals can be viewed as iterated It integrals. Concretely,
we have the following definition.

Definition 8.1 Let (B():er be a Wiener process. The Hermite
process (Z*,(t))s 9 of order k and with self-similarity index

I
HE (: Dis defined as

‘ft (148 (8.3)
Zyit)= (Hk) [ [ (icon ) dB, do
RESO \i=I

where x, = max (x, 0). The above integral is a multiple integral of
order k with respect to the Brownian motion B and the constant

ke)? —
c(H, k) is a normalizing constant that ensures thatE (Z;) =1

Remark 8.1 Throughout, a random variable that has the same law
as Z*;,(1) will be called as Hermite random variable.

The most studied Hermite process is, of course, the fractional
Brownian motion (which is obtained in (8.3) for k = 1) due to its
\n\n=== PAGE 258 ===\n(8.4)
(8.5)
(8.6)
large range of applications. The process obtained in (8.3) for k = 2 is
known as the Rosenblatt process. It was introduced by Rosenblatt in
[29] and it has been named in this way by M. Taqqu in [30].
First, we mention that Zk
H is a centered process since it is defined by
a multiple stochastic integral. Of fundamental importance is the fact
that the covariance of Zk
H is identical to that of fBm, namely, for
every s, t ≥ 0
The basic properties of the Hermite process are listed below:
the Hermite process Zk
H is H-self-similar and it has stationary
increments.
the mean square of the increment is given by, for s, t ≥ 0
as a consequence, it follows will little extra effort from
Kolmogorov’s continuity criterion that Zk
H has Hölder-
continuous paths of any exponent δ < H. From the self-similarity
and the stationary of increments of the Hermite process, we also
have, for every p > 2, that
it exhibits long-range dependence in the sense that
. In fact, the summand in
this series is of order n2H − 2. This property is identical to that of
fBm since the processes share the same covariance structure, and
the property is well-known for fBm with H > 1/2.
\n\n=== OCR PAGE 258 ===\nlarge range of applications. The process obtained in (8.3) for k = 2 is
known as the Rosenblatt process. It was introduced by Rosenblatt in
[29] and it has been named in this way by M. Taqqu in [30].

First, we mention that Z*,, is a centered process since it is defined by
a multiple stochastic integral. Of fundamental importance is the fact
that the covariance of Z*;, is identical to that of fBm, namely, for
every s,t20

: [ 1 > 2 2
RUt,8) = E [ZZ (9)] = 5" + — |e 5"), (8.4)
The basic properties of the Hermite process are listed below:

+ the Hermite process Z*,, is H-self-similar and it has stationary
increments.

e

the mean square of the increment is given by, for s, t > 0

op ; 8.
E zi - 24 | = |t—s|2"; (8.5)

as a consequence, it follows will little extra effort from
Kolmogorov’s continuity criterion that Z*;, has Hélder-
continuous paths of any exponent 6 < H. From the self-similarity
and the stationary of increments of the Hermite process, we also
have, for every p > 2, that

E | zn(0 — Zyof'] = Ie 50" (8.6)

it exhibits long-range dependence in the sense that

k k k _
Laz EZ; Zim + 1) — Z7,()] = ©, Ty fact, the summand in
this series is of order n? ~ 2. This property is identical to that of

fBm since the processes share the same covariance structure, and
the property is well-known for fBm with H > 1/2.
\n\n=== PAGE 259 ===\n(8.7)
(8.8)
8.2.2 WIENER INTEGRALS WITH RESPECT TO THE
HERMITE PROCESS
In this section, we recall the definition of the Wiener integrals with
respect to the Hermite process. This construction has been done in
[26]. Consider a Hermite process given by (8.3).
Let us denote by  the class of elementary functions on  of the form
For 
 as above, it is natural to define its Wiener integral with
respect to the Hermite process Zk
H by
In order to extend the definition (8.8) to a larger class of integrands,
let us make first some observations. By formula (8.3), we can write
where by I we denote the mapping on the set of functions 
to the set of functions 
Note that for k = 1 this operator can be expressed in terms of
fractional integrals and derivatives, (see [28]). Thus, the definition
(8.8) can also be written in the form, because of the obvious linearity
of I
\n\n=== OCR PAGE 259 ===\n8.2.2 WIENER INTEGRALS WITH RESPECT TO THE
HERMITE PROCESS
In this section, we recall the definition of the Wiener integrals with

respect to the Hermite process. This construction has been done in
[26]. Consider a Hermite process given by (8.3).

Let us denote by € the class of elementary functions on R of the form

n

f(u) = Y 411 t)<ty), @ ER, l=1,...,n.
1=1

(8.7),

For f € E as above, it is natural to define its Wiener integral with
respect to the Hermite process Z*,, by

[seodeigen = Pa, (Zit) -Z@))- (8.8),
R I=1

In order to extend the definition (8.8) to a larger class of integrands,
let us make first some observations. By formula (8.3), we can write

Z(t) = [ I (Lpogq) O15 + 59)ABO)) -.. ABO)»
where by I we denote the mapping on the set of functions f: R > R
to the set of functions f : R‘ > R

1, 1a
i444)

k
Io. = Hb) [ fon Toy du.
R i=l

Note that for k = 1 this operator can be expressed in terms of
fractional integrals and derivatives, (see [28]). Thus, the definition
(8.8) can also be written in the form, because of the obvious linearity
of I
\n\n=== PAGE 260 ===\n(8.9)
We introduce now the following space:
endowed with the norm
It holds that
Hence we have
\n\n=== OCR PAGE 260 ===\n[food (u) = ¥ a, (Zh(ty41) -— Zit) (8.9)

= $a 7 IN g,4j,)) 1s «++ +e) ABO) «-- ABQ)
I= RS
= [ TAO, aes -,)dB(y,) oes dB(y;,).
Re

We introduce now the following space:

= {r RR Lf (IPO...) dy vd, <0}
Rt

endowed with the norm

lj, = [ (AGO, se yp) dy, 1 Ag.
RE

It holds that

ILfll3, = (Hk? [ ( [ / roopeo [Jor te)

Li
X(v—-y; ,( ‘ ant) dy,,..., dy,

k
= c(H, ky [ [vewo(] yt Poy —Y)y -G a)

x dvdu
= H(2H -1) [ [ f(wf(v)|u — v4? dvdu.
RJR

Hence we have
\n\n=== PAGE 261 ===\n(8.10)
and
Let us observe that the mapping
provides an isometry from  to L2(Ω). Indeed, for f of the form (8.7),
it holds that
where R represents the covariance function (8.4). On the other hand,
it has been proved in [28] that the set of elementary functions  is
dense in 
. As a consequence, the mapping (8.10) can be extended to
an isometry from 
 to L2(Ω) and relation (8.9) still holds. This
extension will be called the Wiener integral with respect to the
Hermite process.
The space 
 coincides with the canonical Hilbert space associated
with the fBm. Therefore, the followings facts hold (see [28] or [25]):
\n\n=== OCR PAGE 261 ===\nH= {r :-ROR | /, [reareote — vt dvdu < oo}
RJR

fl, = HOH - Df [ rearor — vp dydu.

and

Let us observe that the mapping

pre [ pundzi (8.10)
R

provides an isometry from € to L?(Q). Indeed, for f of the form (8.7),
it holds that

n-1

EL] = ¥ aij (Zhi) — ZA) (ZG) — Z|
ij=0
n-l

= DP aia; (Reig tens) — Reig t)) — RG tia) + RG, 4)

ij=0
nol fig tie

= YX a,ajH(2H _ vf / ju- vp! dvdu
n-1

= YX ajaj{ Lo...) Ling DH = Il, >
ij=0

where R represents the covariance function (8.4). On the other hand,
it has been proved in [28] that the set of elementary functions € is
dense in H. As a consequence, the mapping (8.10) can be extended to
an isometry from H to L?(Q) and relation (8.9) still holds. This
extension will be called the Wiener integral with respect to the
Hermite process.

The space H coincides with the canonical Hilbert space associated
with the fBm. Therefore, the followings facts hold (see [28] or [25]):
\n\n=== PAGE 262 ===\n(8.11)
(8.12)
The elements of 
 may be not functions but distributions; it is
therefore more practical to work with subspaces of 
 that are sets
of functions. Such a subspace is
Then 
 is a strict subspace of 
 and we actually have the
inclusions
The space 
 is not complete with respect to the norm 
, but
it is a Banach space with respect to the norm
8.2.3 INFINITELY DIVISIBLE CASCADING NOISE
Let M denote an infinitely divisible, independently scattered random
measure on the set 
 with generating infinitely divisible
distribution G satisfying
for some function ρ and for every 
. We assume that M has
control measure m on 
 meaning that for every Borel set
 it holds
The fact that M is independently scattered means that the random
variables
\n\n=== OCR PAGE 262 ===\n¢ The elements of H may be not functions but distributions; it is
therefore more practical to work with subspaces of H that are sets
of functions. Such a subspace is

|71| = {r: R-R |/ | FOOL LEO) [Ju — v2! Pdvdu < co}.
RJR

Then |Hlis a strict subspace of H and we actually have the
inclusions
LR) ALR) C Li(R) C|HI CH. (8.11)

+ The spacel#is not complete with respect to the norml « llz, but
it is a Banach space with respect to the norm

WFR) = | | Fw Foyt — vPt dvd.
RJR

8.2.3 INFINITELY DIVISIBLE CASCADING NOISE

Let M denote an infinitely divisible, independently scattered random
measure on the set ® X Ry with generating infinitely divisible
distribution G satisfying

[ e* G(dx) = e0
R

for some function p and for every 7 © R. We assume that M has

control measure mon R x R, meaning that for every Borel set
ACRXR, it holds

Ee — Pam) for every g € R. (8.12)

The fact that M is independently scattered means that the random
variables
\n\n=== PAGE 263 ===\n(8.13)
(8.14)
(8.15)
(8.16)
(8.17)
are independent whenever the Borel sets 
 are
disjoint. We define the Infinitely Divisible Cascading noise (IRC) by
for every r > 0 and 
. Here Cr(t) is the cone in 
 defined by
We will use the following facts throughout our chapter. We refer to
[11] or [1] for the their proofs. First, let us note the scaling property
of the moments of the IRC
and the expression of its covariance: for every r > 0 and 
where we denoted by for u ≥ 0, r > 0.
and by
In this work, we consider
and m vanishes if r ≥ 1. Here c is a strictly positive constant. The case
when the measure m is given by (8.17) is called in [4] (see also [1])
the exact invariant scaling case. This implies
\n\n=== OCR PAGE 263 ===\nM(A,),M(A)), -.., M(A,)

are independent whenever the Borel sets Aj, A, ERXR, are
disjoint. We define the Infinitely Divisible Cascading noise (IRC) by

eM(C,() (8.13),

EeMic,(o)

Q(t) =

for every r > o andt € R. Here C,(#) is the cone in RXR, defined by

Af
cfcra th (8.14)

We will use the following facts throughout our chapter. We refer to
[11] or [1] for the their proofs. First, let us note the scaling property
of the moments of the IRC

~

C(t) = {ee r<r<l,jt-

vl

EQ,(! = em
and the expression of its covariance: for everyr > o and4.sER
EQ,(t)O,(s) = e POMS) (8.15),
where we denoted by for u = 0, r > 0.
mu) =m (C,(0) fa) C,(u))
and by
(4) = p(q) — gp(1). (8.16)
In this work, we consider
m(dt, dr) = a if0O<r<l (8.17)
and m vanishes if r > 1. Here c is a strictly positive constant. The case

when the measure m is given by (8.17) is called in [4] (see also [1])
the exact invariant scaling case. This implies
\n\n=== PAGE 264 ===\n(8.18)
(8.19)
(8.20)
(8.21)
The scaling of the moment of Q can be extended to the following
scaling property in distribution: for t ∈ (0, 1)
where “= (d)” means equivalence of finite dimensional distributions.
Here Ωt denotes a random variable independent by Q, which
satisfies, if the measure m is given by (8.17),
Remark 8.2 As noticed in [1], we have φ(2) < 0.
In [1] (see also [20]) the MRW based on fractional Brownian motion
is defined as limit when r → 0 (in some sense) of the family of
stochastic integrals (ZH
r(t))r > 0 defined by
where (BH
t)t ∈ [0, T] is a fractional Brownian motion with Hurst
parameter H ∈ (0, 1). The fractional Brownian motion (BH
t)t ∈ [0, T]
with Hurst parameter H ∈ (0, 1) is a centered Gaussian process
starting from 0 with covariance function
In [1], it is assumed that M and BH are independent. We will follow
this assumption in our work. We mention that a situation when M
and BH are dependent has been treated in [15]. Therefore, the
stochastic integral with respect to BH in (8.20) behaves mainly as a
\n\n=== OCR PAGE 264 ===\nm,(u) = Oif u> 1,m,(u) = —clnu+c(u—l)ifr<u<l
and m,(u) = ~clnr + cu (1 -+) ifO<u<r.
r

The scaling of the moment of Q can be extended to the following
scaling property in distribution: for t € (0, 1)

(0,,(tu)) en =? e* (2,(u)) ep (8.18)

where “= “” means equivalence of finite dimensional distributions.
Here Q, denotes a random variable independent by Q, which
satisfies, if the measure m is given by (8.17),

Eem® = 799, (8.19)
Remark 8.2 As noticed in [1], we have p(2) < 0.

In [1] (see also [20]) the MRW based on fractional Brownian motion
is defined as limit when r — 0 (in some sense) of the family of

stochastic integrals (Z”,(t)),., defined by

zr = [ O,(u)dB"(u), tt € [0,T], (8.20)
0

where (B",); ¢ fo, 11 is a fractional Brownian motion with Hurst
parameter H ¢ (0, 1). The fractional Brownian motion (B”,), « [o, TI

with Hurst parameter H € (0, 1) is a centered Gaussian process
starting from o with covariance function

R"(t,s) = fie" +s _|p—sp"),  s,t © [0,7]. (8.21)

In [1], it is assumed that M and B” are independent. We will follow
this assumption in our work. We mention that a situation when M

and B” are dependent has been treated in [15]. Therefore, the
stochastic integral with respect to B” in (8.20) behaves mainly as a
\n\n=== PAGE 265 ===\n(8.22)
(8.23)
Wiener integral since, because of the independence, the integrand
Qr(u) can be viewed as deterministic function for the integrator BH.
Another important fact in the development of this theory is that the
IRC Q is a martingale with respect to the argument r. Let us recall
the following result (see [11]):
Lemma 8.1 For every u > 0 the stochastic process (Qr(u))r > 0 is a
martingale with respect to its own filtration. As a consequence, for
every u, v, r, r′ > 0 with r < r′ it holds
The property (8.22) plays an important role in the construction of
the MRW process in [1] or [11].
8.3 Multifractal random walk driven by a
Hermite process
8.3.1 DEFINITION AND EXISTENCE
Let us fix 
, and define for every r > 0
where Zk
H denotes a Hermite process of order k and with self-
similarity order H (see Section 8.2). This is the approximating
Hermite Multifractal Random Walk in the sense that its limit, when r
→ 0, will be defined as the Hermite MRW.
For the sake of simplicity, let us denote ZH
k by Z in the sequel. We
will assume that the multifractal measure M and the Hermite process
Z are independent and therefore the integral in (8.23) can be
understood as a Wiener integral with respect to the Hermite process
as described in Section 2.2.
\n\n=== OCR PAGE 265 ===\nWiener integral since, because of the independence, the integrand
Q,(u) can be viewed as deterministic function for the integrator B”.

Another important fact in the development of this theory is that the
IRC Q is a martingale with respect to the argument r. Let us recall
the following result (see [11]):

Lemma 8.1 For every u > 0 the stochastic process (Q,(u)),., 9 is a

martingale with respect to its own filtration. As a consequence, for
every u, v,7T,T’ > O withr <r’ it holds

EQ,(u)Q,.(v) = EQ(wQ,(v). (8.22)

The property (8.22) plays an important role in the construction of
the MRW process in [1] or [11].

8.3 Multifractal random walk driven by a
Hermite process

8.3.1 DEFINITION AND EXISTENCE

I
Let us fix 4 EG: 1) and define for every r > O

t
JO

where Z*;, denotes a Hermite process of order k and with self-
similarity order H (see Section 8.2). This is the approximating
Hermite Multifractal Random Walk in the sense that its limit, when r
— 0, will be defined as the Hermite MRW.

For the sake of simplicity, let us denote Z#, by Z in the sequel. We
will assume that the multifractal measure M and the Hermite process
Z are independent and therefore the integral in (8.23) can be
understood as a Wiener integral with respect to the Hermite process
as described in Section 2.2.
\n\n=== PAGE 266 ===\n(8.24)
We can easily prove that XH
r(t) is well-defined for every r, t. Indeed,
the stochastic integral in (8.23) can be viewed as a Wiener integral
with respect to Z and it follows easily from the definition of the
Wiener integral with respect to the Hermite process that XH
r(t) has
the same square mean independently on k ≥ 1 and we can directly
use the results obtained in [1] in the case k = 1. In fact, with αH =
H(2H − 1),
where we used the fact that 
 with cr a
strictly positive constant.
We can prove the following result.
Theorem 8.1 Let φ be given by (8.16) and assume
For every t ≥ 0 and for every p ≥ 2, the sequence (XH
r(t))r > 0
converges in Lp(Ω) to a random variable XH(t).
Proof The L2 convergence is immediate. Let r, r′ > 0 and consider
the difference 
. Its square mean can be written as
follows:
\n\n=== OCR PAGE 266 ===\nWe can easily prove that X”/,(t) is well-defined for every r, t. Indeed,
the stochastic integral in (8.23) can be viewed as a Wiener integral
with respect to Z and it follows easily from the definition of the
Wiener integral with respect to the Hermite process that X",(t) has

the same square mean independently on k = 1 and we can directly
use the results obtained in [1] in the case k = 1. In fact, with ay =

H(2H - 1),

t t
E |x" = ay [ [ dudvE(Q,(u)Q,(v))\u — v|24-?
0 Jo
t t
= ay [ | dudv|u — v|7#-2.e-POmneluv)
0 Jo
< C.,

—p(2)m. Vv) rn :
where we used the fact that EQ,(w)Q,(v) = e?O”"™""") < c, with ea
strictly positive constant.

We can prove the following result.

Theorem 8.1 Let ¢ be given by (8.16) and assume

co(p) + Hp > |. (8.24)

For every t > 0 and for every p > 2, the sequence (X",(t)),s 6
converges in LP(Q) to a random variable X"(t).

Proof The L? convergence is immediate. Let r, r’ > 0 and consider
the difference *,
follows:

H
— XO. tts square mean can be written as
\n\n=== PAGE 267 ===\nThroughout, we denote by BH a fBm with 
 (which could be Z1
H)
and the last convergence to 0 is a consequence of the proof of
Proposition 3.1 in [1] under assumption (8.24).
If p ≥ 2, we need a different argument based on the
hypercontractivity property of multiple stochastic integrals (see, e.g.,
[21]). For every p > 2, since XH
r(t) is conditionally a multiple
integral,
Definition 8.2 The process (XH(t))t ≥ 0 from Theorem NaN will be
called the Hermite Multifractal Random Walk (Hermite MRW or
HMRW).
Remark 8.3 The limit as r → 0 is the above construction in order
to reproduce the multifractal character described by the scaling
property (8.1).
\n\n=== OCR PAGE 267 ===\n2

t
E|x"() —X2 (yn) =E (/ (Q,(u) — Q.(wdZ0u))
0

t 2
=E (/ (Q,(w) — 0,(wd8"w)
0
t 2 t 2
=E (/ 0,(w"w -E (/ no)
0 0

> 149 9-

1
Throughout, we denote by BY a fBm with H> 2 (which could be Z*;;)
and the last convergence to 0 is a consequence of the proof of
Proposition 3.1 in [1] under assumption (8.24).
If p = 2, we need a different argument based on the
hypercontractivity property of multiple stochastic integrals (see, e.g.,
[21]). For every p > 2, since X",(¢) is conditionally a multiple
integral,

E |x" (1) — x2)"

e

t rt 2]2
< cnn ( [ [ (Q,(u) — Q,,(u))(O,(v) — Q,,(r)) lu — Hau) |
0/0

> 39 0.

Definition 8.2 The process (X(t); 5 9 from Theorem NaN will be
called the Hermite Multifractal Random Walk (Hermite MRW or
HMRW).

Remark 8.3 The limit as r — 0 is the above construction in order
to reproduce the multifractal character described by the scaling

property (8.1).
\n\n=== PAGE 268 ===\n(8.25)
8.3.2 PROPERTIES OF THE HERMITE MULTIFRACTAL
RANDOM WALK
The first observation is that XH is a process with stationary
increments. Indeed, for any h, r > 0, since Qr is stationary and Z has
stationary increments,
where = (d) stands for the equivalence of finite dimensional
distributions. Taking the limit as r → 0, we obtain the stationarity of
the increments of the process XH.
Other properties can be obtained by using the results in the
fractional MRW case and from the fact that the covariance and the
scaling properties of XH are independent of the order k ≥ 1. Let us list
these properties in the following proposition:
Proposition 8.1 Let XH be the Hermite MRW from Definition 8.2.
a. Denote for every l ≥ 1,
Then as l → ∞,
b. The following scaling property holds
\n\n=== OCR PAGE 268 ===\n8.3.2 PROPERTIES OF THE HERMITE MULTIFRACTAL
RANDOM WALK

The first observation is that X" is a process with stationary
increments. Indeed, for any h, r > 0, since Q, is stationary and Z has
stationary increments,

tth
XM¢+h)—X"(h) = Q,(u)dZ(u)
A
h

Q,(u + h)dZ(u + h)
0

oh
—(d) O,(u)dZ(u)
0

where = stands for the equivalence of finite dimensional
distributions. Taking the limit as r — 0, we obtain the stationarity of
the increments of the process X”.

Other properties can be obtained by using the results in the
fractional MRW case and from the fact that the covariance and the
scaling properties of X¥ are independent of the order k > 1. Let us list
these properties in the following proposition:

Proposition 8.1 Let X" be the Hermite MRW from Definition 8.2.

a. Denote for every 1 = 1,

I, = X"(1+4 1A) — XM (IA).
Then as 1— ~,

Elly ~ ay, XtPt. (8.25)
b. The following scaling property holds
\n\n=== PAGE 269 ===\n(8.26)
(8.27)
where Ωa denotes a random variable independent by XH that
satisfies (8.19).
c. Under (8.24), it holds that
for every fixed t and for every p ≥ 2. Moreover, for every t > s,
and q ≥ 2
Proof: As we mentioned, the proof follows the arguments in the case
of the MRW driven by the fractional Brownian motion, due to the
fact that the covariance structure of the Hermite process is the same
for all k ≥ 1. For the sake of completeness, let us give some details. In
order to compute 
, we notice
that we can formally write XH(t) = ∫t
0Q0(u)dZ(u), where Q0 satisfies
(8.15) with r = 0. Therefore, with αH = H(2H − 1), we have
and point a. follows by noting that (|l + 1|2H + |l − 1|2H − 2|l|2H)
converges as H(2H − 1)l2H − 2 when l goes to infinity. Let us justify
(8.27). This follows from the stationarity of the increments of XH and
point b. above. Indeed,
\n\n=== OCR PAGE 269 ===\n(X" (at)),epo,1y =? ate X" ()) 10,1] (8.26)
where Q,, denotes a random variable independent by X® that
satisfies (8.19).

c. Under (8.24), it holds that

E|xX"(n)|” < 00

for every fixed t and for every p = 2. Moreover, for every t > s,
andq=2

E |X"(t) — XM(s)[? = |r — s|9PO EX" (I. (8.27)

Proof: As we mentioned, the proof follows the arguments in the case
of the MRW driven by the fractional Brownian motion, due to the
fact that the covariance structure of the Hermite process is the same
for all k > 1. For the sake of completeness, let us give some details. In
order to compute Eljly = E(X"((I + 1)A) — X4UA))X"(A), we notice

that we can formally write X(t) = f',Q,(w)dZ(u), where Q, satisfies
(8.15) with r = 0. Therefore, with a; = H(2H — 1), we have

(41)
Elly = ay | EQ,(u)Qo(v)\u — ve? dudv
IA

41 pl
2H + 2H-2
= ay, A~ Ju—v|" “dudv
fi 0

ay 7H 2H 2H 2H
= (4 1) 4 [2-1 — 211°
Seay (ll + 1PM + 1! — 2")
and point a. follows by noting that (|/ + 1]? + |J- 1|27 — 2|1|2%)
converges as H(2H — 1)/?4 - 2 when 1 goes to infinity. Let us justify

(8.27). This follows from the stationarity of the increments of X” and
point b. above. Indeed,
\n\n=== PAGE 270 ===\nusing (8.19).
8.4 Financial applications
Let us now describe our numerical results. Although the theoretical
results in Section 8.3 have been obtained for a general Hermite
process of order k ≥ 1, we will restrict our numerical study to the
cases k = 1 and k = 2. As mentioned before, the case k = 1
corresponds to the fractional Brownian motion while for k = 2 the
driving process is a Rosenblatt process. While fBm is a Gaussian
process, the Rosenblatt process is not Gaussian and its numerical
analysis is more complex and have been developed quite recently.
Our restriction to the cases k = 1, 2 comes from the difficulty to
simulate a Hermite process a order k ≥ 3; therefore, the results in
Section 8.3 remain at this point only theoretical for k ≥ 3. The
simulation of a general Hermite process is still an open problem.
Our purpose is to make here a comparative numerical case between
the Gaussian case (meaning that the integrator in (8.2) is the
fractional Brownian motion) and the non-Gaussian case (when the
integrator in (8.2) is a Rosenblatt process). We will put in light the
advantages and the weak points of each model and we hope in this
way to give to practitioners the possibility to choose their own model
consistent with the data taken into account.
8.4.1 SIMULATION OF THE HMRW
Before discussing how the fractional or Rosenblatt MRW compares
with the high-frequency financial data, we will explain the main
ingredients to simulate these processes. While the MRW based on
fractional Brownian motion has been already simulated (see [1, 3] or
[15] for related results), the numerical study of the MRW based on
the Rosenblatt process is completely new. Our R code is freely
available and documented upon request.
The simulation of the object (8.2) contains three steps:
\n\n=== OCR PAGE 270 ===\nE|x"(t) —X"(s)|? = E |x" (e-s)|*
= |t _ s|PO+4GE|X4( 1 |2

using (8.19).

8.4 Financial applications

Let us now describe our numerical results. Although the theoretical
results in Section 8.3 have been obtained for a general Hermite
process of order k > 1, we will restrict our numerical study to the
cases k = 1and k = 2. As mentioned before, the case k = 1
corresponds to the fractional Brownian motion while for k = 2 the
driving process is a Rosenblatt process. While fBm is a Gaussian
process, the Rosenblatt process is not Gaussian and its numerical
analysis is more complex and have been developed quite recently.
Our restriction to the cases k = 1, 2 comes from the difficulty to
simulate a Hermite process a order k = 3; therefore, the results in
Section 8.3 remain at this point only theoretical for k = 3. The
simulation of a general Hermite process is still an open problem.

Our purpose is to make here a comparative numerical case between
the Gaussian case (meaning that the integrator in (8.2) is the
fractional Brownian motion) and the non-Gaussian case (when the
integrator in (8.2) is a Rosenblatt process). We will put in light the
advantages and the weak points of each model and we hope in this
way to give to practitioners the possibility to choose their own model
consistent with the data taken into account.

8.4.1 SIMULATION OF THE HMRW

Before discussing how the fractional or Rosenblatt MRW compares
with the high-frequency financial data, we will explain the main
ingredients to simulate these processes. While the MRW based on
fractional Brownian motion has been already simulated (see [1, 3] or
[15] for related results), the numerical study of the MRW based on
the Rosenblatt process is completely new. Our R code is freely
available and documented upon request.

The simulation of the object (8.2) contains three steps:
\n\n=== PAGE 271 ===\n(8.28)
The simulation of the noise Y, which is a fBm or a Rosenblatt
process,
the simulation of the integrand Q, which is an IDC (Infinite
Divisible cascade), and
the simulation of the stochastic integral (8.2).
A first step is to simulate the noise, that is, the fBm or the Rosenblatt
process. Since the methodology to simulate the trajectory of a
fractional Brownian motion is pretty well-known nowadays (actually,
in our work, we adopt the approach in [35] to simulate a stationary
Gaussian sequence), we will focus on the case of the Rosenblatt
process.
The numerical analysis of the Rosenblatt process is recent. One
possible approach is to use the approximation of the Rosenblatt
process by two-dimensional perturbed random walks proven in [31].
This is based on the fact that the sequence
where the kernel F is given by
(K is the usual kernel of the fBm that allows the representation BH
t =
∫t
0KH(t, s)dWs and 
), convergence weakly to the Rosenblatt
process in the Skorohod topology (see [31].)
Even if this simulation approach seems quite natural, the cost is
expensive due to five sums that appear in order to simulate Zn (two
sums, two integrals, and F itself are an integral).
The second approximation (introduced in the paper [33]) is related
to the so-called noncentral limit theorem involving long-range
dependent random variables. Let (ξk, k > 0) be a stationary Gaussian
\n\n=== OCR PAGE 271 ===\n¢ The simulation of the noise Y, which is a fBm or a Rosenblatt
process,

¢ the simulation of the integrand Q, which is an IDC (Infinite
Divisible cascade), and

¢ the simulation of the stochastic integral (8.2).

A first step is to simulate the noise, that is, the fBm or the Rosenblatt
process. Since the methodology to simulate the trajectory of a
fractional Brownian motion is pretty well-known nowadays (actually,
in our work, we adopt the approach in [35] to simulate a stationary
Gaussian sequence), we will focus on the case of the Rosenblatt
process.

The numerical analysis of the Rosenblatt process is recent. One
possible approach is to use the approximation of the Rosenblatt
process by two-dimensional perturbed random walks proven in [31].
This is based on the fact that the sequence

tri] a fh . & (8.28)
Zi (0) = ¥ m / r(e ') dvdu 2,

ij=hisj nyn
where the kernel F is given by
KH H!
(u, y,)
ou Ou

F(t, ¥},2) = QA) 1 6401 Mono | (u, y2)du.

yiVY>

(K is the usual kernel of the fBm that allows the representation B", =
1 _ H+
SioK"(t, s)\dW, and H= “2 ), convergence weakly to the Rosenblatt

process in the Skorohod topology (see [31].)

Even if this simulation approach seems quite natural, the cost is
expensive due to five sums that appear in order to simulate Z” (two
sums, two integrals, and F itself are an integral).

The second approximation (introduced in the paper [33]) is related
to the so-called noncentral limit theorem involving long-range
dependent random variables. Let (&;, k > 0) be a stationary Gaussian
\n\n=== PAGE 272 ===\n(8.29)
(8.30)
sequence with 
, 
 and a correlation structure
 with H ∈ (1/2, 1), it can be shown (see [30] or [13])
that the process defined by
converges in law to the Rosenblatt process Z(t) as n tends to infinity.
Here dn is the normalization constant chosen asymptotically
proportional to 
. More precisely, we have
.
This numerical method requires only to simulate correlated Gaussian
random variables and therefore we can employ the algorithm already
utilized to simulate the fBm (we recall that we employ the standard
circulant matrix idea from [35]). Let us note that using (8.29), we
obtain a convergence in law to the Rosenblatt process. In order to
have a better approximation, we employ an idea from [2]. We can
rewrite the sum (8.29) as
where Yk is understood as a FARIMA(0,k,0) object. Above (ϵt)t a
Gaussian white noise and then the process (Xt)t ≤ 0 given by
is also a FARIMA (0,k,0) object. One can show that we have the same
covariance structure as the fBm. This approximation, introduced in
[2] and using a wavelet-based method, leads us to an almost sure
convergence of (8.30) to the Rosenblatt process (instead of the
convergence in law). Therefore, we have chosen to use an algorithm
based on the sequence (8.30). Since the algorithm is a little bit more
\n\n=== OCR PAGE 272 ===\nsequence with Fe, = 0, Ee, = | anda correlation structure
Fes, = KO” with H © (1/2, 1), it can be shown (see [30] or [13])
that the process defined by

[nt]
on 1 7 (8.29)
Zo => Y(&-")
n k=l
converges in law to the Rosenblatt process Z(t) as n tends to infinity.
Here d, is the normalization constant chosen asymptotically

proportional to E (Lis o. — 1 y More precisely, we have

1/2
=, 2
d, =n (a) .

This numerical method requires only to simulate correlated Gaussian
random variables and therefore we can employ the algorithm already
utilized to simulate the fBm (we recall that we employ the standard
circulant matrix idea from [35]). Let us note that using (8.29), we
obtain a convergence in law to the Rosenblatt process. In order to
have a better approximation, we employ an idea from [2]. We can
rewrite the sum (8.29) as

, ea (8.30)

Zi = yk » (vy; -EY;).
< fel

where Y; is understood as a FARIMA(0,k,0) object. Above (€,); a
Gaussian white noise and then the process (X;); < 9 given by

X,— kX. 4 — Dy jo =e,
is also a FARIMA (0,k,0) object. One can show that we have the same
covariance structure as the fBm. This approximation, introduced in
[2] and using a wavelet-based method, leads us to an almost sure
convergence of (8.30) to the Rosenblatt process (instead of the
convergence in law). Therefore, we have chosen to use an algorithm
based on the sequence (8.30). Since the algorithm is a little bit more

\n\n=== PAGE 273 ===\n(8.31)
complicated, involving wavelets and multiresolution analysis (MRA)
(see [27] for a nice review), we prefer to omit the details (but, as
mentioned, our code is freely available).
Once the trajectory of the fBm or the Rosenblatt process numerically
obtained, we have to simulate in addition the infinite divisible
cascade noise (IDC in our short notation, denoted by Q in our
chapter) in order to obtain the MRW. We briefly explain how we
proceed. Basically, when the random measure M has normal
distribution, Qr (which is given by formula (8.13)) follows a
lognormal distribution. A crucial step in the simulation is to find the
covariance structure of the process log Qr(t), t ≥ 0. From relation
(8.12), we have
where X is a 
 random variable. Hence,
Simple calculations show that
Second, we have chosen the control measure to be 
for 0 < r ≤ 1 [see (8.17)]. Other choices are possible but this
particular case leads us to an exact invariance scaling, that is,
while other choice induces asymptotic invariance scaling
for some constant Kq. Then, for r < |τ| ≤ 1,
\n\n=== OCR PAGE 273 ===\ncomplicated, involving wavelets and multiresolution analysis (MRA)
(see [27] for a nice review), we prefer to omit the details (but, as
mentioned, our code is freely available).

Once the trajectory of the {Bm or the Rosenblatt process numerically
obtained, we have to simulate in addition the infinite divisible
cascade noise (IDC in our short notation, denoted by Q in our
chapter) in order to obtain the MRW. We briefly explain how we
proceed. Basically, when the random measure M has normal
distribution, Q,. (which is given by formula (8.13)) follows a
lognormal distribution. A crucial step in the simulation is to find the
covariance structure of the process log Q,(t), t = 0. From relation

(8.12), we have

x

p(q) = — log Ee*

N(u,02)

where X isa random variable. Hence,

Ee
)= — gp(1) = — log { —— }.
9(4) = P(g) — ap) og (4 ~)
Simple calculations show that

o (8.31)
P(g) = zal —q).

Second, we have chosen the control measure to be dm(t, r) = pdrdt
for 0 < r <1 [see (8.17)]. Other choices are possible but this
particular case leads us to an exact invariance scaling, that is,
E[|X(t) — X(t—2)["]= Kt, g=1,2,...,
while other choice induces asymptotic invariance scaling
E[|X(t) — X(t—r)|"] ~ Kt, g=1,2,...,

for some constant K,. Then, for r < |t| < 1,
\n\n=== PAGE 274 ===\n(8.32)
(8.33)
(8.34)
(8.35)
Finally, using (8.31), (8.32), and (8.15), we have for any r ≤ τ ≤ 1
Since the IDC is normalized, the expectation of the process is, for
every r, t,
Now, the trajectory of Qr can be easily simulated (as the exponential
of a Gaussian process with covariance (8.33)). Of course, there are
several parameters to take into account (r, σ2
G, and c) and we will
explain below how these parameters are chosen or estimated.
The last step is to simulate the MRW that is defined as the integral of
Q with respect to the noise. This integral, which behaves as a Wiener
integral, can be naturally approximated by standard Riemann sums.
That is, the MRW driven by a Hermite process ZH of self-similarity
parameter H ∈ (1/2, 1) will be approximated by standard Riemann
sums given by
We give below some realizations of MRW driven by a Rosenblatt
process on Figure 8.1 for different value of H, σG = 0.15 and c = 1.
Empirically, the difference between the Rosenblatt MRW and fBm-
based MRW is not obvious and we will make a deeper numerical
analysis in order to distinguish their properties.
\n\n=== OCR PAGE 274 ===\nm(C,(t) A C(t + T)) = —c log |r]. (8.32)
Finally, using (8.31), (8.32), and (8.15), we have for anyr<t<1

Cov(log(Q,(s)), log(Q,(t + 7))) = —coz, log(|z]). (8.33)

Since the IDC is normalized, the expectation of the process is, for
every r, t,

E log(Q,(t)) = 0. (8.34)

Now, the trajectory of Q,. can be easily simulated (as the exponential
of a Gaussian process with covariance (8.33)). Of course, there are
several parameters to take into account (7, 0°g, and c) and we will
explain below how these parameters are chosen or estimated.

The last step is to simulate the MRW that is defined as the integral of
Q with respect to the noise. This integral, which behaves as a Wiener
integral, can be naturally approximated by standard Riemann sums.
That is, the MRW driven by a Hermite process Z” of self-similarity
parameter H €& (1/2, 1) will be approximated by standard Riemann
sums given by

“4 (8.35)
X(t) = Yi O,(sZ"(s) - Zs -r)).

s=l
We give below some realizations of MRW driven by a Rosenblatt
process on Figure 8.1 for different value of H, og = 0.15 and c = 1.
Empirically, the difference between the Rosenblatt MRW and fBm-
based MRW is not obvious and we will make a deeper numerical
analysis in order to distinguish their properties.
\n\n=== PAGE 275 ===\nFIGURE 8.1 Paths of MRW driven by a Rosenblatt process, from
top left to bottom right, H = 0.501; 6; 7; 8; 9, and 0.99.
If X is an MRW driven by a standard Brownian motion, the
probability distribution of the increments X(t) − X(t − τ) changes
according to the considered lag τ (this is because the MRW satisfies
the so-called Castaing equation, see, e.g., [10]). We present in Figure
8.2 the estimation of the probability density in the fBm and
Rosenblatt cases and a common example of real-world data (the
financial index S P 500). As we can see, in both cases, we keep this
property (meaning the law of the increment changed with the lag)
and thus in principle, the extended MRW (fBm or Rosenblatt) can be
used to model high-frequency financial data. The MRW process can
adapt to any frequency and, in particular, to high-frequency
sampling. For the fBm we plot in Figure 8.2, the probability
distribution for H = 0.5, which is a standard Brownian motion and
for H = 0.56 (this value is chosen because in the next section, we
study in detail the particular case of the stock price for Netflix for
which we found after estimation H = 0.56). For the Rosenblatt case,
also for the Netflix index, we found after estimation H = 0.52 and we
plot the empirical density for this value. As we can see, for this case
and more generally, when H is close to 0.5, we have a positive skew.
Actually, this is more or less expected since equation (8.30) indicated
that the Rosenblatt process is somehow a shifted chi-square
distribution, then, since the IDC and the noise are independent, the
density of the HMRW is simply the product of the two densities.
Note that this is not obvious but for H → 1, the Rosenblatt process
\n\n=== OCR PAGE 275 ===\nFs T g
8] &
8 1 8
8 s| g
0.0 02 04 06 08 1.0 "0.0 02 04 o6 os 1.0 0.0 02 04 06 08 1.0
Ss “4
g
1 8
3
of
0.0 02 04 06 08 1.0 0.0 02 04 06 os 1.0 0.0 02 04 06 os 1.0
FIGURE 8.1 Paths of MRW driven by a Rosenblatt process, from

top left to bottom right, H = 0.501; 6; 7; 8; 9, and 0.99.

If X is an MRW driven by a standard Brownian motion, the
probability distribution of the increments X(t) — X(t — tT) changes
according to the considered lag t (this is because the MRW satisfies
the so-called Castaing equation, see, e.g., [10]). We present in Figure
8.2 the estimation of the probability density in the fBm and
Rosenblatt cases and a common example of real-world data (the
financial index S&P 500). As we can see, in both cases, we keep this
property (meaning the law of the increment changed with the lag)
and thus in principle, the extended MRW (fBm or Rosenblatt) can be
used to model high-frequency financial data. The MRW process can
adapt to any frequency and, in particular, to high-frequency
sampling. For the fBm we plot in Figure 8.2, the probability
distribution for H = 0.5, which is a standard Brownian motion and
for H = 0.56 (this value is chosen because in the next section, we
study in detail the particular case of the stock price for Netflix for
which we found after estimation H = 0.56). For the Rosenblatt case,
also for the Netflix index, we found after estimation H = 0.52 and we
plot the empirical density for this value. As we can see, for this case
and more generally, when H is close to 0.5, we have a positive skew.
Actually, this is more or less expected since equation (8.30) indicated
that the Rosenblatt process is somehow a shifted chi-square
distribution, then, since the IDC and the noise are independent, the
density of the HMRW is simply the product of the two densities.
Note that this is not obvious but for H — 1, the Rosenblatt process
\n\n=== PAGE 276 ===\nhas a nonshifted density function (see [33]), which will be better for
financial comparison, but the counterpart is to increase long memory
in log-returns.
FIGURE 8.2 From top left to bottom right: Empirical density of
S&P 500, with (from top to bottom) τ = 15 s, 30 min, 4 h, and 1 day;
empirical density of the MRW driving by a fBm with the following
values of τ from top to bottom: τ = 10− 4, 10− 2, 1, 102, and H = 0.5
(and σ2= 0.02), fBm with H = 0.56 and with a Rosenblatt process
with H = 0.52. In any cases, the black line corresponds to the
Gaussian prediction. We used the logarithmic scale.
\n\n=== OCR PAGE 276 ===\nhas a nonshifted density function (see [33]), which will be better for
financial comparison, but the counterpart is to increase long memory
in log-returns.

18-03 18-01 T9401 10103 19405

te07 0-04 to ten tet
,
:
;
3

H
a, eR EO ae fa, 7 eas <a
3
DW»
?
FIGURE 8.2 From top left to bottom right: Empirical density of

S&P 500, with (from top to bottom) t = 15 s, 30 min, 4 h, and 1 day;
empirical density of the MRW driving by a fBm with the following
values of t from top to bottom: t = 10° 4, 107 ?, 1, 10°, and H = 0.5
(and o?= 0.02), fBm with H = 0.56 and with a Rosenblatt process
with H = 0.52. In any cases, the black line corresponds to the
Gaussian prediction. We used the logarithmic scale.
\n\n=== PAGE 277 ===\n8.4.2 FINANCIAL STATISTICS
The main statistical properties of the MRW driven by the standard
Brownian motion are preserved when the driving process is replaced
by a Hermite process (and in particular by a Rosenblatt process or by
a fractional Brownian motion). Recall that the Hermite process is
defined for 
. In this case, its increments (which corresponds to
the financial returns) are positively correlated, meaning that an
upward jump is followed by another upward jump. This situation is
most frequently observed in the high-frequency financial data and
with liquid badly arbitrated stock. When H becomes closer and closer
to 1/2, the correlation of the increments of the our process is close to
0 and this can be observed in our empirical study.
In the previous section, we described the algorithm to simulate the
MRW driven by the fBm and by the Rosenblatt process. In this
numerical analysis, several parameters appear: these are the self-
similarity index H, the parameters c, σG, and r that are involving in
the definition of Q (see Section 8.2).
First notice that the parameter c is always linked to parameter σG,
see (8.33). The parameter c appears only in the covariance structure
of Q and it is associated to the volatility σG. Without loss of
generality, we can assume c = 1 and then, cσ2
G = σG
2. On the other
hand, r is chosen to be close to 0, we precisely consider r = 10− 8.
This reduces the number of parameters to estimate, which are now H
and σG defined above. We will explain how these parameters are
estimated in practice using the theoretical results proven in our
chapter.
So, the set of parameters is reduced to {σG, H}. We will not complete
an estimation theory and asymptotic properties in this general
situation (this is beyond the purpose of our work), but we can easily
find an empirical approximation.
The model has to satisfy the major stylized facts, that is, the
autocorrelation of log-return R1 and of the squared log-return R2 and
the multifractal character with spectrum ζ(q); see (8.27). These
quantities are, respectively, defined by
\n\n=== OCR PAGE 277 ===\n8.4.2 FINANCIAL STATISTICS

The main statistical properties of the MRW driven by the standard
Brownian motion are preserved when the driving process is replaced
by a Hermite process (and in particular by a Rosenblatt process or by
a fractional Brownian motion). Recall that the Hermite process is

defined for H> 2. In this case, its increments (which corresponds to
the financial returns) are positively correlated, meaning that an
upward jump is followed by another upward jump. This situation is
most frequently observed in the high-frequency financial data and
with liquid badly arbitrated stock. When H becomes closer and closer
to 1/2, the correlation of the increments of the our process is close to
o and this can be observed in our empirical study.

In the previous section, we described the algorithm to simulate the
MRW driven by the {Bm and by the Rosenblatt process. In this
numerical analysis, several parameters appear: these are the self-
similarity index H, the parameters c, 0g, and r that are involving in

the definition of Q (see Section 8.2).

First notice that the parameter c is always linked to parameter og,
see (8.33). The parameter c appears only in the covariance structure
of Q and it is associated to the volatility og. Without loss of
generality, we can assume c = 1 and then, co?g = 0,7. On the other

hand, r is chosen to be close to 0, we precisely consider r = 107 8,
This reduces the number of parameters to estimate, which are now H
and 0g defined above. We will explain how these parameters are
estimated in practice using the theoretical results proven in our
chapter.

So, the set of parameters is reduced to {og, H}. We will not complete

an estimation theory and asymptotic properties in this general
situation (this is beyond the purpose of our work), but we can easily
find an empirical approximation.

The model has to satisfy the major stylized facts, that is, the
autocorrelation of log-return R, and of the squared log-return R, and

the multifractal character with spectrum ¢(q); see (8.27). These
quantities are, respectively, defined by
\n\n=== PAGE 278 ===\n(8.36)
where Cor denotes correlation operator and δτX(t) = X(t + τ) − X(t).
We used a least square estimation procedure; that is, we find the
values minimizing the mean squared error
where 
, 
, and 
 are the empirical values. We have minimized
this sum, using a genetic algorithm.
Note an alternative estimation procedure. Note that the
autocorrelation function of the MRW driven by a Hermite process is
given by (8.25) and this formula depends only on H. Therefore, we
can simply obtain from the empirical autocorrelation the index H.
Concerning σG, recall that we chose a log-normal distribution for the
IDC with 
. From equation (8.27), we have
and this allows to estimate the volatility.
We compared the two approaches. Using the second procedure, the
computing cost is strictly inferior, but we think that the result are
poorest. Therefore, the figures presented in the chapter are based on
the least squares estimation approach.
We will apply our model to the Nasdaq 100 index from January 2 to
March 28, 2013, at a 1-min frequency, meaning about 23,000
observations in the series. One of the most interesting cases is the
asset Netflix, Inc. (NFLX), that we plot in Figure 8.3.
\n\n=== OCR PAGE 278 ===\nR,(k) = Cor(6,X(t),6.X(t+k)), k>O
Ry(k) = Cor(S,X7(t),6,X7(t+k), k>O
log + Dy 16, X(OI2

o(q) =
logt

where Cor denotes correlation operator and 6,X(t) = X(t + 1) — X(t).

We used a least square estimation procedure; that is, we find the
values minimizing the mean squared error

(6¢.H) = arg max {|| (k) — Ri (WIP + ||IRO- RIP (8.36)
+|10(q) - &@)I7}

where ), Ro, and €(q) are the empirical values. We have minimized
this sum, using a genetic algorithm.

Note an alternative estimation procedure. Note that the
autocorrelation function of the MRW driven by a Hermite process is
given by (8.25) and this formula depends only on H. Therefore, we
can simply obtain from the empirical autocorrelation the index H.
Concerning 6g, recall that we chose a log-normal distribution for the

IDC with 0 = yal —® From equation (8.27), we have

a,
C(qg= srl —q)+Hq

and this allows to estimate the volatility.

We compared the two approaches. Using the second procedure, the
computing cost is strictly inferior, but we think that the result are
poorest. Therefore, the figures presented in the chapter are based on
the least squares estimation approach.

We will apply our model to the Nasdaq 100 index from January 2 to
March 28, 2013, at a 1-min frequency, meaning about 23,000
observations in the series. One of the most interesting cases is the
asset Netflix, Inc. (NFLX), that we plot in Figure 8.3.
\n\n=== PAGE 279 ===\nFIGURE 8.3 NFLX from January 2 to March 28, 2013, at 1-min
frequency, 24,832 observations.
After parameter estimation for the HMRW driven by a Rosenblatt
process using (8.36), we found H = 0.52 and σ2
G = 0.02 while for the
HMRW driven by a fBm H = 0.56 our estimation gives and σ2
G = 0.2.
In Figure 8.4, we give the autocorrelation functions of the returns
and of the squared returns and below their corresponding values
obtained via HMRW.
\n\n=== OCR PAGE 279 ===\n100 120 140 160 180 200

0 5000 10000 15000 20000 25000
k (1 min)

FIGURE 8.3 NFLX from January 2 to March 28, 2013, at 1-min
frequency, 24,832 observations.
After parameter estimation for the HMRW driven by a Rosenblatt
process using (8.36), we found H = 0.52 and 0°¢ = 0.02 while for the
HMRW driven by a fBm H = 0.56 our estimation gives and 0?¢ = 0.2.
In Figure 8.4, we give the autocorrelation functions of the returns

and of the squared returns and below their corresponding values
obtained via HMRW.
\n\n=== PAGE 280 ===\nFIGURE 8.4 Autocorrelation function of the returns (left) and of
the squared returns (right) of NFLX. In red, the average of the
autocorrelations of 10 paths of the HMRW driven by an fBm (H =
0.56), with squared increments (right) and with standard increments
(left). In green for the HRMW driven by a Rosenblatt process (H =
0.52).
Finally, in Figure 8.5, we trace the multifractal spectrum for q = 0, …,
5. As we can notice, the multifractral nature of the spectrum can be
well observed and the spectrum of HMRW well describes the
spectrum of the asset.
\n\n=== OCR PAGE 280 ===\n0.15

©
So
wo
84
o 2 A
ol, 2S 9-0 on 2 A A

f} by et ST Sy a Sverre
8 oP No 8SY se Ss
wo
o4
5

[) 10 20 30 40

0.00 0.05 0.10 0.15 0.20 0.25 0.30
ni 1 1

S§=O=0=Bx Hz ps0 <gae=8>engn0—BS0—9n0o Bs em omecesenow One

0 10 20 30 40

FIGURE 8.4 Autocorrelation function of the returns (left) and of
the squared returns (right) of NFLX. In red, the average of the
autocorrelations of 10 paths of the HMRW driven by an {Bm (H =
0.56), with squared increments (right) and with standard increments
(left). In green for the HRMW driven by a Rosenblatt process (H =
0.52).

Finally, in Figure 8.5, we trace the multifractal spectrum for q = 0, ...,
5. As we can notice, the multifractral nature of the spectrum can be
well observed and the spectrum of HMRW well describes the
spectrum of the asset.
\n\n=== PAGE 281 ===\nFIGURE 8.5 Multifractal spectrum of NFLX (black dots) and of
HMRW driven by an mbf in red with H = 0.56 and σG = 0.19. q
varies from 0. to 5.
Since the Rosenblatt process is not defined for H = 1/2, our theory
cannot be applied. Nevertheless, taking the limit H close to 1/2, for
example, H = 0.501, we find significant empirical results. See Figure
8.6 for the case of the Electronics Art Inc. (EA) asset price with H =
0.501.
FIGURE 8.6 EA from January 2 to March 28, 2013, at 1 min,
23,139 observations.
In Figure 8.7, we compare the autocorrelations functions and the
spectra of the EA index and those obtained using HMRW.
\n\n=== OCR PAGE 281 ===\n15 20

zeta (q)
1.0

0.5

0.0

q

FIGURE 8.5 Multifractal spectrum of NFLX (black dots) and of
HMRW driven by an mbf in red with H = 0.56 and og = 0.19. q

varies from 0. to 5.

Since the Rosenblatt process is not defined for H = 1/2, our theory
cannot be applied. Nevertheless, taking the limit H close to 1/2, for
example, H = 0.501, we find significant empirical results. See Figure
8.6 for the case of the Electronics Art Inc. (EA) asset price with H =
0.501.

20

$
14 15 16 17 18 19

0 5000 10000 15000 20000
k (1 min)

FIGURE 8.6 EA from January 2 to March 28, 2013, at 1 min,
23,139 observations.

In Figure 8.7, we compare the autocorrelations functions and the
spectra of the EA index and those obtained using HMRW.
\n\n=== PAGE 282 ===\nFIGURE 8.7 Down: autocorrelation of the returns (left) and of the
squared returns (right) from EA. In red, the average of 10 paths of
the HMRW with squared increments (right) and standard
increments (left). Up: Multifractal spectrum of EA (black dots) and
of HMRW (red) with H = 0.501 and σG = 0.19. q varies from 0 to 5.
8.5 Concluding remarks
The multifractal models capture the main stylized facts observed in
empirical data. The multifractality itself, as defined by the property
\n\n=== OCR PAGE 282 ===\n&
By
2
so
2
q
0 10 40 50
k (1 min)
<
3
2
3
0
8
S
2
3
a4 :
0 10 40 50
k (1 min)
6
ee
3
8
Ps
3
°
sty :
° 1 2 3 4
q
FIGURE 8.7 Down: autocorrelation of the returns (left) and of the

squared returns (right) from EA. In red, the average of 10 paths of
the HMRW with squared increments (right) and standard
increments (left). Up: Multifractal spectrum of EA (black dots) and
of HMRW (red) with H = 0.501 and og = 0.19. q varies from 0 to 5.

8.5 Concluding remarks

The multifractal models capture the main stylized facts observed in
empirical data. The multifractality itself, as defined by the property
\n\n=== PAGE 283 ===\n(8.1), became a stylized fact for the financial models. Our primary
purpose here was to enlarge the class of multifractal models and in
this way to enlarge the modelling tool kit. We constructed models of
the form (8.2), which are integrals of an Infinite Divisible Cascade Q
with respect to a Hermite process of order k. The most interesting
cases are k = 1 (the integrator in (8.2) is a fractional Brownian
motion) and k = 2 (the integrator in (8.2) is the Rosenblatt process).
We show in Section 8.3 that these integrals are well-defined and they
present the multifractal structure; see Proposition NaN in Section
8.3.
We explain in Section 8.2 how the object (8.2) can be simulated, for
k = 1 and k = 2. While the fBm case k = 1 was already studied before,
the algorithm to simulate the MRW based on the Rosenblatt noise is
new. As explained in the first part of Section 8.3, in the case k = 2,
the algorithm is more complex, because of the non-Gaussian
structure of the Rosenblatt process. Also, in Section 8.4, we compare
these models with the real-world data in the high-frequency context.
The main conclusion of our numerical analysis is the following: both
models (obtained for k = 1 and k = 2) capture some of the stylized
facts, such as autocorrelation of the returns or of the squared returns
or the multifractal spectrum. On the other hand, in the Rosenblatt
model with H close to one half, the probability distribution presents
a skewness, which is not the case for financial data. This skewness is
less visible when H is close to 1.
Taking into account the above remarks and the complexity of the
algorithm, the choice of the fBm instead of a Rosenblatt process
appears to be much more appropriate in general. Nevertheless, the
Rosenblatt process with a self-similarity index could be used in
situations when the non-Gaussian character of the data appears to be
more pronounced.
References
1. P. Abry, P. Chainais, L. Coutin, and V. Pipiras (2009):
Multifractal random walks as fractional Wiener integrals. IEEE
Transactions on Information Theory, 55(8), 3825–3846.
\n\n=== PAGE 284 ===\n2. P. Abry and V. Pipiras (2006): Wavelet-based synthesis of the
Rosenblatt process. Signal Processing, 86, 2326–2339.
3. E. Bacry, L. Duvernet, and J.F. Muzy (2012): Continuous-time
skewed multifractal processes as a model for financial returns.
Journal of Applied Probability, 49, 482–502.
4. E. Bacry and J.-F. Muzy (2002): Multifractal stationary random
measures and multifractal walks with log-infinitely divisible
scaling laws. Physical Review E, 66, 056121.
5. E. Bacry, A. Kozhemyak, and J.-F. Muzy (2008): Continuous
cascade models for asset returns. Journal of Economic Dynamics
and Control, 32(1), 156–199.
6. E. Bacry, J.F. Muzy, and J. Delour (2001): Multifractal random
walks. Physical Review E, 64, 026103-026106.
7. J.M. Bardet, G. Lang, G. Oppenheim, A. Philippe, and M. Taqqu
(2003): Generators of long-range dependent processes: A
survey. Long-range Dependence: Theory and Applications,
Birkhauser.
8. J. Barral and B. Mandelbrot (2002): Multifractal products of
cylindrical pulses. Probability Theory and Related Fields, 124,
409–430.
9. L. Calvet and A. Fisher (2008): Multifractal Volatility: Theory,
Forecasting and Princing. Cambridge University Press.
10. B. Castaing (1996): The temperature of turbulent flows. Journal
de Physique II France, 6, 105–114.
11. P. Chainais, R. Riedi, and P. Abry (2005): On non-scale invariant
infinitely divisible cascades. IEEE Transactions on Information
Theory, 51(3), 1063–1083.
12. A. Chakraborti, I. Muni Toke, M. Patriarca, and F. Abergel (2011):
Econophysics review: I. Empirical Facts. Quantitative Finance,
11(7), 991–1012.
13. R.L. Dobrushin and P. Major (1979): Non-Central Limit
Theorems for Non-Linear Functionals of Gaussian Fields.
Zeitschrift fr Wahrscheinlichkeitstheorie und Verwandte Gebiete,
50(N1), 27–52.
14. R.F. Engle (1982): Autoregressive conditional heteroskedasticity
with estimates of the variance of the U.K. inflation.
Econometrica, 987–1108.
15. A. Fauth and C.A. Tudor (2012): Multifractal random walks with
fractional Brownian motion via Malliavin calculus. IEEE
\n\n=== PAGE 285 ===\nTransactions on Information Theory, 60 (2014), 3, 1963–1975.
16. A. Fisher, L. Calvet, and B. Mandelbrot (1997): Multifractality of
Deutschemark/US Dollar Exchange Rates. Cowles Foundation
Discussion Paper No. 1166.
17. S. Ghashghaie, W. Breymann, J. Peinke, P. Talkner, and Y. Dodge
(1996): Turbulent Cascades in Foreign Exchange Markets.
Nature, 381, 767–770.
18. P. Gopikrishnan, V. Plerou, L.A. Amaral, M. Meyer, and H.E
Stanley (1999): Scaling of the Distribution of Fluctuations of
Financial Market Indices. Physical Review E, 60, 5305–5316.
19. F. Lillo and J. D. Farmer (2004): The long memory of the
efficient market. Studies in Nonlinear Dynamics  Econometrics,
8(3), 1558–3708.
20. C. Ludena (2008): Lpvariations for multifractal fractional
random walks. Annals of Applied Probability, 18(3), 1138–1163.
21. Major, P. (2005). Tail behavior of multiple random integrals
andU-statistics. Probability Surveys, 2, 448–505.
22. B. Mandelbrot (1963): The variation of certain speculative prices.
Journal of Business, 36, 394.
23. J.F. Muzy, J. Delour, and E. Bacry. (2000): Modeling
fluctuations of financial time series: from cascade process to
stochastic volatility model. European Physical Journal B, 17,
537–548.
24. D. Nelson (1991): Conditional heteroskedasticity in asset
returns: A new approach. Econometrica, 59, 347–370.
25. D. Nualart (2006): The Malliavin Calculus and Related Topics.
Second Edition, Springer.
26. M. Maejima and C.A. Tudor (2007): Wiener integrals and a non-
central limit theorem for Hermite processes. Stochastic Analysis
and Applications, 25(5), 1043–1056.
27. S. Mallat (2008): A Wavelet Tour of Signal Processing, Academic
Press, 3rd edition, 2008.
28. V. Pipiras and M. Taqqu (2001): Integration questions related to
the fractional Brownian motion. Probability Theory and Related
Fields, 118(2), 251–281.
29. M. Rosenblatt (1961): Independence and dependence.
Proceedings of 4th Berkeley Symposium on Mathematics and
Statistics, II, 431–443.
\n\n=== OCR PAGE 285 ===\n16.

17.

18.

19.

20.
21.
22.

23.

24.
25.
26.

27.
28.

29.

Transactions on Information Theory, 60 (2014), 3, 1963-1975.
A. Fisher, L. Calvet, and B. Mandelbrot (1997): Multifractality of
Deutschemark/US Dollar Exchange Rates. Cowles Foundation
Discussion Paper No. 1166.

S. Ghashghaie, W. Breymann, J. Peinke, P. Talkner, and Y. Dodge
(1996): Turbulent Cascades in Foreign Exchange Markets.
Nature, 381, 767-770.

P. Gopikrishnan, V. Plerou, L.A. Amaral, M. Meyer, and H.E
Stanley (1999): Scaling of the Distribution of Fluctuations of
Financial Market Indices. Physical Review E, 60, 5305-5316.

F. Lillo and J. D. Farmer (2004): The long memory of the
efficient market. Studies in Nonlinear Dynamics & Econometrics,
8(3), 1558-3708.

C. Ludena (2008): LPvariations for multifractal fractional
random walks. Annals of Applied Probability, 18(3), 1138-1163.
Major, P. (2005). Tail behavior of multiple random integrals
andU-statistics. Probability Surveys, 2, 448-505.

B. Mandelbrot (1963): The variation of certain speculative prices.
Journal of Business, 36, 394.

J.F. Muzy, J. Delour, and E. Bacry. (2000): Modeling
fluctuations of financial time series: from cascade process to
stochastic volatility model. European Physical Journal B, 17,
537-548.

D. Nelson (1991): Conditional heteroskedasticity in asset
returns: A new approach. Econometrica, 59, 347-370.

D. Nualart (2006): The Malliavin Calculus and Related Topics.
Second Edition, Springer.

M. Maejima and C.A. Tudor (2007): Wiener integrals and a non-
central limit theorem for Hermite processes. Stochastic Analysis
and Applications, 25(5), 1043-1056.

S. Mallat (2008): A Wavelet Tour of Signal Processing, Academic
Press, 3rd edition, 2008.

V. Pipiras and M. Taqqu (2001): Integration questions related to
the fractional Brownian motion. Probability Theory and Related
Fields, 118(2), 251-281.

M. Rosenblatt (1961): Independence and dependence.
Proceedings of 4th Berkeley Symposium on Mathematics and
Statistics, II, 431-443.
\n\n=== PAGE 286 ===\n30. M. Taqqu (1975): Weak convergence to the fractional Brownian
motion and to the Rosenblatt process. Zeitschrift fr
Wahrscheinlichkeitstheorie und Verwandte Gebiete, 31, 287–
302.
31. S. Torres and C.A. Tudor (2009): Donsker type theorem for the
Rosenblatt process and a binary market model. Stochastic
Analysis and Applications, 27(3), 555–573.
32. C.A. Tudor (2006): Analysis of the Rosenblatt process. ESAIM
Probability and Statistics, 12, 230–257. MR2374640
33. M.S. Veillette and M.S. Taqqu (2013): Properties and numerical
evaluation of the Rosenblatt distribution. Bernoulli, 19(3), 982–
1005.
34. J. Voit (2005): The Statistical Mechanics of Financial Markets.
Springer, 3rd edition, 378.
35. A. Wood and G. Chan (1994): Simulation of stationary Gaussian
processes in[0, 1]d. Journal of Computational and Graphical
Statistics, 3(4), 409–432.
Notes
1 Supported by the CNCS grant PN-II-ID-PCCE-2011-2-0015.
Associate member of the team Samm, Université de Panthéon-
Sorbonne Paris 1.
\n\n=== PAGE 287 ===\nChapter Nine
Interpolating Techniques and Nonparametric
Regression Methods Applied to Geophysical
and Financial Data Analysis
K. Basu1 and Maria C. Mariani2
1Department of Mathematics, Occidental College, Los Angeles, CA,
USA
2Department of Mathematical Sciences, University of Texas at El
Paso, El Paso, TX, USA
9.1 Introduction
In the past few years, researchers have a renewed interest in analyzing
“critical value phenomena,” and in ([12, 15]), the authors applied three
powerful modeling techniques to predict and estimate parameters for
these major-events (major earthquakes or financial market crashes)
efficiently. Powerful modeling techniques for earthquake data can be
found in [7, 22]. In particular, the role of stress transfer leading to
earthquake and the earthquake hazards after the mainshock is discussed
in detail in [20, 21]. There are several models available in literature that
deal with both space and time aspects of the earthquake data such as [19],
although our approach to deal with geophysical and high-frequency
finance data is based on a completely different approach. Instead of
estimating the major earthquake date (i.e., deal with the time series data),
we implemented a spatial analysis where we have fixed the time (in this
case, the year) and we have collected the earthquake data from different
locations of a particular geographical region. On the basis of these data
trends, we applied two different versions of the local weighted regression
method to estimate the magnitude of the earthquake in the same year at a
location whose data were not used in the calculations. The method has
proved to be efficient and useful. We also applied a similar smoothing
technique for a time series data arising in the financial market for five
different financial institutions. In this case, we used four different
versions of weighted local regression curves that fitted very well the
financial data.
\n\n=== PAGE 288 ===\nWe first introduce nonparametric regressions techniques, specially the
local regression method [11]. We focus our work in two different
applications: geophysical data and financial high-frequency data. Spatial
analysis is performed to different data sets corresponding to different
locations by varying the latitude and longitude, to estimate the magnitude
of the earthquake at any given time. The application of these models was
proved to be efficient by looking at the results. A modified local regression
method that is suitable for applications to one-dimensional input data is
introduced, and four different modified versions are applied to the high-
frequency financial data leading to the Bear Stearns crash, which occurred
in mid-March 2008. One of our main goals is to find out any possible
future avenue where we can use these deterministic modeling techniques
along with traditional stochastic models.
In the second part of our work, we present five powerful interpolation
methods for two-dimensional input data. Numerical estimations are
implemented with these interpolation techniques and, in addition, we
present the “goodness of fit” parameters such as sum of squares of error
(SSE) and R-square value. Looking at the parameters, it seems that some
of the techniques produced more promising results than others. This is
probably due to the fact that these techniques are very much dependent
on the local structure of the data. Although the overall application has
opened a new perspective to analyze the data.
The results of our numerical methods applied to both earthquake data
and financial data indicate that the weighted local regression approach
and the interpolation methods are very powerful techniques to extract
information from large-scale data sets. The simplicity yet robustness of
these methods makes them easy to apply and the estimations are very
accurate. In this chapter, we applied these deterministic modeling
approach to look for any alternative modeling endeavor dealing with
similar data sets. We believe that these parameter approximation
methods with the help of all the well-established stochastic processes
analysis can produce better models in the future. In the conclusions, we
discuss the advantages and disadvantages of using these modeling
techniques and indicate future work directions. We recall that in this
work, it is not our intention to perform a statistical data analysis of the
given data, but rather to present an innovative and mathematical model
approach to handle these kinds of data set.
\n\n=== PAGE 289 ===\n9.2 Nonparametric regression models
The traditional nonlinear regression model fits the following model
where β = (β1, β2, …, βp)′ is a vector of parameters to be estimated and xi′
= (x1, x2, …, xk) is a vector of predictors for the ith of n observations; here
we assume that the corresponding errors ϵi for this model are
independently and normally distributed with mean 0 and constant
variance σ2. The function f( · ), relating the average value of the response
y to the predictors, is generally specified in advance, whereas it is a linear
or nonlinear function.
The general nonparametric regression model is written in a similar
manner, but the function f is left unspecified,
Moreover, the object of the nonparametric regression is not to estimate
the parameters, but rather estimate the function f( · ) itself. Most of the
nonparametric regression techniques implicitly assume that the function
f( · ) is a smooth and continuous function, except [16, 17]. In nonlinear
regression, it is a standardized assumption that 
.
An important special case of the general model is nonparametric simple
regression, where there is only one predictor:
The nonparametric simple regression is often termed as “scatterplot
smoothing” because an important application is to tracing a smooth curve
through a scatterplot of y against x.
As it is complicated and difficult to fit a general nonparametric regression
model when there are more than one predictor and to display the fitted
model when there are more several predictors, more restrictive models
have been developed. As an example, we can present additive regression
model,
\n\n=== OCR PAGE 289 ===\n9.2 Nonparametric regression models

The traditional nonlinear regression model fits the following model

¥ =F(B.%') +;

where B = (B,, B., -.., Bp)’ is a vector of parameters to be estimated and x;’
= (X,, Xp, ..., X;) is a vector of predictors for the ith of n observations; here
we assume that the corresponding errors €; for this model are
independently and normally distributed with mean o and constant

variance o°. The function f{ - ), relating the average value of the response
y to the predictors, is generally specified in advance, whereas it is a linear
or nonlinear function.

The general nonparametric regression model is written in a similar
manner, but the function fis left unspecified,

yy = F(x’) + €;
HSK Xige + Nig) FE;

Moreover, the object of the nonparametric regression is not to estimate
the parameters, but rather estimate the function f{ - ) itself. Most of the
nonparametric regression techniques implicitly assume that the function
ft -) is asmooth and continuous function, except [16, 17]. In nonlinear

regression, it is a standardized assumption that €: ~ NID(O, 0°),

An important special case of the general model is nonparametric simple
regression, where there is only one predictor:

Yi =F (a) + €;

The nonparametric simple regression is often termed as “scatterplot
smoothing” because an important application is to tracing a smooth curve
through a scatterplot of y against x.

As it is complicated and difficult to fit a general nonparametric regression
model when there are more than one predictor and to display the fitted
model when there are more several predictors, more restrictive models
have been developed. As an example, we can present additive regression
model,
\n\n=== PAGE 290 ===\nwhere the partial regression functions fj( · ) are assumed to be smooth and
have to be estimated by using the given data. This model is substantially
more restrictive than the general nonparametric model, but less
restrictive than the linear regression model, which assumes that all the
partial regression functions are linear.
There are some variations of the additive regression model, such as
semiparametric models, in which some of the predictors are entered
linearly, for example,
(particularly useful when some of the predictors are factors), and models
in which some predictors enter into interactions, which appears as
higher-dimensional terms in the model, for example,
All of these models extend to generalized nonparametric regression
straightforward. The random and link components are as in the
generalized linear models, with the linear predictor of the generalized
linear model
is replaced, for example, by an unspecified smooth function
in the most general case, or by a sum of smooth partial-regression
functions
for the generalized additive model.
A detailed discussion of all the aforementioned nonparametric models
can be found in [4, 5].
9.2.1 LOCAL POLYNOMIAL REGRESSION
There are several approaches to evaluate the nonparametric regression
models, such as local polynomial regression and smoothing splines. We
\n\n=== OCR PAGE 290 ===\nYi = AAS (Xi) +A) + oe + LKR) + E;

where the partial regression functions nA - ) are assumed to be smooth and

have to be estimated by using the given data. This model is substantially
more restrictive than the general nonparametric model, but less
restrictive than the linear regression model, which assumes that all the
partial regression functions are linear.

There are some variations of the additive regression model, such as
semiparametric models, in which some of the predictors are entered
linearly, for example,

Vp = A+ Pix +A jr) + +A) +E;

(particularly useful when some of the predictors are factors), and models
in which some predictors enter into interactions, which appears as
higher-dimensional terms in the model, for example,

Vp = a+ fpo(%j1.Xj2) +Aj3) + + + AX) + E;

All of these models extend to generalized nonparametric regression
straightforward. The random and link components are as in the
generalized linear models, with the linear predictor of the generalized
linear model

N= A+ Pix + PyXin +o + Beri
is replaced, for example, by an unspecified smooth function

N; =f (Xj, .Xj25 «++ Kix)

in the most general case, or by a sum of smooth partial-regression
functions

4; = a+f, (Xi) +f) + +A (XR)
for the generalized additive model.

A detailed discussion of all the aforementioned nonparametric models
can be found in [4, 5].

9.2.1 LOCAL POLYNOMIAL REGRESSION

There are several approaches to evaluate the nonparametric regression
models, such as local polynomial regression and smoothing splines. We
\n\n=== PAGE 291 ===\nwill discuss the local regression method that is the main focus of this
work.
9.2.1.1 Simple regression
Under simple regression, we are looking to fit the model:
Let us concentrate in evaluating the regression function at a particular
value of x, say focal point, x*. (Finally, we will fit the model at a
representative range of values of x or simply at the n number of
observations, xi.) We proceed to perform a pth-order weighted least-
squares polynomial regression of y on x,
we choose a weight function to give the greatest weight to the
observations that are closest to the focal point,
Let 
 be the scaled distance between the predictor value for
the ith observations, xi, and the focal point. Here h is the half-width of the
window enclosing the observations for the local regression centered at x*.
The fitted value at x*, that is, the estimated height of the regression curve,
is simply 
 (produced conveniently by having centered the predictor
x at the focal point x*).
It is typical to adjust h so that each local regression includes a fixed
proportion s of the data; then, s is called the span of the local-regression
smoother. The larger the span, the smoother the fit; conversely, the larger
the order of the local regression p, the more flexible the smooth.
9.2.1.2 Multiple regression
The nonparametric multiple regression model is
\n\n=== OCR PAGE 291 ===\nwill discuss the local regression method that is the main focus of this
work.
9.2.1.1 Simple regression

Under simple regression, we are looking to fit the model:

Yi =F (a) + €;

Let us concentrate in evaluating the regression function at a particular
value of x, say focal point, x*. (Finally, we will fit the model at a
representative range of values of x or simply at the n number of
observations, x;.) We proceed to perform a pth-order weighted least-

squares polynomial regression of y on x,

y, = at by (x; — x") + by(x; — 2°? He + bfx; — xP + e;

we choose a weight function to give the greatest weight to the
observations that are closest to the focal point,

/3)3 >
w= {G-kPY for kt
0 for |z|>1
(x; — x*)
Let h _ bethe scaled distance between the predictor value for

the ith observations, x;, and the focal point. Here h is the half-width of the
window enclosing the observations for the local regression centered at x*.
The fitted value at x*, that is, the estimated height of the regression curve,
is simply ) ji=a (produced conveniently by having centered the predictor
x at the focal point x*).

It is typical to adjust h so that each local regression includes a fixed
proportion s of the data; then, s is called the span of the local-regression
smoother. The larger the span, the smoother the fit; conversely, the larger
the order of the local regression p, the more flexible the smooth.

9.2.1.2 Multiple regression

The nonparametric multiple regression model is

yi = SR) +E;
=f (Xj1.Xj25 «+s Xig) + E;
\n\n=== PAGE 292 ===\nSimple extension of the local-polynomial approach to multiple regression
is conceptually very simple but can run into practical difficulties.
Followings are the ordered steps to execute the technique:
i. The first step is to define a multivariate neighborhood around the focal
point x*′ = (x*1, x2*, …, x*k)). The default approach for Loess function
is to employ scaled Euclidean distances:
where the zj are the scaled predictors,
Here  is the mean of the jth predictor and sj is its standard deviation.
ii. Weights are defined by using the scaled distances:
where W( · ) is a suitable weight function, such as a tricube, in which h
is the half-width of the window of the neighborhood. As in local
regression, h may be adjusted to define a neighborhood including the
nearest neighborhood [ns] of the focal point (where the square bracket
denotes the box function).
iii. Perform a weighted polynomial regression of y on the x’s; for example,
a local linear fit takes the following form:
The fitted value at x* is then simply 
.
iv. The procedure is repeated for representative combinations of
predictor values to create a regression surface.
9.2.2 LOWESS/LOESS METHOD
Lowess and Loess (locally weighted scatterplot smoothing) are two
strongly related nonparametric regression methods that include multiple
\n\n=== OCR PAGE 292 ===\nSimple extension of the local-polynomial approach to multiple regression
is conceptually very simple but can run into practical difficulties.

Followings are the ordered steps to execute the technique:

i. The first step is to define a multivariate neighborhood around the focal
point x*’ = (x*,, x,*, ..., x*;,)). The default approach for Loess function
is to employ scaled Euclidean distances:

Here *) is the mean of the jth predictor and s; is its standard deviation.

ii. Weights are defined by using the scaled distances:

ww Po x
' h
where W(- ) is a suitable weight function, such as a tricube, in which h
is the half-width of the window of the neighborhood. As in local
regression, h may be adjusted to define a neighborhood including the
nearest neighborhood [ns] of the focal point (where the square bracket
denotes the box function).

iii.

i=

Perform a weighted polynomial regression of y on the x’s; for example,
a local linear fit takes the following form:

¥p = A+ D(X — x7) + by (Xin — 44) Ho + D-H}
The fitted value at x* is then simply 3" = 4,
iv. The procedure is repeated for representative combinations of
predictor values to create a regression surface.

9.2.2 LOWESS/LOESS METHOD

Lowess and Loess (locally weighted scatterplot smoothing) are two
strongly related nonparametric regression methods that include multiple
\n\n=== PAGE 293 ===\nregression models in a k-nearest-based metamodel.“Loess”is a much
generic version of “Lowess”; its name arises in “LOcal regrESSion.” They
are constructed on both the linear and nonlinear least square regressions.
These methods are more powerful and effective for studies in which the
classical regression procedures cannot produce satisfactory results or
cannot be efficiently applied without undue labor. Loess incorporates
much of the simplicity of the linear least squares regression with some
room for nonlinear regression. It works by fitting simple models to
localized subsets of the data to construct a function that describes
pointwise the deterministic part of the variation of data. The main
advantage of this method is that we need no data analyst to determine a
global function of any form to fit a model to the entire data set, only to the
segment of data.
This method involves a lot of increased computation as it is a
computationally intense procedure. But in the modern computational
setup, Lowess/Loess has been designed to take the advantage of current
computational ability to fullest advantage in order to achieve goals not
easily achieved by traditional methods.
A smooth curve through a set of data points obtained with a statistical
technique is called a Loess curve, particularly when smoothed value is
obtained by a weighted quadratic least squares regression over the span of
values of the y-axis scattergram criterion variable. Similarly, the same
process is called Lowess curve when each smoothed value is given by
weighted linear least squares regression over the span, although some
literature presents Lowess and Loess as synonymous. Some key
features of the local regression models are presented in the next
paragraphs.
Lowess/Loess, originally proposed by Cleveland [1] and further improved
by Cleveland and Devlin [2], specifically denoted a method that is also
known as locally weighted polynomial regression. At each point in the
data set, a low-degree polynomial is fitted to a subset of the data, with
explanatory variable values near the point whose response is being
estimated. Weighted least square method is implemented to fit the
polynomial where more weightage is given to the points near the point
whose response is being estimated and less importance to the points
further away. The value of the regression function for the point is then
evaluated by calculating the local polynomial using the explanatory
variable values for that data point. One needs to compute the regression
function values for each of the n data points in order to complete the
\n\n=== PAGE 294 ===\nLowess/Loess processes. Many of the details of these methods, such as
degree of the polynomial model and weights, are flexible.
The subset of data used for each weighted least square fit in Lowess/Loess
is decided by a nearest neighbors algorithm. One can predetermine the
specific input for the process called the “bandwidth” or “smoothing
parameter” that determines how much of the data is used to fit each local
polynomial according to the need. The smoothing parameter α is
restricted between the value 
 and 1, with λ denoting the degree of the
local polynomial. The value of α is the proportion of data used in each fit.
The subset of data used in each weighted least squares fit comprises the
nα points (rounded to the next larger integer) whose explanatory variable
values are closest to the point at which the response is being evaluated.
The smoothing parameter α is named because it controls the flexibility of
the Lowess/Loess regression function. Large values of α produce the
smoothest functions that wiggle the least in response to fluctuations in
the data. The smaller α is, the closer the regression function will conform
to the data, but using a very small value for the smoothing parameter is
not desirable since the regression function will eventually start to capture
the random error in the data. For the majority of the Lowess/Loess
applications, α values are chosen from the range of 0.25–0.5. First- and
second-degree polynomial are used to fit local polynomials to each subset
of data. That means, either a locally linear or quadratic function, are most
useful; using a zero polynomial turns Lowess/Loess into a weighted
moving average. Such a simple model might work well for some situations
and may approximate the underlying function well enough. Higher-
degree polynomials work great in theory, but the Lowess/Loess methods
are based on the idea that any function can be approximated in a small
neighborhood by a low-degree polynomial and simple models can be fit
data easily. High-degree polynomials would tend to overfit the data in
each subset and are numerically unstable, making precise calculation
almost impossible.
As mentioned above, Lowess/Loess methods use the traditional tri-cubed
weight function. However, any other weight function that satisfies the
properties listed in Cleveland [1] could also be taken into consideration.
The process of calculating the weight for a specific point in any localized
subset of data is done by evaluating the weight function at the distance
between the point and the point of estimation, after scaling the distance
so that the maximum absolute distance over all possible points in the
subset of data is exactly one.
\n\n=== OCR PAGE 294 ===\nLowess/Loess processes. Many of the details of these methods, such as
degree of the polynomial model and weights, are flexible.

The subset of data used for each weighted least square fit in Lowess/Loess
is decided by a nearest neighbors algorithm. One can predetermine the
specific input for the process called the “bandwidth” or “smoothing
parameter” that determines how much of the data is used to fit each local
polynomial according to the need. The smoothing parameter a is

(Atl)
restricted between the value» and 1, with A denoting the degree of the
local polynomial. The value of a is the proportion of data used in each fit.
The subset of data used in each weighted least squares fit comprises the
na points (rounded to the next larger integer) whose explanatory variable

values are closest to the point at which the response is being evaluated.

The smoothing parameter a is named because it controls the flexibility of
the Lowess/Loess regression function. Large values of a produce the
smoothest functions that wiggle the least in response to fluctuations in
the data. The smaller a is, the closer the regression function will conform
to the data, but using a very small value for the smoothing parameter is
not desirable since the regression function will eventually start to capture
the random error in the data. For the majority of the Lowess/Loess
applications, a values are chosen from the range of 0.25—0.5. First- and
second-degree polynomial are used to fit local polynomials to each subset
of data. That means, either a locally linear or quadratic function, are most
useful; using a zero polynomial turns Lowess/Loess into a weighted
moving average. Such a simple model might work well for some situations
and may approximate the underlying function well enough. Higher-
degree polynomials work great in theory, but the Lowess/Loess methods
are based on the idea that any function can be approximated in a small
neighborhood by a low-degree polynomial and simple models can be fit
data easily. High-degree polynomials would tend to overfit the data in
each subset and are numerically unstable, making precise calculation
almost impossible.

As mentioned above, Lowess/Loess methods use the traditional tri-cubed
weight function. However, any other weight function that satisfies the
properties listed in Cleveland [1] could also be taken into consideration.
The process of calculating the weight for a specific point in any localized
subset of data is done by evaluating the weight function at the distance
between the point and the point of estimation, after scaling the distance
so that the maximum absolute distance over all possible points in the
subset of data is exactly one.
\n\n=== PAGE 295 ===\n9.2.3 NUMERICAL APPLICATIONS
In this section, we present the numerical simulations performed with
Lowess/Loess methods applied to geophysical data and high-frequency
financial data.
9.2.3.1 Application to geophysics
The geological data was obtained at the U.S. Geological Survey (USGS)
from January 1, 1973, to November 9, 2010 ([23]). The data contains
information about the date, longitude, latitude, and magnitude of each
recorded earthquake in the region.
The location of the major earthquake chosen defines the area studied. The
area chosen cannot be too small (lack of data) or too big (noise from
unrelated events). The data is obtained using a square centered at the
coordinates of the major event. The sides of the square were usually
chosen as ± 0.1°–0.2° in latitude and ± 0.2°–0.4° in longitude. A segment
0.1° of latitude at the equator is ≈ 11.11 km ≈ 6.9 miles in length.
The earthquake magnitude is the recorded data used in the analysis. The
policy of the USGS regarding recorded magnitude is the following [23]:
Magnitude is a dimensionless number between 1 and 12.
The reported magnitude should be moment magnitude, if available.
The least complicated, and probably most accurate terminology is to
just use the term “magnitude” and to use the symbol M.
In the numerical study, we use data collected from different locations at a
given time to estimate the magnitude of the earthquake at a given
location, where the real magnitude is known. The magnitude is recorded
in the data used and where available moment magnitude is used. For
more information, we refer to the specific USGS documentation available
at:
http://earthquake.usgs.gov/aboutus/docs/020204mag_policy.php.
To study the efficiency and accuracy of the Lowess/Loess methods on the
geophysical data set, we have applied both processes to the same data and
listed the estimated magnitude of the earthquake in 10 different
geographical locations. Moreover, the relative errors with respect to its
actual given magnitude were computed to compare the strength between
linear least square local fit (Lowess) versus quadratic least square local fit
(Loess). In the calculation, we used “lowess” and “loess” built in
\n\n=== PAGE 296 ===\nsubroutine under curve fitting tool in Matlab. Results are presented for
five randomly selected years where the magnitude for the earthquake in
different locations is available. In Figures 9.1, 9.2, 9.3, 9.4, and 9.5 below,
we display typical results for the earthquake estimation surface simulated
by the Lowess/Loess techniques in some areas of California. We used data
from 1973, 1979, 1988, 1996, and 2008. Real value data were used to draw
the estimation surface.
\n\n=== PAGE 297 ===\nFIGURE 9.1 Simulation results for the earthquake that happened in the
months of January–April in 1973. For both simulations, we have used 653
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== OCR PAGE 297 ===\n(a) (2 Lowess surface}
© Data
8
7
re
gs
54
= 3
2
(b)
8
7
Ze
es
Ea
3
2
FIGURE 9.1 Simulation results for the earthquake that happened in the

months of January—April in 1973. For both simulations, we have used 653
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== PAGE 298 ===\nFIGURE 9.2 Simulation results for the earthquake that happened in the
months of January–May in 1979. For both simulations, we have used 1139
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== OCR PAGE 298 ===\n(a)

Magnitu

(b) * Data

Magnitude|-]
o

Longitude  -150 “40

FIGURE 9.2 Simulation results for the earthquake that happened in the
months of January—May in 1979. For both simulations, we have used 1139
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== PAGE 299 ===\nFIGURE 9.3 Simulation results for the earthquake that happened in the
months of April–June in 1988. For both simulations, we have used 700
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== OCR PAGE 299 ===\n(a) = Lowes surface]
© Data

Magnitude[-]

Longitude ~150

i (Loess surtace

Magnitude[-}

60 Latitude

FIGURE 9.3 Simulation results for the earthquake that happened in the
months of April—June in 1988. For both simulations, we have used 700
data points that contain the magnitude of the earthquake collected from
different locations. (a) The local regression estimation surface generated
by Lowess method and (b) the local regression estimation surface
generated by Loess method.
\n\n=== PAGE 300 ===\nFIGURE 9.4 Simulation results for the earthquake that happened in the
months of September–October in 1996. For both simulations, we have
used 560 data points that contain the magnitude of the earthquake
collected from different locations. (a) The local regression estimation
surface generated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
\n\n=== OCR PAGE 300 ===\n(a) | Lowess surface
° Data

Magnitude{-]

(b)

Magnitudef-]

FIGURE 9.4 Simulation results for the earthquake that happened in the
months of September—October in 1996. For both simulations, we have
used 560 data points that contain the magnitude of the earthquake
collected from different locations. (a) The local regression estimation
surface generated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
\n\n=== PAGE 301 ===\nFIGURE 9.5 Simulation results for the earthquake that happened in the
months of November–December in 2008. For both simulations, we have
used 700 data points that contain the magnitude of the earthquake
collected from different locations. (a) The local regression estimation
surface generated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
\n\n=== OCR PAGE 301 ===\n(G9 Lowess surface

bd ° Data

Magnitude|-]

(b)

Magnitude-]

~150 -20
Longitude ~e0 “40 Latitude

FIGURE 9.5 Simulation results for the earthquake that happened in the
months of November—December in 2008. For both simulations, we have
used 700 data points that contain the magnitude of the earthquake
collected from different locations. (a) The local regression estimation
surface generated by Lowess method; and (b) the local regression
estimation surface generated by Loess method.
\n\n=== PAGE 302 ===\nFIGURE 9.6 Exxon Mobile Corporation—The solid red line represents
the best fit curve with four different methodology using same data.
\n\n=== OCR PAGE 302 ===\n[Return]

{minutes}

FIGURE 9.6 Exxon Mobile Corporation—tThe solid red line represents
the best fit curve with four different methodology using same data.
\n\n=== PAGE 303 ===\nFIGURE 9.7 The Cal Group (CAL)–The solid red line represents the best
fit curve with four different methodology using same data.
\n\n=== OCR PAGE 303 ===\nt{minutes]

FIGURE 9.7 The Cal Group (CAL)-The solid red line represents the best
fit curve with four different methodology using same data.
\n\n=== PAGE 304 ===\nFIGURE 9.8 JP Morgan Chase—The solid red line represents the best fit
curve with four different methodology using same data.
\n\n=== OCR PAGE 304 ===\nFIGURE 9.8 JP Morgan Chase—The solid red line represents the best fit
curve with four different methodology using same data.
\n\n=== PAGE 305 ===\nFIGURE 9.9 International Business Machines—The solid red line
represents the best fit curve with four different methodology using same
data.
\n\n=== OCR PAGE 305 ===\n[Return]

[Return]

[Return]

[Return]

(minutes)

FIGURE 9.9 International Business Machines—The solid red line
represents the best fit curve with four different methodology using same
data.
\n\n=== PAGE 306 ===\nFIGURE 9.10 MFA Financial, Inc-The solid red line represents the best
fit curve with four different methodologies using same data.
\n\n=== OCR PAGE 306 ===\nFIGURE 9.10 MFA Financial, Inc-The solid red line represents the best
fit curve with four different methodologies using same data.
\n\n=== PAGE 307 ===\n\n\n=== OCR PAGE 307 ===\nwae)

)

Reet

HlopneuBeyy

co}

\n\n=== PAGE 308 ===\nFIGURE 9.11 Simulation results for the earthquake that happened in the
months of January–April in 1973. For all the simulations, we have used
653 data points that contain the magnitude of the earthquake collected
from different locations. The Interpolant estimation surface is generated
by the following: (a) nearest neighborhood method; (b) bilinear method;
(c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== OCR PAGE 308 ===\nMagnitude[-]

FIGURE 9.11 Simulation results for the earthquake that happened in the
months of January—April in 1973. For all the simulations, we have used
653 data points that contain the magnitude of the earthquake collected
from different locations. The Interpolant estimation surface is generated
by the following: (a) nearest neighborhood method; (b) bilinear method;
(c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== PAGE 309 ===\n\n\n=== OCR PAGE 309 ===\n@

@)

\n\n=== PAGE 310 ===\nFIGURE 9.12 Simulation results for the earthquake that happened in
the months of January–May in 1979. For all the simulations, we have
used 1139 data points that contain the magnitude of the earthquake
collected from different locations. The Interpolant estimation surface
generated by the following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== OCR PAGE 310 ===\n+ Real data

(d)

7+ >
ges
$5.
3
|
oO; ~
20> a
Pio 80
80-199
Longitude
e)
e a

-]

Magnitu

: So
160 139 Say 40 Latitude

FIGURE 9.12 Simulation results for the earthquake that happened in
the months of January—May in 1979. For all the simulations, we have
used 1139 data points that contain the magnitude of the earthquake
collected from different locations. The Interpolant estimation surface
generated by the following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== PAGE 311 ===\n\n\n=== OCR PAGE 311 ===\n@

Pm)

neroear

©

\n\n=== PAGE 312 ===\nFIGURE 9.13 Simulation results for the earthquake that happened in
the months of April–June in 1988. For all the simulations, we have used
700 data points that contain the magnitude of the earthquake collected
from different locations. The Interpolant estimation surface generated by
the following: (a) nearest neighborhood method; (b) linear method; (c)
cubic method; (d) biharmonic method; and (e) TPS.
\n\n=== OCR PAGE 312 ===\n=

Magnitude[-]
noe aon

Magnitudel-]
non aon

Latitude

FIGURE 9.13 Simulation results for the earthquake that happened in
the months of April—June in 1988. For all the simulations, we have used
700 data points that contain the magnitude of the earthquake collected
from different locations. The Interpolant estimation surface generated by
the following: (a) nearest neighborhood method; (b) linear method; (c)
cubic method; (d) biharmonic method; and (e) TPS.
\n\n=== PAGE 313 ===\n\n\n=== PAGE 314 ===\nFIGURE 9.14 Simulation results for the earthquake that happened in
the months of September–October in 1996. For all the simulations, we
have used 560 data points that contain the magnitude of the earthquake
collected from different locations. The Interpolant estimation surface
generated by the following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== OCR PAGE 314 ===\ns

Magnitude[-}
vos DON

°

“20g
80 10> ~-
210075 1
Longitude 120-140 5 eat
2160 jag

180

FIGURE 9.14 Simulation results for the earthquake that happened in
the months of September—October in 1996. For all the simulations, we
have used 560 data points that contain the magnitude of the earthquake
collected from different locations. The Interpolant estimation surface
generated by the following: (a) nearest neighborhood method; (b) bilinear
method; (c) bicubic method; (d) biharmonic method; and (e) TPS.
\n\n=== PAGE 315 ===\n\n\n=== OCR PAGE 315 ===\n(@) (+ Real data

O80 eg #0
(b)
6
5
qe
3
5
a
°

= Cy

o ~ ©
2100196 gy 10 Lattude
© = Real data
6

2180" 60
\n\n=== PAGE 316 ===\nFIGURE 9.15 Simulation results for the earthquake that happened in
the months of November–December in 2008. The Interpolant estimation
surface generated by the following: (a) nearest neighborhood method; (b)
blinear method; (c) bicubic method; and (d) biharmonic method.
The data for these figures was measured in the Western Hemisphere (i.e.,
− 180° in longitude). Cross-validation was used to get a better estimate of
the estimation error. Almost 25% of the randomly selected original data
was estimated and only 10 of the random results are presented in the
table format. The entire set of earthquakes analyzed (from 1973, 1978,
1988, 1996, and 2008) is presented in Tables 9.1, 9.2, 9.3, 9.4, and 9.5,
respectively. Each table displays the date of the earthquake, the actual
magnitude, estimated magnitude of the earthquake and finally the
\n\n=== OCR PAGE 316 ===\n(d)

Magnitude[-]
conn oeaa

(e)

Magnitude{-]
os nose

0 7 in
-20 40>
69 So
2100
Longitude —120.449

180180 ~Teo | ~40 Latitude

FIGURE 9.15 Simulation results for the earthquake that happened in
the months of November—December in 2008. The Interpolant estimation
surface generated by the following: (a) nearest neighborhood method; (b)
blinear method; (c) bicubic method; and (d) biharmonic method.

The data for these figures was measured in the Western Hemisphere (i.e.,
— 180° in longitude). Cross-validation was used to get a better estimate of
the estimation error. Almost 25% of the randomly selected original data
was estimated and only 10 of the random results are presented in the
table format. The entire set of earthquakes analyzed (from 1973, 1978,
1988, 1996, and 2008) is presented in Tables 9.1, 9.2, 9.3, 9.4, and 9.5,
respectively. Each table displays the date of the earthquake, the actual
magnitude, estimated magnitude of the earthquake and finally the
\n\n=== PAGE 317 ===\nrelative precision taking into consideration the actual magnitude of the
earthquake.
Table 9.1 Results for earthquakes in 1973.
Estimated
magnitude
Relative
error (%)
Latitude Longitude Actual
(°)
(°)
magnitude Lowess Loess
Lowess Loess
56.825
−153.845
4.00
4.024672 4.021482 0.62
0.54
−20.608
−176.426
4.50
4.861483 4.864919 8.03
8.11
62.148
−144.870
3.60
3.653676 3.654828 1.49
1.52
6.408
−79.262
4.60
4.662425 4.724964 1.36
2.72
−26.854
−177.731
4.60
4.872853 4.867820 5.93
5.82
16.744
−86.502
4.80
4.517581 4.498685 5.88
6.28
28.336
−111.934
4.40
4.028162 3.882251 8.45
11.77
33.583
−117.733
3.60
3.824336 3.646300 6.23
1.29
−18.178
−178.158
4.60
4.843991 4.807303 5.30
4.51
37.423
−121.802
3.60
3.685618 3.571850 2.38
0.78
\n\n=== PAGE 318 ===\nTable 9.2 Results for earthquakes in 1979.
Estimated
magnitude
Relative
error (%)
Latitude Longitude Actual
(°)
(°)
magnitude Lowess
Loess
Lowess Loess
34.932
−116.683
3.30
3.465456 3.403476 5.01
3.14
−27.829
−66.582
4.90
4.880643 4.884803 0.38
0.31
5.224
−75.801
4.90
4.731234 4.638491 3.44
5.39
−25.829
−70.370
4.80
4.887798 4.894319 1.83
1.96
44.947
−111.869
3.60
3.547208 3.386407 1.47
5.93
−15.140
−173.522
5.10
4.825427 4.865640 5.38
4.60
61.735
−149.881
3.40
3.566559 3.534701 4.88
3.96
60.048
−152.263
3.60
3.803186 3.820450 5.64
6.12
−22.684
−175.375
5.10
4.889236 4.903117 4.13
3.86
−9.961
−74.822
4.90
4.814565 4.877262 1.74
0.46
Table 9.3 Results for earthquakes in 1988.
Estimated
magnitude
Relative
error (%)
Latitude Longitude Actual
(°)
(°)
magnitude Lowess
Loess
Lowess Loess
57.002
−142.372
3.90
3.886364 4.132071 0.35
5.95
−21.366
−67.937
4.70
4.712730 5.002906 0.27
6.44
40.445
−124.753
3.30
3.386009 3.303831 2.61
0.12
53.941
−163.268
4.40
4.365138 4.684361 0.79
6.46
51.660
−173.708
4.50
4.602204 4.710451 2.27
4.68
−0.163
−77.793
4.80
4.686259 4.652767 2.37
3.07
−32.382
−69.710
4.50
4.469986 4.168801 0.67
7.36
48.228
−116.387
3.40
3.498185 3.133430 2.89
7.84
57.813
−142.916
4.00
3.882614 4.079561 2.93
1.99
50.966
−177.640
4.70
4.701313 4.603498 0.03
2.05
\n\n=== PAGE 319 ===\nTable 9.4 Results for earthquakes in 1996.
Estimated
magnitude
Relative
error (%)
Latitude Longitude Actual
(°)
(°)
magnitude Lowess
Loess
Lowess Loess
−18.010
−178.542
4.70
4.563134 4.551168 2.91
3.17
49.011
−128.009
3.80
3.866504 4.163363 1.75
9.56
51.719
−179.526
4.60
4.687235 4.637800 1.89
0.82
−20.619
−178.744
4.60
4.562643 4.554294 0.81
0.99
44.549
−110.527
3.30
3.343416 3.083925 1.32
6.55
60.203
−140.855
3.20
3.178109 3.298102 0.68
3.07
11.114
−86.074
4.30
4.462813 4.651453 3.79
8.17
44.804
−111.276
3.40
3.362970 3.179219 1.09
6.49
−32.744
−179.065
4.60
4.524058 4.580376 1.65
0.42
16.710
−98.731
4.50
4.423162 4.591036 1.71
2.03
Table 9.5 Results for earthquakes in 2008.
Estimated
magnitude
Relative
error (%)
Latitude Longitude Actual
(°)
(°)
magnitude Lowess
Loess
Lowess Loess
40.320
−124.595
3.10
3.199942 3.132174 3.22
1.04
54.054
−162.958
3.60
3.554314 3.377265 1.27
6.19
−32.007
−177.174
4.40
4.533094 4.460330 3.02
1.37
59.323
−152.959
3.30
3.215022 3.077414 2.58
6.75
19.016
−64.781
3.30
3.212336 3.197769 2.66
3.10
61.526
−149.953
3.00
3.089869 3.074387 2.99
1.17
65.889
−140.218
3.10
2.989829 3.074387 3.55
0.83
58.534
−155.296
3.40
3.281178 3.118368 3.49
8.28
−59.452
−23.717
4.50
4.457317 4.726484 0.94
5.03
17.310
−100.770
4.20
4.204719 4.359230 0.11
3.79
\n\n=== PAGE 320 ===\n9.2.3.2 Application to financial data sampled with high frequency
In this section, we study the high-frequency data corresponding to the
collapse of the Bear Stearns in March 2008. The data involves the week
(five trading days) March 10–14, 2008, before the merging
announcement over the weekend as well as the two following trading days
March 17 and 18. On Friday, March 14, 2008, at around 9:14 a.m., JP
Morgan Chase and the Federal Reserve Bank of New York announced an
emergency loan (of about 29 billion, term undisclosed) to prevent the firm
from becoming insolvent. This bailout was declared to prevent the very
likely crash of the market as a result of the fall of one of the biggest
investment banks at the time. This measure proved to be insufficient to
keep the firm alive and 2 days later on Sunday March 16, 2008, Bear
Stearns signed a merger agreement with JP Morgan Chase essentially
selling the company for $2 a share (price revised on March 24 to
$10/share). The same stock traded at $172 in January 2007 and $93 a
share in February 2007. Today, this collapse is viewed as the first sign of
the risk management meltdown of investment bank industry in
September 2008 and the subsequent global financial crisis and recession.
In this part of the chapter, we study how to fit different local regression
models (one-dimensional version to deal with time-series data) into high-
frequency data collected for five different financial institutions: JP
Morgan Chase, Exxon Mobile Corporation, The Cal Group, International
Business Machines Corporation, and MFA Financial, Inc. We work with
the stock return value for each of the five financial companies with sample
period of time being T = 1 min.
We have applied the “smooth” subroutine from Matlab with different
given models as mentioned in the figures. The “smooth” function smooths
the given data using a moving average filter, and the results are returned
in the new column vector. The default span for the moving average is 5.
Under different modeling techniques, the smooth function will produce
different smothered data. We have considered the four most popular local
regression models applied for modeling the data arising in five different
financial institutions, as mentioned above; then we have compared the
results provided by the different methods for the same set of data.
In this section, we present the numerical study of the high-frequency data
described in Section 9.2.3.1. We have used Matlab as our computational
software where different subroutines were applied for the different local
regression methods. The results are presented for subroutines “lowess,”
“rlowess,” “loess,” and “rloess” as different smothering techniques. The
results are quite promising even when applied to the random high-
\n\n=== PAGE 321 ===\nfrequency data. The data set we used contains highly random data and the
feedback obtained from the results is the indication of this method
efficiency. Because of the design of robust “r” versions of “lowess” and
“loess,” the results seem to be more desirable than the ones obtained
when using weighted local regression with first- and second-order
polynomial approximations.
The Matlab command yy = smooth(y, “method”) smooths the data in y by
using one of the Lowess/Loess methods and the default span. The
supported description for each method is listed in the table below.
Method Description
“lowess” Local regression using weighted linear least squares and a
first-degree polynomial model.
“loess”
Local regression using weighted linear least squares and a
second-degree polynomial model.
“rlowess” A robust version of “lowess” that assigns lower weight to
outliers in the regression. The method assigns zero weight to
data outside six mean absolute deviations.
“rloess”
A robust version of “loess” that assigns lower weight to outliers
in the regression. The method assigns zero weight to data
outside six mean absolute deviations.
9.2.3.3 Highlights and discussions
In Table. 9.6, we have listed all the results from the year-wise data
analysis. Sometimes it is surprising to observe that Lowess method has
produced a better approximation than its quadratic counterpart (Loess).
This can happen because of different reasons, in order to address this
question accurately, we need to know sufficient background of the data. If
the whole system is more toward steady state then data can behave
linearly or most of the data collection tools are linear in nature. As we do
not have sufficient background for the data set, it is difficult to explain
these results.
\n\n=== PAGE 322 ===\nTable 9.6 Highlights of geophysical data analysis.
Max rel. error (%) Min. rel. error (%)
Year Lowess
Loess
Lowess
Loess
1973 8.45
11.77
0.62
0.54
1979 5.64
6.12
0.38
0.31
1988 2.93
7.84
0.03
0.12
1996 3.79
9.56
0.81
0.42
2008 3.55
8.28
0.11
0.83
Table 9.7 Goodness-of-fit parameters for the data of the year 1973.
Method
SSE R-square
“Nearest-neighbor” 0.31 0.999
“Bilinear”
0.31 0.999
“Bicubic”
0.31 0.999
“Biharmonic”
0.31 0.999
“Thin-plate Spline” 0.31 0.999
Table 9.8 Goodness-of-fit parameters for the data of the year 1979.
Method
SSE R-square
“Nearest-neighbor” 14.14 0.9825
“Bilinear”
14.14 0.9825
“Bicubic”
14.14 0.9825
“Biharmonic”
14.14 0.9825
“Thin-plate Spline” 251
0.6897
Table 9.9 Goodness-of-fit parameters for the data of the year 1988.
Method
SSE R-square
“Nearest-neighbor” 4.18 0.994
“Bilinear”
4.2
0.993
“Bicubic”
4.2
0.993
“Biharmonic”
4.21 0.993
“Thin-plate Spline” 4.18 0.994
\n\n=== PAGE 323 ===\nTable 9.10 Goodness-of-fit parameters for the data of the year 1996.
Method
SSE
R-square
“Nearest-neighbor”
0
1
“Bilinear”
1.059 e-25
1
“Bicubic”
2.078 e-27
1
“Biharmonic”
7.259 e-12
1
“Thin-plate Spline” 1.435 e-10
1
Table 9.11 Goodness-of-fit parameters for the data of the year 2008.
Method
SSE
R-square
“Nearest-neighbor”
0
1
“Bilinear”
9.263 e-26
1
“Bicubic”
3.253 e-28
1
“Biharmonic”
2.414 e-12
1
“Thin-plate Spline” 3.942 e-11
1
9.3 Interpolation methods
In numerical analysis, interpolation is a process for estimating values that
lie within the range of a known discrete set of data points. In engineering
and science, one often has a number of data points, obtained by sampling
or experimentation, which represents the values of a function for a limited
number of values of the independent variable. It is often required to
interpolate (i.e., estimate) the value of that function for an intermediate
value of the independent variable. This may be achieved by curve fitting
or regression analysis.
Another problem that is similar to the interpolation problem is to
approximate complicated functions with the means of simple functions.
Suppose that we know a formula to evaluate a function but it is too
complex to calculate for the given data points. A few known data points
from the original function can be used to create an interpolation based on
a simpler function. Of course, when a simple function is used to estimate
data points from the original, interpolation errors are usually present;
however, depending on the problem, domain, and the interpolation
method that is used, the gain in simplicity may be of greater value than
\n\n=== PAGE 324 ===\nthe resultant loss in accuracy. There is another kind of interpolation in
mathematics called “Interpolation of operators.”
In this work, we have implemented five simple interpolation models to
our geophysical data set that lists the magnitude of earthquake intensities
in different parts of California. Here we will explain the details of our
interpolation methods.
9.3.1 NEAREST-NEIGHBOR INTERPOLATION
Nearest-neighbor interpolation (also known as proximal interpolation) is
a simple method of multivariate interpolation in one or more dimensions.
The nearest neighbor algorithm selects the value of the nearest point and
does not consider the values of neighboring points at all, yielding a
piecewise-constant interpolant. The algorithm is very simple to
implement and is commonly used (usually along with mipmapping) in
real-time 3D rendering to select color values for a textured surface.
9.3.2 BILINEAR INTERPOLATION
In numerical methods, bilinear interpolation is a special technique that is
an extension of regular linear interpolation for interpolation functions of
two variables (i.e., x and y) on a regular two-dimensional grid. The main
idea is to perform linear interpolation first in one direction and then
again in the other direction. Although each step is linear in the sampled
values and in the position, the interpolation as a whole is not linear but
rather quadratic in the sample location. Bilinear interpolation is a
continuous fast method where one needs to perform only two operations:
one is to multiply and the other one to divide. For these methods, bounds
are fixed at extremes.
9.3.2.1 Algorithm
Suppose that we want to find the value of the unknown function f at the
point P = (x, y). It is assumed that we know the value of f at the four
points Q11 = (x1, y1), Q12 = (x1, y2), Q21 = (x2, y1), and Q22 = (x2, y2).
We first do linear interpolation in the x-direction. This yields
where R1 = (x, y1),
\n\n=== OCR PAGE 324 ===\nthe resultant loss in accuracy. There is another kind of interpolation in
mathematics called “Interpolation of operators.”

In this work, we have implemented five simple interpolation models to
our geophysical data set that lists the magnitude of earthquake intensities
in different parts of California. Here we will explain the details of our
interpolation methods.

9.3.1 NEAREST-NEIGHBOR INTERPOLATION

Nearest-neighbor interpolation (also known as proximal interpolation) is
a simple method of multivariate interpolation in one or more dimensions.
The nearest neighbor algorithm selects the value of the nearest point and
does not consider the values of neighboring points at all, yielding a
piecewise-constant interpolant. The algorithm is very simple to
implement and is commonly used (usually along with mipmapping) in
real-time 3D rendering to select color values for a textured surface.

9.3.2 BILINEAR INTERPOLATION

In numerical methods, bilinear interpolation is a special technique that is
an extension of regular linear interpolation for interpolation functions of
two variables (i.e., x and y) on a regular two-dimensional grid. The main
idea is to perform linear interpolation first in one direction and then
again in the other direction. Although each step is linear in the sampled
values and in the position, the interpolation as a whole is not linear but
rather quadratic in the sample location. Bilinear interpolation is a
continuous fast method where one needs to perform only two operations:
one is to multiply and the other one to divide. For these methods, bounds
are fixed at extremes.

9.3.2.1 Algorithm

Suppose that we want to find the value of the unknown function fat the
point P = (x, y). It is assumed that we know the value of fat the four
points Q,, = 0%; Ys), Qyo = My, Yo), Qor = Mo, Yx), aNd Qyo = (Xp; Yo).

We first do linear interpolation in the x-direction. This yields

xy XxX XX.
F(R\) & ——F(Q),) + - —f(Q>))
XX, XY

x

where R, = (x, y,),
\n\n=== PAGE 325 ===\nwhere R2 = (x, y2).
We next proceed by interpolating in the y-direction
This follows the desired estimate of f(x, y).
Here we can note that the same result will be achieved if we execute the y-
interpolation first and x-interpolation second.
9.3.2.2 Unit square
If we select the four points where f is given to be (0, 0), (1, 0), (0, 1), and
(1, 1) as the four vertices of the unit square, then the interpolation formula
simplifies to
9.3.2.3 Nonlinear
Contrary to what the name suggests, the bilinear interpolant is not linear;
nor is it the product of two linear functions. In other words, the
interpolant can be written as
\n\n=== OCR PAGE 325 ===\nXy—x XX,
F(R) x —f(Q)9) + F(Q>7)
XX, XX)

where R, = (x, Yo).

We next proceed by interpolating in the y-direction

F(P) & Sa fh +S = f(Ra)

This follows the desired estimate of f(x, y.
Flay) © (= «05 =p? x)(y. — y)
F(Q>)) ae

@ox my —)" x) -y)

_ FQ) (xy _ x)(y - yy )

(x) — x) )(¥2 — Y))
(Qo) Vey
F(x) (F(Qi (xX. — x) — Y) + (Qa) Mx — 4) Q2 — Y)

(%) — X)O2 - 1)

tf(Qir)(%y — YY — ¥1) + f(Qy2)(X — XO — Y,))
Here we can note that the same result will be achieved if we execute the y-
interpolation first and x-interpolation second.
9.3.2.2 Unit square

If we select the four points where fis given to be (0, 0), (1, 0), (0, 1), and
(1, 1) as the four vertices of the unit square, then the interpolation formula
simplifies to

F(x yf (0,0) — 0) — y) +f, Ox — y) +f(0, DU — xy +f, Dury.

9.3.2.3 Nonlinear

Contrary to what the name suggests, the bilinear interpolant is not linear;
nor is it the product of two linear functions. In other words, the
interpolant can be written as
\n\n=== PAGE 326 ===\nwhere
In both cases, the number of constants (four) corresponds to the number
of data points where f is given. The interpolant is linear along lines
parallel to either the x or y direction, equivalently if x or y is set constant.
Along any other straight line, the interpolant is quadratic. However, even
if the interpolation is not linear in the position (x and y variables), it is
linear in the amplitude, as it follows from the equations above: all the
coefficient bj, j = 1, …, 4, are proportional to the value of the function f.
The result of bilinear interpolation is independent of which axis is
interpolated first and which second. If we had first performed the linear
interpolation in the y-direction and then in the x-direction, the resulting
approximation would be the same. The obvious extension of bilinear
interpolation to three dimensions is called trilinear interpolation; this
process needs no arithmetic operations and is very fast. It has a
discontinuity at each value and its bounds are fixed at extreme points.
9.3.3 BICUBIC INTERPOLATION
In mathematics, bicubic interpolation is an extension of cubic
interpolation for interpolating data points on a two-dimensional regular
grid. The interpolated surface is smoother than corresponding surfaces
obtained by bilinear interpolation interpolation or nearest-neighbor
interpolation. Bicubic interpolation can be accomplished using Lagrange
polynomials, cubic splines, or cubic convolution algorithms.
Suppose that the function values of f, and its derivatives fx, fy, and fxy are
known at the four corners (0, 0), (1, 0), (0, 1), and (1, 1) of the unit square.
The interpolated surface can then be written as:
\n\n=== OCR PAGE 326 ===\nby + box + byy + byxy

where

b, =f (0,0)

by =f (1,0) —f(0, 0)

b, =f(0, 1) — f(0,0)

b, =F(0,0) —f(,0) —f(0, 1) +f, D)

In both cases, the number of constants (four) corresponds to the number
of data points where fis given. The interpolant is linear along lines
parallel to either the x or y direction, equivalently if x or y is set constant.
Along any other straight line, the interpolant is quadratic. However, even
if the interpolation is not linear in the position (x and y variables), it is
linear in the amplitude, as it follows from the equations above: all the
coefficient b; j=1,..., 4, are proportional to the value of the function f.

The result of bilinear interpolation is independent of which axis is
interpolated first and which second. If we had first performed the linear
interpolation in the y-direction and then in the x-direction, the resulting
approximation would be the same. The obvious extension of bilinear
interpolation to three dimensions is called trilinear interpolation; this
process needs no arithmetic operations and is very fast. It has a
discontinuity at each value and its bounds are fixed at extreme points.

9.3.3 BICUBIC INTERPOLATION

In mathematics, bicubic interpolation is an extension of cubic
interpolation for interpolating data points on a two-dimensional regular
grid. The interpolated surface is smoother than corresponding surfaces
obtained by bilinear interpolation interpolation or nearest-neighbor
interpolation. Bicubic interpolation can be accomplished using Lagrange
polynomials, cubic splines, or cubic convolution algorithms.

Suppose that the function values of f, and its derivatives f,, f,, and fy, are

known at the four corners (0, 0), (1, 0), (0, 1), and (1, 1) of the unit square.
The interpolated surface can then be written as:
\n\n=== PAGE 327 ===\n(9.1)
The interpolation problem consists of determining the 16 coefficients aij.
Matching p(x, y) with the function values yields four equations,
All the directional coefficients can be determined by the following
identities:
This procedure yields a surface p(x, y) on the unit square [0, 1] × [0, 1],
which is continuous and with continuous derivatives. Bicubic
interpolation on an arbitrarily sized regular grid can then be
accomplished by patching together such bicubic surfaces, ensuring that
the derivatives match on the boundaries. If the derivatives are unknown,
they are typically approximated by the function values at points
neighboring the corners of the unit square, for example, using finite
differences. The unknowns coefficients aij can be easily found out by
solving a linear equation.
9.3.4 BIHARMONIC INTERPOLATION
Polynomial splines in 
 are functions of the form
with ν a positive integer and p being a polynomial of degree at most equal
to ν. One reason for the name of polyharmonic spline is that |x|2ν − 1 is a
multiple of the fundamental solution Φ to the distributional equation
\n\n=== OCR PAGE 327 ===\n33
P(x, y) = Dy YX a;x'y!
i=0 j=0
The interpolation problem consists of determining the 16 coefficients aj.
Matching p(x, y) with the function values yields four equations,

F(0,0) = p(O, 0) = agg

FU, 0) = p(X, 0) = agg + aj + Ar9 + 439
F(0, 1) = pO, 1) = dog + Go) + ox + dos
FAV) = PAV) = Lp Dj-0 4%

All the directional coefficients can be determined by the following
identities:

f(%.y) = p(x. y) =D Lino 4 itty
f(y) = py(%y) =>), re a,ixy- !
fy Y) = Py ¥) = Ly pe aij ly"

This procedure yields a surface p(x, y) on the unit square [0, 1] x [o, 1],
which is continuous and with continuous derivatives. Bicubic
interpolation on an arbitrarily sized regular grid can then be
accomplished by patching together such bicubic surfaces, ensuring that
the derivatives match on the boundaries. If the derivatives are unknown,
they are typically approximated by the function values at points
neighboring the corners of the unit square, for example, using finite
differences. The unknowns coefficients aj can be easily found out by
solving a linear equation.

9.3.4 BIHARMONIC INTERPOLATION
Polynomial splines in R? are functions of the form

N

S(x) = p(x) + YX d;|x — Pert,

i=l

(9.1),

with v a positive integer and p being a polynomial of degree at most equal
to v. One reason for the name of polyharmonic spline is that |x|?’ ~*is a
multiple of the fundamental solution ® to the distributional equation
\n\n=== PAGE 328 ===\nwhere the Laplacian is denoted by Δ and δ0 is the Dirac measure at origin.
The main advantage of using polyharmonic splines is their smoothing
interpolation property. Focusing on the 
 case, given a set of distinct
points 
 unisolvent for π3
ν, and corresponding functional
values for 
, there is a unique (ν + 1)-harmonic splines S of the form
(9.1) satisfying the interpolation conditions
and the side conditions
Biharmonic case is a special case for ν = 1 in equation (9.1).
9.3.5 THIN PLATE SPLINES
Thin plate splines (TPSs) are an interpolation and smoothing technique,
the generalization of splines so that they may be used in two or more
dimensions.
9.3.5.1 Physical analogy
The name TPS refers to a physical analogy involving the bending of a thin
sheet of metal. Just as the metal has rigidity, the TPS fit resists bending
also, implying a penalty involving the smoothness of the fitted surface. In
the physical setting, the deflection is in the z direction, orthogonal to the
plane. To apply this idea to the problem of coordinates transformation,
one interprets the lifting of the plate as a displacement of the x or y
coordinates within the plane. In the two-dimensional cases, given a set of
K corresponding points, the TPS warp is described by 2(K + 3)
parameters, which include SIX global affine motion parameters and 2K
coefficients for correspondences of the control points. These parameters
are computed by solving a linear system; in other words, TPS has a
closed-form solution.
\n\n=== OCR PAGE 328 ===\nAY! ® = 6p,

where the Laplacian is denoted by A and 6, is the Dirac measure at origin.
The main advantage of using polyharmonic splines is their smoothing
interpolation property. Focusing on the R* case, given a set of distinct
points {x} aa eR unisolvent for 23,, and corresponding functional
values for fi © R, there is a unique (v + 1)-harmonic splines S of the form
(9.1) satisfying the interpolation conditions

Sx=fs §=1,2,....N,

and the side conditions

N
Y d,q(x;) = 0, Vqe nm.
i=l

Biharmonic case is a special case for v = 1 in equation (9.1).

9.3.5 THIN PLATE SPLINES

Thin plate splines (TPSs) are an interpolation and smoothing technique,
the generalization of splines so that they may be used in two or more
dimensions.

9.3.5.1 Physical analogy

The name TPS refers to a physical analogy involving the bending of a thin
sheet of metal. Just as the metal has rigidity, the TPS fit resists bending
also, implying a penalty involving the smoothness of the fitted surface. In
the physical setting, the deflection is in the z direction, orthogonal to the
plane. To apply this idea to the problem of coordinates transformation,
one interprets the lifting of the plate as a displacement of the x or y
coordinates within the plane. In the two-dimensional cases, given a set of
K corresponding points, the TPS warp is described by 2(K + 3)
parameters, which include SIX global affine motion parameters and 2K
coefficients for correspondences of the control points. These parameters
are computed by solving a linear system; in other words, TPS has a
closed-form solution.
\n\n=== PAGE 329 ===\n9.3.5.2 Smoothness measure
The TPS arises on considering the square of the second derivative integral
—this forms its smoothness measure. In the case where x is two-
dimensional, for interpolation, the TPS fits a mapping function f(x)
between corresponding point-sets yi and xi that minimizes the following
energy function:
The smoothing variant, correspondingly, uses a tuning parameter λ to
control how nonrigid is allowed for the deformation, balancing the
aforementioned criterion with the measure of goodness of fit, thus
minimizing
For this variational problem, it can be shown that there exists a unique
minimizer f. The finite element discretization of this variational problem,
the method of elastic maps, is used for data mining and nonlinear
dimensionality reduction.
9.3.5.3 Radial basis function
The TPS has a natural representation in terms of radial basis functions.
Given a set of control points {wi, i = 1, 2, …, K}, a radial basis function
basically defines a spatial mapping that maps any location x in space to a
new location f(x), represented by
\n\n=== OCR PAGE 329 ===\n9.3.5.2 Smoothness measure

The TPS arises on considering the square of the second derivative integral
—this forms its smoothness measure. In the case where x is two-
dimensional, for interpolation, the TPS fits a mapping function f(x)
between corresponding point-sets y; and x; that minimizes the following

energy function:

e-[ [ (1) +2( )+( “1 dx, dx.
6x2 OX OX> 6x2

The smoothing variant, correspondingly, uses a tuning parameter A to
control how nonrigid is allowed for the deformation, balancing the
aforementioned criterion with the measure of goodness of fit, thus
minimizing

Exps(f) = Dw: -ropr +a [ | ()+ 2(-.)
+(#) dx dx).
Ox;

For this variational problem, it can be shown that there exists a unique
minimizer f. The finite element discretization of this variational problem,
the method of elastic maps, is used for data mining and nonlinear
dimensionality reduction.

9.3.5.3 Radial basis function
The TPS has a natural representation in terms of radial basis functions.
Given a set of control points {w;, i = 1, 2, ..., K}, a radial basis function

basically defines a spatial mapping that maps any location x in space to a
new location f(x), represented by
\n\n=== PAGE 330 ===\nwhere ‖ · ‖ denotes the usual Euclidean norm and {ci} is a set of mapping
coefficients. The TPS corresponds to the radial basis kernel φ(r) = r2log r.
9.3.6 NUMERICAL APPLICATIONS
We study the efficiency and accuracy of the different interpolation
techniques on the geophysical data set, and we have applied five different
interpolation processes to the same data set. Moreover, we calculated the
best fit parameters such as SSE and R-square. These parameters indicate
how well fitted the surfaces are with respect to the given data set.
In this numerical exploration of the data set, we used curve fitting toolbox
in Matlab to draw all the interpolation surfaces and calculate the
parameters of best fit. Results are presented for five randomly selected
years where the magnitude for the earthquake in different locations is
available.
In Figures 9.1, 9.2, 9.3, 9.4, and 9.5 below, we display typical results for
the earthquake estimation surface simulated by the five interpolation
techniques in some areas of California. We used data from 1973, 1979,
1988, 1996, and 2008 for certain range of months. Real value data were
used to draw the estimation surface. The data for these figures was
measured in the Western Hemisphere (i.e., − 180° in longitude). The
entire set of earthquakes analyzed (from 1973, 1978, 1988, 1996, and
2008) is presented in Tables 9.1, 9.2, 9.3, 9.4, and 9.5, respectively. Each
table lists the parameters of goodness of fit, like SSE and R-square, that
were obtained by using different interpolation methods for five separate
years. These parameters are an excellent indicator of how good our
surface of fitness is.
The numerical results obtained by implementing our interpolation
methods on the geophysical data set indicated that our estimated
interpolation surface actually fitted very good with respect to the data set
that was used. Looking at the goodness-of-fit parameters, such as SSE
and R-square, all the data were very accurately and efficiently used to
generate the interpolating surface. It is realized that goodness-of-fits are
generally dependent on the number of data points used. In most of the
cases it will give a near-zero SSE value and an R-square value close to 1
that are considered to be an excellent measure of fit. Although when we
applied our interpolating techniques on the data set from 1979, the result
\n\n=== OCR PAGE 330 ===\nK

Fe) =D cawilke — will

i=l
where || - || denotes the usual Euclidean norm and {c;} is a set of mapping
coefficients. The TPS corresponds to the radial basis kernel ¢(r) = r’log r.

9.3.6 NUMERICAL APPLICATIONS

We study the efficiency and accuracy of the different interpolation
techniques on the geophysical data set, and we have applied five different
interpolation processes to the same data set. Moreover, we calculated the
best fit parameters such as SSE and R-square. These parameters indicate
how well fitted the surfaces are with respect to the given data set.

In this numerical exploration of the data set, we used curve fitting toolbox
in Matlab to draw all the interpolation surfaces and calculate the
parameters of best fit. Results are presented for five randomly selected
years where the magnitude for the earthquake in different locations is
available.

In Figures 9.1, 9.2, 9.3, 9.4, and 9.5 below, we display typical results for
the earthquake estimation surface simulated by the five interpolation
techniques in some areas of California. We used data from 1973, 1979,
1988, 1996, and 2008 for certain range of months. Real value data were
used to draw the estimation surface. The data for these figures was
measured in the Western Hemisphere (i.e., — 180° in longitude). The
entire set of earthquakes analyzed (from 1973, 1978, 1988, 1996, and
2008) is presented in Tables 9.1, 9.2, 9.3, 9.4, and 9.5, respectively. Each
table lists the parameters of goodness of fit, like SSE and R-square, that
were obtained by using different interpolation methods for five separate
years. These parameters are an excellent indicator of how good our
surface of fitness is.

The numerical results obtained by implementing our interpolation
methods on the geophysical data set indicated that our estimated
interpolation surface actually fitted very good with respect to the data set
that was used. Looking at the goodness-of-fit parameters, such as SSE
and R-square, all the data were very accurately and efficiently used to
generate the interpolating surface. It is realized that goodness-of-fits are
generally dependent on the number of data points used. In most of the
cases it will give a near-zero SSE value and an R-square value close to 1
that are considered to be an excellent measure of fit. Although when we
applied our interpolating techniques on the data set from 1979, the result
\n\n=== PAGE 331 ===\ndid not come out very good as we used a larger number of data points.
This is only an indication that these methods are very efficient with local
data.
On the application of our methods to data corresponding to the years
1973 and 1988, we noticed that the parameters of goodness of fit came out
same by applying different interpolation techniques. So we concluded that
these methods are highly data dependent and work best when applied
locally. Some more investigation is needed to realize the true connection
between these processes and the data set. But overall, we can conclude
that the interpolation techniques can serve as a very powerful and robust
methodology to process spatial data.
9.4 Conclusion
As discussed in Section 9.3, the biggest advantage that the Lowess/Loess
methods have over many other methods is the fact that they do not
require the specification of a function to fit a model over the sampled
global data. Instead, an analyst has to provide a smoothing parameter
value and the degree of the local polynomial. Moreover, the flexibility of
this process makes it ideal for modeling complex processes for which no
theoretical model exists. Also, the simplicity to execute the methods
makes these processes very popular among the modern era regression
methods that fit the general framework of least squares regression, but
having a complex deterministic structure. Although they are less obvious
than some of the other methods related to linear least squares regression,
Lowess/Loess also enjoy most of the benefits generally shared by the
other methods, the most important of those is the theory for computing
uncertainties for prediction, estimation, and calibration. Many other tests
and processes used for validation of least square models can also be
extended to Lowess/Loess. The major drawback of Lowess/Loess is the
inefficient use of data compared with other least square methods.
Typically they require fairly large, densely sampled data sets to create
good models, the reason behind is that the Lowess/Loess relies on the
local data structure when performing the local fitting, thus proving less
complex data analysis in exchange of increased computational cost. The
Lowess/Loess methods do not produce a regression function that is
represented by a mathematical formula, what may be a disadvantage: At
times it can make really difficult to transfer the results of an analysis to
other researchers, to transfer the regression function to others, they
would need the data set and the code for the Lowess/Loess calculations.
In nonlinear regression, on the other hand, it is only necessary to write
\n\n=== PAGE 332 ===\ndown a functional form in order to provide estimates of the unknown
parameters and the estimated uncertainty. On the basis of the application,
this could be either a major or minor setback of using Lowess/Loess. In
particular, the simple form of Lowess/Loess cannot be applied for
mechanistic modeling where fitted parameters specify particular physical
properties of the system. Finally, it is worth mentioning the computation
cost associated with this procedure, although this should not be a problem
in the modern computing environment, unless the data sets being used
are very large. Lowess/Loess methods also have a tendency to be affected
by the outliers in the data set, like any other least square methods. There
is an iterative robust version of Lowess/Loess (see [1]) that can be applied
to reduce sensitivity to outliers, but if there are too many extreme
outliers, this robust version also fails to produce the desired results.
Analyzing earthquake data sets is not always a very easy modeling
procedure, as many different factors can be involved in these phenomena.
If we analyze the time series data in order to estimate parameters
corresponding to some extreme earthquakes, the modeling technique has
to be dependent on traditional stochastic procedure. As we performed
spatial analysis of the data with a freezing time, the deterministic
behavior can be taken into account. We observe that the results were
more dependent on the nature of the data and how the different locations
(where the magnitude of the earthquake is given) are close to each other,
if we consider data for locations that are sparsely located, then the local
regression model will not work up to our satisfaction; we have considered
the locations that are geographically closer. We conclude that somehow
Lowess has been proved to be a better estimator than the other process.
That may be due to some data trends in the data set. Overall, this method
has been proven to be a better deal to get numerical estimations for
spatial analysis.
The high-frequency data arising from financial market were treated with
different smoothing techniques and the best curve fitted provided a very
good estimation to the data. The robust version of the weighted local
regression technique was much more desirable than its original version.
The current work shows that these modeling methods may be applied to
high-frequency data and to individual equity data.
In previous works that were done with these geophysical data sets, Ising-
type models, Lévy models, and scale invariance properties were used (see
[12, 15] and the references therein) and the authors provide estimations of
a “critical phenomenon” by using the time series data. Similar work has
been done in [3, 6].
\n\n=== PAGE 333 ===\nWith our weighted local regression-type model, we fixed the time (in this
case the year) and used the magnitude of the earthquake from different
locations within the time frame to estimate the magnitude of the
earthquake at locations whose data were not used. In other words, our
model performed a spatial analysis with the given geophysical data. As an
extension of this work, we have applied different interpolation methods to
the same geophysical data sets and obtained very promising results.
Although these are all deterministic models and in general earthquake
data are stochastic, we have plans to somehow merge these deterministic
models with a strong Lévy’s model to see any possible modeling approach
that can open a perspective to deal with these data in future. Generally, a
Lévy process consists of three essential components: (i) deterministic
part, (ii) continuous random Brownian part, and (iii) discontinuous jump
part. For spatial analysis using geophysical data, the third part does not
play a big role so modified the deterministic approach can be considered
an efficient one to deal this phenomenon.
To model high-frequency data, we fitted a curve of best fit in the time
series data where returns of the stock price were given for every minute
for five different financial institutions. Time series modeling (exponential
smoothing and ARIMA) is a well-established methodology to address
questions of prediction in financial time series. Many works have been
done to address these questions, but our aim is to show the usefulness of
the Local Regression models with some modification applied to such
time-dependent data set. In literature, there are numerous such fits like
the one we presented here, but our fit is very appropriate and efficient to
apply for local data. As a matter of fact, when stock prices are, in general,
locally influenced, this fit will act as better than many others. Overall, we
conclude that our approach is a very powerful and easy-to-apply method
that produces numerical results with excellent efficiency.
On a final note, we believe that this manuscript deals with a relevant
problem: the estimation of parameters associated with major events in
geophysics. We believe that the methodology that we present can also be
used to estimate and predict parameters associated with major/extreme
events in econophysics, for example, phase transition. The analogy
between phase transition and financial modeling can be easily done when
considering the original one-dimensional Ising model in phase transition,
this simple model has been used in physics to describe the
ferromagnetism. Isings model considers a lattice composed of N atoms
that interact with their immediate lattice neighbors. Likewise, the
financial model will consider a lattice composed of N traders (each trader
can also represent a cluster of traders) that interact in a similar manner.
\n\n=== PAGE 334 ===\nIn the model for ferromagnetism, a material evidences a net
magnetization below a critical parameter, when all spins were arranged
on the same direction. In a similar way, in the model for a market crash,
the crash happens when all the traders in the market start to sell.
Acknowledgments
The authors thank the anonymous reviewer(s) for the careful reading of
the manuscript and the fruitful suggestions.
References
1. Cleveland, William S. (1979), Robust locally weighted regression and
smoothing scatterplots, Journal of the American Statistical
Association, 74 (368), Pages 829–836.
2. Cleveland, William S., Devlin, Susan J. (1988) ‘Locally-weighted
regression: An approach to regression analysis by local fitting, Journal
of the American Statistical Association, 83 (403), Pages 596–610.
3. Fonseca R. C. B., Figueiredo A., Castro M. T., Mendes F. M. (2013),
Generalized Ornstein–Uhlenbeck process by Doob’s theorem and the
time evolution of financial prices, Physica A: Statistical Mechanics
and Its Applications, 392, Pages 1671–1680.
4. Fox, J. (2000a), Nonparametric simple regression: Smoothing
scatterplots. Sage, Thousand Oaks, CA.
5. Fox, J. (2000b), Multiple and generalized nonparametric regression.
Sage, Thousand Oaks, CA.
6. Habtemicael, S., SenGupta, I. (2014), Ornstein–Uhlenbeck processes
for geophysical data analysis, Physica A: Statistical Mechanics and Its
Applications, 399, Pages 147–156.
7. Hainzl, S., Ogata, Y. (2005), Detecting fluid signals in seismicity data
through statistical earthquake modeling, Journal of Geophysical
Research: Solid Earth, 110, B05S07.
8. Ising, E. (1925), Beitrag zur Theorie des Ferromagnetismus,
Zeitschrift fur Physik, 31, Pages 253–258.
9. Johansen, A., Sornette, D. (1997), Large financial crashes, Physica A,
245 (N3-4), Pages 411–422.
10. Johansen, A., Ledoit, O., Sornette, D. (2000), Crashes as critical
points, International Journal of Theoretical and Applied Finance, 3
(1), Pages 219–255.
\n\n=== PAGE 335 ===\n11. Mariani, M. C., Basu, K. (2014), Local regression type methods applied
to the study of geophysics and high frequency financial data, Physica
A: Statistical Mechanics and Its Applications, 410, Pages 609–622.
12. Mariani, M. C., Florescu, I., Sengupta, I., Beccar Varela, M. P., Bezdek,
P., Serpa, L. (2013), Lévy models and scale invariance properties
applied to geophysics, Physica A: Statistical Mechanics and Its
Applications, 392, Pages 824–839.
13. Mantegna, R. N., Stanley, H. E. (1999), An Introduction to
Econophysics: Correlations and Complexity in Finance, Cambridge
University Press, Cambridge.
14. Mariani, M. C., Liu, Y. (2006), A new analysis of intermittence, scale
invariance and characteristic scales applied to the behavior of financial
indices near a crash, Physica A, 367, Pages 345–352.
15. Mariani, M. C., Bezdek, P., Serpa, L., Florescu, I. (2011), Ising type
models applied to geophysics and high frequency market data, Physica
A, 390, 23/24, Pages 4396–4402.
16. Nason, G. P., Silerman, B. W. (1994), The discrete wavelet transform
in S, Journal of Computational and Graphical Statstics, 3, Pages
163–191.
17. Nason, G. P., Silerman, B. W. (2000), Wavelets for regression and
other statistical problems, In Smoothing and Regression:
Approaches, Computation, and Application, Wiley, New York.
18. Ogata, Y. (1988), Statistical models for earthquake occurrences and
residual analysis for point processes, Journal of the American
Statistical Association, 83, Pages 9–27.
19. Ogata, Y. (1998), Space-time point-process models for earthquake
occurrences, Annals of the Institute of Statistical Mathematics, 50,
Pages 379–402.
20. Reasenberg, P. A., Jones L. M. (1989), Earthquake hazard after a
mainshock in California, Science, 243, Pages 1173–1176.
21. Stein, R. S. (1999), The role of stress transfer in earthquake
occurrence, Nature, 402, Pages 605–609.
22. Toda, S., Stein, R. S., Reasenberg, P. A., Dieterich, J. H., Yoshida, A.
(1998), Stress transferred by the 1995 Mw = 6.9 Kobe, Japan, shock:
Effect on aftershocks and future earthquake probabilities, Journal of
Geophysical Research: Solid Earth, 103, Pages 24543–24565.
23. The USGS Earthquake Magnitude Working Group, USGS Earthquake
Magnitude Policy,USGS. 29 March 2010. Web November 26, 2010.
<http://earthquake.usgs.gov/aboutus/docs/020204mag_policy.php>
\n\n=== PAGE 336 ===\nChapter Ten
Study of Volatility Structures in Geophysics and
Finance Using Garch Models
Maria C. Mariani1, F. Biney1, and I. SenGupta2
1Department of Mathematical Sciences, University of Texas at El Paso, El Paso,
TX, USA
2Department of Mathematics, North Dakota State University, Fargo, ND, USA
10.1 Introduction
With the availability of high-frequency data for financial market analysis, there has
been an increase in the studies dealing with the persistence of shocks in both the
mean and variance of financial instruments return. Several studies report evidence
of persistence (long-memory) behavior in squared returns or empirical volatilities;
see Breidt et al. [1], Robinson [2], Shephard [3], Lobato and Savin [4], and Baillie
[5]. Similar features have been observed in data from other fields such as physics
and geophysics. In physics, the presence of strong autocorrelation in the squares of
differences in velocity of the mean wind direction has been explored by Barndorff–
Nielsen and Shephard [6].
The pioneering work of Box et al. [7] in the area of autoregressive moving average
models paved the way for related work in the area of volatility modeling with the
introduction of ARCH and then GARCH models by Engle [8] and Bollerslev [9],
respectively. In terms of the statistical framework, these models provide motion
dynamics for the dependency in the conditional time variation of the distributional
parameters of the mean and variance, in an attempt to capture such phenomena as
autocorrelation in returns and squared returns. Extensions to these models have
included more sophisticated dynamics such as the threshold model (TGARCH)
[Zakoian (1994)] to capture the asymmetry in the news impact, the NGARCH model
[10], the EGARCH models [11], the stochastic volatility models [12], the FIGARCH
and FIEGARCH models [13, 14], and the long-memory generalized autoregressive
conditionally heteroskedastic (LMGARCH) models [2, 15, 16] as well as distributions
other than the normal to account for the skewness and excess kurtosis observed in
practice such as Student’s t test, Generalized error, Generalized Hyperbolic [17], the
Normal inverse gaussian distribution, and Johnson’s SU distribution [18].
In the literature, the most propular GARCH model is the GARCH(1,1), where the
persistence parameter is less than 1 to ensure strictly and covariance stationarity. It
turns out that on modeling using GARCH, most often the persistence parameter is
approximately 1 but yet the model does not adequately capture the persistence in
volatility. This fact motivated the introduction of the integrated GARCH (IGARCH)
model where Bollerslev and Engel [19] allowed for unit persistence in the GARCH
model; that is, the persistence was set to 1. The IGARCH model has some structural
\n\n=== PAGE 337 ===\n(10.1)
(10.2)
(10.3)
complication in the sense that its unconditional variance does not exist. Baillie et al.
[13] extended the IGARCH to the fractional IGARCH (FIGARCH) by allowing for
high persistence (long memory) directly in the conditional variance, while avoiding
the complications of IGARCH; that is, they allowed the integration coefficient to vary
between [0,1].
Our main interest is to investigate the underlying volatility process in earthquake
series, explosive series, high-frequency financial data, and financial indices and
examines the applicability of a range of GARCH specifications for modeling volatility
of these series to identify similarities and differences in the volatility structure.
This chapter is organized as follows: In Sections 10.2 and 10.3, we review short
memory processes (ARMA and GARCH models) and long memory processes
(ARFIMA, ARFIMA-GARCH, and FIGARCH), respectively. In Section 10.12, we
present methods for detecting and estimating long memory processes such as ADF
test, KPSS test, and Whittle approximation. In Section 10.16, we discuss the data and
empirical results. Finally, in Section 10.6, we give the conclusion.
10.2 Short memory models
In this section, we present a brief introduction of short memory processes. A
stationary process has a short memory if its autocorrelation functions decay to zero
exponentially fast (ρk → 0 exponentially fast as k → ∞) [20] and the autocorrelations
are absolutely summable [21]. That is, there exist a D > 0 and r < 1 such that
and
10.2.1 ARMA(p,q) MODEL
The general ARMA(p, q) model is given by
where at is a white noise series and p and q are nonnegative integers. The
autoregressive (AR) and moving average (MA) models are special cases of the
ARMA(p, q) model. Using the back-shift operator, the model can be written as
\n\n=== OCR PAGE 337 ===\ncomplication in the sense that its unconditional variance does not exist. Baillie et al.
[13] extended the IGARCH to the fractional IGARCH (FIGARCH) by allowing for
high persistence (long memory) directly in the conditional variance, while avoiding
the complications of IGARCH; that is, they allowed the integration coefficient to vary
between [0,1].

Our main interest is to investigate the underlying volatility process in earthquake
series, explosive series, high-frequency financial data, and financial indices and
examines the applicability of a range of GARCH specifications for modeling volatility
of these series to identify similarities and differences in the volatility structure.

This chapter is organized as follows: In Sections 10.2 and 10.3, we review short
memory processes (ARMA and GARCH models) and long memory processes
(ARFIMA, ARFIMA-GARCH, and FIGARCH), respectively. In Section 10.12, we
present methods for detecting and estimating long memory processes such as ADF
test, KPSS test, and Whittle approximation. In Section 10.16, we discuss the data and
empirical results. Finally, in Section 10.6, we give the conclusion.

10.2 Short memory models

In this section, we present a brief introduction of short memory processes. A
stationary process has a short memory if its autocorrelation functions decay to zero
exponentially fast (p, — 0 exponentially fast as k — 0) [20] and the autocorrelations
are absolutely summable [21]. That is, there exist a D > 0 and r < 1 such that

py ~ D\r\‘ (10.1)
and
Y lo <e ue)
k=-00

10.2.1 ARMA(p,q) MODEL
The general ARMA(p, q) model is given by

P gq
(10.3)
r= Pot Dy Dili + ay - YX Oj4,_;
i= i=l

where a, is a white noise series and p and q are nonnegative integers. The

autoregressive (AR) and moving average (MA) models are special cases of the
ARMA(p, q) model. Using the back-shift operator, the model can be written as
\n\n=== PAGE 338 ===\n(10.4)
(10.5)
(10.6)
(10.7)
The polynomial 1 − ϕ1B − ⋅⋅⋅ − ϕpBp is the AR polynomial of the model. Similarly, 1 −
θ1B... − θqBq is the MA polynomial. It is required that there are no common factors
between the AR and MA polynomials; otherwise, the order(p, q) of the model can be
reduced [20].
10.2.2 garch(p,q) model
The standard GARCH model [23] may be written as:
where σ2
t denoting the conditional variance, ω the intercept, ϵt is a sequence of iid
random variables with mean 0 and variance 1, α0 > 0, αi ≥ 0, βi ≥ 0 and
The GARCH order is defined by (q,p)(ARCH, GARCH), and  is the persistence
defined below:
10.2.3 IGARCH(1,1) MODEL
From (5), when p = q = 1 and α1 + β1 = 1 the GARCH(1,1) model becomes
This model, first developed by Engle and Bollerslev [24], is referred to as an
Integrated GARCH model or as an IGARCH model. Squared shocks are persistent,
so the variance follows a random walk with a drift ω. It is known that a GARCH
model is analogous to an ARMA model and the IGARCH model where the variance
process has a unit root is analogous to an ARIMA model. Because of unit
persistence, its unconditional variance does not exist. Nelson [25] showed that while
the process in the GARCH(1,1) model is covariance stationary, strictly stationary,
and ergodic, the IGARCH(1,1) model is not covariance stationary but is still strictly
stationary and ergodic, distinguishing it from the random walk with drift case.
\n\n=== OCR PAGE 338 ===\n(1 - pB- + — @,B’)r, = bo + (1 - OB... — 0,B")a, (10.4)

The polynomial 1 — ,B - --- — @,B? is the AR polynomial of the model. Similarly, 1 -

6,B... — 8,B1 is the MA polynomial. It is required that there are no common factors

between the AR and MA polynomials; otherwise, the order(p, q) of the model can be
reduced [20].

10.2.2 garch(p,q) model
The standard GARCH model [23] may be written as:

T,=H+E, €,=0,€

q p
2 2 2 (20.5)
op =art YX ae + YX Bo;;
i=l i=l

where 0°, denoting the conditional variance, w the intercept, €; is a sequence of iid
random variables with mean 0 and variance 1, dy > 0, a; = 0, B; = o and

Ya + vp, <i.
i=l i=l

The GARCH order is defined by (q,p)(ARCH, GARCH), and P is the persistence
defined below:

P= y a+ y 4, (10.6)
j=l jal

10.2.3 IGARCH(1,1) MODEL
From (5), when p = q = 1anda, + B, = 1 the GARCH(1,1) model becomes

o=ot(1 — fer, + Bion, (10.7)

This model, first developed by Engle and Bollerslev [24], is referred to as an
Integrated GARCH model or as an IGARCH model. Squared shocks are persistent,
so the variance follows a random walk with a drift w. It is known that a GARCH
model is analogous to an ARMA model and the IGARCH model where the variance
process has a unit root is analogous to an ARIMA model. Because of unit
persistence, its unconditional variance does not exist. Nelson [25] showed that while
the process in the GARCH(1,1) model is covariance stationary, strictly stationary,
and ergodic, the IGARCH(1,1) model is not covariance stationary but is still strictly
stationary and ergodic, distinguishing it from the random walk with drift case.
\n\n=== PAGE 339 ===\n(10.8)
(10.9)
(10.10)
(10.11)
(10.12)
(10.13)
10.3 Long memory models
In this section, we define long memory model and look at some processes that have
long memory. A process has long memory if its autocovariances are not absolutely
summable, that is:
and the autocovariances decay hyperbolically or at a polynomial rate (slow rate), that
is:
as k → ∞, where d is the long-memory parameter and c > 0 [21].
10.3.1 ARFIMA(p,d,q) MODEL
The autoregressive fractionally integrated moving-average (ARFIMA) process is a
class of long-memory models introduced by Granger, Joyeux [26] and Hosking [27].
An ARFIMA process Yt may be defined by
where (B) is the back shift operator, ϕ(B) = 1 + ϕ1B + ⋅⋅⋅ + ϕpBp and θ(B) = 1 + θ1B +
⋅⋅⋅ + θpBp are the autoregressive and moving-average polynomials, respectively; ϕ(B)
and θ(B) have no common roots, (1 − B)− d is a fractional differencing operator
defined by the binomial expansion
where
with d < 1, d ≠ 0, −1, −2, ..., ϵt a white noise sequence with finite variance and Γ the
gamma function.
10.3.2 ARFIMA(p,d,q)-GARCH(r,s)
An ARFIMA(p, d, q) − GARCH(r, s) process is defined by equation
\n\n=== OCR PAGE 339 ===\n10.3 Long memory models

In this section, we define long memory model and look at some processes that have
long memory. A process has long memory if its autocovariances are not absolutely
summable, that is:

y In| = 0 (10.8)
l=

k=-00
and the autocovariances decay hyperbolically or at a polynomial rate (slow rate), that
is:
1 ~ cee (10.9)

as k — «, where d is the long-memory parameter and c > 0 [21].

10.3.1 ARFIMA(p,d,q) MODEL

The autoregressive fractionally integrated moving-average (ARFIMA) process is a
class of long-memory models introduced by Granger, Joyeux [26] and Hosking [27].
An ARFIMA process Y; may be defined by

P(B)Y, = 0(B)(1 — BY“e,, (10.10)

where (B) is the back shift operator, @(B) =1+ ),B +--+ 6,B? and 0(B) =1+6,B+
+ + 0,B? are the autoregressive and moving-average polynomials, respectively; p(B)

and @(B) have no common roots, (1 — B)- “is a fractional differencing operator
defined by the binomial expansion

— . 10.11
(By 4 = Di niBy (o.1)
j=0
where
TG +d) (10.12)

U~ TG4 Dr@)’

with d <1, d # 0, -1, -2, ..., €; a white noise sequence with finite variance and I the
gamma function.

10.3.2 ARFIMA(p,d,q)-GARCH(r,s)
An ARFIMA(p, d, q) - GARCH(r, s) process is defined by equation

PLY, = OL) — L)*e,, (10.13)
\n\n=== PAGE 340 ===\n(10.14)
(10.15)
(10.16)
(10.17)
(10.18)
where Yt − 1, Yt − 2, Yt − 3, … are the past observations,
the conditional variance of the process {Yt}, the GARCH coefficients α1, α2, …, αr and
β1, β2, …, βs are positive, and satisfy:
{εt} is a sequence of independent and identically distributed zero mean and unit
variance random variables. Although εt is often assumed to be Gaussian, in some
cases it may be specified by a t-distribution, double exponential distribution, or a
generalized error distribution, among others. These distributions have a greater
flexibility to accommodate a possible heavy tail behavior of some financial time
series [28].
10.3.3 INTERMEDIATE MEMORY PROCESS
The ARFIMA − GARCH process defined in (13) − (17) has been employed for
modeling many financial series exhibiting long-range dependence. Nevertheless, the
squares of an ARFIMA-GARCH process have only intermediate memory for d ∈ (0,
1/4). A process is said to have intermediate memory if for a large lag k its auto
correlation function (ACF) behaves like ρ(k) ~ c|k|2d − 1 with d < 0 and c > 0. Thus,
the ACF decays to zero at an hyperbolic rate but it is absolutely summable [28], that
is:
10.3.4 FIGARCH MODEL
The previous section showed that the ARFIMA model can be used directly to model
the long memory behavior observed in the financial asset returns. However,
sometimes we may want to model the dynamics of the asset returns, together with its
volatility. In those situations, the GARCH class models provide viable alternatives
for volatility modeling. As illustrated in Section 10.2, two components GARCH
models can be used to capture the high persistence in volatility by allowing a highly
persistent long-run component but yet the squared GARCH process have only short
memory.
\n\n=== OCR PAGE 340 ===\n&, = €0, (10.14)

2 3 LM (10.15)
op =at+ Y ae, + »y Bo}
jal j=l

where Y;_,, Y;_ 9, Y;_ 3, -- are the past observations,

oa = E[Y?] (10.16)

the conditional variance of the process {Y;}, the GARCH coefficients a,, ds, ..., a, and
B,, Bo, ..., B, are positive, and satisfy:

Ye + y Be< i, (10.17)
j=l j=l

{e,} is a sequence of independent and identically distributed zero mean and unit
variance random variables. Although e; is often assumed to be Gaussian, in some
cases it may be specified by a t-distribution, double exponential distribution, or a
generalized error distribution, among others. These distributions have a greater
flexibility to accommodate a possible heavy tail behavior of some financial time
series [28].

10.3.3 INTERMEDIATE MEMORY PROCESS

The ARFIMA — GARCH process defined in (13) — (17) has been employed for
modeling many financial series exhibiting long-range dependence. Nevertheless, the
squares of an ARFIMA-GARCH process have only intermediate memory for d € (0,
1/4). A process is said to have intermediate memory if for a large lag k its auto
correlation function (ACF) behaves like p(k) ~ c|k|24~1 with d < 0 andc > o. Thus,
the ACF decays to zero at an hyperbolic rate but it is absolutely summable [28], that
is:

= 10.18
Y leh) < 00 (048)
=0

10.3.4 FIGARCH MODEL

The previous section showed that the ARFIMA model can be used directly to model
the long memory behavior observed in the financial asset returns. However,
sometimes we may want to model the dynamics of the asset returns, together with its
volatility. In those situations, the GARCH class models provide viable alternatives
for volatility modeling. As illustrated in Section 10.2, two components GARCH
models can be used to capture the high persistence in volatility by allowing a highly
persistent long-run component but yet the squared GARCH process have only short
memory.
\n\n=== PAGE 341 ===\n(10.19)
(10.20)
(10.21)
(10.22)
(10.23)
(10.24)
This subsection shows how GARCH models can be allowed directly for long memory
and high persistence in volatility. From the lag operation representation, the
GARCH(p,q) model can be written as
where
and L is the lag operator (polynomial),
The stationarity of this process is achieved when α(L) + β(L) < 1.
Let νt = ϵ2
t − σ2
t, GARCH(p,q) model can be written as an ARMA(m,q) process in
terms of squared residuals as
with m = max{p, q}. From this formulation, allowing for the presence of a unit root
in [1 − α(L) − β(L)], Bollerslev and Engel (1996) defined the integrated
GARCH(IGARCH(p, q)) process:
where ϕ(L) = ∑m − 1
i = 1ϕiLi and as it is of order m − 1, then the unconditional
variance does not exist [25]. To allow for high persistence and long memory in the
conditional variance while avoiding the complications of IGARCH models, Baillie
et al. [13] extended the IGARCH case allowing for the integration coefficient to vary
between [0, 1] by using parallel argument from ARMA(p, q) process to ARFIMA(m,
d, q) processes. The FIGARCH(p, d, m) process is defined as follows:
where ϕ(L) = ∑m − 1
i = 1ϕiLi and is of order m − 1 and all the roots of ϕ(z) = 0 and
β(L) = 0 lie outside the unit circle. When d = 0, this reduces to the usual GARCH
model, which has short memory; when d = 1, this becomes the IGARCH model,
which has infinite persistence and when 0 < d < 1, the fractionally differenced
squared residuals, (1 − L)dϵ2
t, follows a stationary ARMA(m, q) process. The ARMA
representation of the FIGARCH process is given by
\n\n=== OCR PAGE 341 ===\nThis subsection shows how GARCH models can be allowed directly for long memory
and high persistence in volatility. From the lag operation representation, the
GARCH(p,q) model can be written as

o =ot+ a(L)e? + B(L)o? (10.19)

where

Y="t+e,  €,~ iid(0,07) (10.20)

and L is the lag operator (polynomial),

(10.21)

q P
aL) = Pali, ply= > pli
i=l j=l

The stationarity of this process is achieved when a(L) + B(L) < 1.

Let v; = €?, — 0?,, GARCH(p,q) model can be written as an ARMA(m,q) process in
terms of squared residuals as

[1 — a(L) — P(L)Je? = w + [1 - BL)|v, (10.22)

with m = max{p, g}. From this formulation, allowing for the presence of a unit root
in [1 — a(L) — B(L)], Bollerslev and Engel (1996) defined the integrated
GARCH(UIGARCH(p, q)) process:

(1-L)p(Lye? = o + [1 - Py, (10.23)

where (L) = "~~ +;_ ,,L' and as it is of order m — 1, then the unconditional
variance does not exist [25]. To allow for high persistence and long memory in the
conditional variance while avoiding the complications of IGARCH models, Baillie
et al. [13] extended the IGARCH case allowing for the integration coefficient to vary
between [o, 1] by using parallel argument from ARMA(p, q) process to ARFIMA(m,
d, q) processes. The FIGARCH(p, d, m) process is defined as follows:

PL) — L)‘e? = w + [1 - B(L)]y, (10.24)

where $(L) = ©" ~1;_ ,Z/ and is of order m - 1 and all the roots of o(z) = 0 and
B(L) = 0 lie outside the unit circle. When d = 0, this reduces to the usual GARCH
model, which has short memory; when d = 1, this becomes the JGARCH model,
which has infinite persistence and when 0 < d < 1, the fractionally differenced
squared residuals, (1 — L)*e2,, follows a stationary ARMA(m, q) process. The ARMA
representation of the FIGARCH process is given by
\n\n=== PAGE 342 ===\n(10.25)
(10.26)
(10.27)
(10.28)
(10.29)
(10.30)
and it is defined in terms of the conditional variance as
Baillie [13] referred to the equation (10.26) model as the fractionally integrated
GARCH, or FIGARCH(p, d, m) model. When 0 < d < 1, the coefficients in ϕ(L) and
β(L) capture the short-run dynamics of volatility, while the fractional difference
parameter d models the long-run characteristics of volatility.
10.4 Detection and estimation of long memory
In this section, we discuss some of the methods that are used to test and estimate
long memory parameter. First, the unit root tests (ADF and KPSS) are discussed
followed by the semiparametric, Whittle estimation of long memory long
parameters.
10.4.1 AUGMENTED DICKEY–FULLER TEST(ADF TEST)
The Dickey–Fuller test is based on the AR(1) models
and
where ϵt denotes the error term. The null hypothesis is that there is a unit root; that
is, H0 : ϕ1 = 1, and the alternative H1: ϕ1 < 1, which is stationarity or fractional
integration, assuming that ϕ > −1. This is the well-known unit-root testing problem;
see Dickey and Fuller [29]. The test statistic is the t ratio of the least squares (LS)
estimate of ϕ1 under the null hypothesis. For (27), the LS method gives
where y0 = 0 and T is the sample size. The t ratio is given by:
which is commonly referred to as the Dickey-Fuller (DF) test. If {ϵt} noise series with
finite moments of order slightly greater than 2, then the DF statistic converges to a
function of the standard Brownian motion as 
 (see Chan and Wei [30] and
\n\n=== OCR PAGE 342 ===\neF =o@t([1- PL - Loy, (10.25)

and it is defined in terms of the conditional variance as

t

o =o+[1-(1- ply (1-1) - pb)] e?. (10.26),

Baillie [13] referred to the equation (10.26) model as the fractionally integrated
GARCH, or FIGARCH(p, d, m) model. When 0 < d < 1, the coefficients in @(L) and
B(L) capture the short-run dynamics of volatility, while the fractional difference
parameter d models the long-run characteristics of volatility.

10.4 Detection and estimation of long memory

In this section, we discuss some of the methods that are used to test and estimate
long memory parameter. First, the unit root tests (ADF and KPSS) are discussed
followed by the semiparametric, Whittle estimation of long memory long
parameters.

10.4.1 AUGMENTED DICKEY—FULLER TEST(ADF TEST)
The Dickey—Fuller test is based on the AR(1) models

Y= Pat &p (10.27)

and

Y= Pot Pi + &- (10.28)

where €; denotes the error term. The null hypothesis is that there is a unit root; that
is, Hy: @, = 1, and the alternative H,: , < 1, which is stationarity or fractional
integration, assuming that ¢ > -1. This is the well-known unit-root testing problem;
see Dickey and Fuller [29]. The test statistic is the t ratio of the least squares (LS)
estimate of ¢, under the null hypothesis. For (27), the LS method gives

>, LO - Pi) (10.29)

‘ T-1

pol (10.30)

std(p,)

DF =t ratio=

which is commonly referred to as the Dickey-Fuller (DF) test. If {€,} noise series with

finite moments of order slightly greater than 2, then the DF statistic converges to a
function of the standard Brownian motion as T —> oo (see Chan and Wei [30] and
\n\n=== PAGE 343 ===\n(10.31)
(10.32)
(10.33)
Phillips [31]). If ϕ0 = 0 but (27) is employed, then the resulting t ratio for testing ϕ1
= 1 will converge to another nonstandard asymptotic distribution. In either case,
simulation is used to obtain critical values of the test statistics for selected critical
values [32]. If ϕ0 ≠ 0 and (28) is used, then the t ratio for testing ϕ1 = 1 is
asymptotically normal. However, large sample sizes are needed for the asymptotic
normal distribution to hold.
10.4.2 KPSS TEST
The ADF unit root tests are for the null hypothesis that a time series yt is I(1).
Stationarity tests, on the other hand, are for the null that yt is I(0). The most
commonly used stationarity test, the KPSS test, is due to Kwiatkowski, Phillips,
Schmidt, and Shin [33]. The test is based on the equations
where Dt contains deterministic components (constant or constant plus time trend),
ut is I(0), may be heteroskedastic and is a pure random walk with innovation
variance σ2
ϵ. The null hypothesis that yt is I(0) is formulated as H0 : σ2
ϵ = 0, which
implies that μt is a constant. Although not directly apparent, this null hypothesis also
implies a unit moving average root in the ARMA representation of Δyt. The KPSS
test statistic is the Lagrange multiplier (LM) or score statistic for testing σ2
ϵ = 0
against the alternative that σ2
ϵ > 0 and is given by
where 
 
 is the residual of a regression of yt on Dt and 
 is a consistent
estimate of the long-run variance of μt using 
. Under the null that yt is I(0),
Kwiatkowski, Phillips, Schmidt, and Shin showed that KPSS converges to a function
of standard Brownian motion that depends on the form of the deterministic terms Dt
but not the coefficient values of β. If Dt = 1 then
where V1(r) = W(r) − rW(1) and W(r) is a standard Brownian motion for r ∈ [0, 1].
If 
 then
\n\n=== OCR PAGE 343 ===\nPhillips [31]). If @, = 0 but (27) is employed, then the resulting t ratio for testing ,
= 1 will converge to another nonstandard asymptotic distribution. In either case,
simulation is used to obtain critical values of the test statistics for selected critical
values [32]. If @, # 0 and (28) is used, then the ¢ ratio for testing @, = 1 is

asymptotically normal. However, large sample sizes are needed for the asymptotic
normal distribution to hold.

10.4.2 KPSS TEST

The ADF unit root tests are for the null hypothesis that a time series y; is I(1).
Stationarity tests, on the other hand, are for the null that y; is (0). The most

commonly used stationarity test, the KPSS test, is due to Kwiatkowski, Phillips,
Schmidt, and Shin [33]. The test is based on the equations

y, = BD, + hy (10.31)

My = Mite, €p~ WN(0, 0?)
where D, contains deterministic components (constant or constant plus time trend),
u, is I(o), may be heteroskedastic and is a pure random walk with innovation
variance o°,. The null hypothesis that y, is I(0) is formulated as Hy : o?, = 0, which
implies that y, is a constant. Although not directly apparent, this null hypothesis also
implies a unit moving average root in the ARMA representation of Ay;. The KPSS
test statistic is the Lagrange multiplier (LM) or score statistic for testing 07, = 0
against the alternative that 0, > o and is given by

2yT
KPSS = oa) (10.92)
Re

- fn. .
where! = d= Hi» Fi is the residual of a regression of y, on D, and /7 is a consistent
estimate of the long-run variance of p, using #. Under the null that y; is I(0),

Kwiatkowski, Phillips, Schmidt, and Shin showed that KPSS converges to a function
of standard Brownian motion that depends on the form of the deterministic terms D,

but not the coefficient values of B. If D, = 1 then

a f! 10.
KPSs 5 [ V,()dr (10.33)
0

where V,(r) = W(r) - rW(a) and W(r) is a standard Brownian motion for r € [o, 1].
1fD, = (1.0 then
\n\n=== PAGE 344 ===\n(10.34)
(10.35)
where V2(r) = W(r) + r(2 − 3r)W(1) + 6r(r2 − 1)∫1
0W(s)d(s). Critical values from the
asymptotic distributions (33) and (34) are obtained by simulation methods, and
these are summarized in Table 10.1 below.
Table 10.1 Quantiles of the distribution of the KPSS statistic.
Right Tail Quantiles
Distribution 0.90
0.925 0.950 0.975
0.99
∫1
0V1(r)
0.349 0.396 0.446 0.592
0.762
∫2
0V2(r)
0.120 0.133 0.149 0.184
0.229
The stationary test is a one-sided right-tailed test so that we reject the null of
stationarity at the 100α% level if the KPSS test statistic (32) is greater than the 100(1
− α)% quantile from the appropriate asymptotic distribution (33) or (34).
10.4.3 WHITTLE METHOD
This methodology gives approximate maximum-likelihood estimates of the long
memory paramaters based on the calculation of the periodogram by means of the
fast Fourier transform (FFT) and the use of the Whittle approximation of the
Gaussian log-likelihood function.
Suppose that the sample vector Y = (y1, …, yn) is normally distributed with zero
mean and variance Γθ. Then, the log-likelihood function divided by the sample size is
given by
where the variance–covariance matrix Γθ may be expressed in terms of the spectral
density of the process fθ( · ) as follows:
where
Two approximations are made to obtain the Whittle method. Since
\n\n=== OCR PAGE 344 ===\na f! 10.
KPSs 5 [ Vo(n)dr (10:34)
0

where V,(r) = W(r) + r(2 - 3r)W(1) + 6r(r? - 1) f1,W(s)d(s). Critical values from the
asymptotic distributions (33) and (34) are obtained by simulation methods, and
these are summarized in Table 10.1 below.

Table 10.1 Quantiles of the distribution of the KPSS statistic.
Right Tail Quantiles

Distribution 0.90 0.925 0.950 0.975 0.99
PV) 0.349 0.396 0.446 0.592 0.762
J2oVolr) 0.120 0.133 0.149 0.184 0.229

The stationary test is a one-sided right-tailed test so that we reject the null of
stationarity at the 100a%é level if the KPSS test statistic (32) is greater than the 100(1
— a)% quantile from the appropriate asymptotic distribution (33) or (34).

10.4.3 WHITTLE METHOD

This methodology gives approximate maximum-likelihood estimates of the long
memory paramaters based on the calculation of the periodogram by means of the
fast Fourier transform (FFT) and the use of the Whittle approximation of the
Gaussian log-likelihood function.

Suppose that the sample vector Y = (y,, ..., y;) is normally distributed with zero
mean and variance Ig. Then, the log-likelihood function divided by the sample size is
given by

2 1 Lt 10.
£(0) = —5- log det Vy ~ =-YT'Y. (10.35)

where the variance—covariance matrix I'y may be expressed in terms of the spectral
density of the process fo( - ) as follows:

(U9); = Yoli- J).
where
rn) = [ So Adexp(iAkda.

Two approximations are made to obtain the Whittle method. Since
\n\n=== PAGE 345 ===\n(10.36)
(10.37)
as 
 the first term of (39) is approximated by
and the second approximated by
where
is the periodogram of the series {yt}. Thus, the log-likelihood function is
approximated, up to a constant, by
with a discrete version given by
10.5 Data collection, analysis, and result
The analysis that follows is focused on the daily prices of the Dow Jones Industrial
Average (DJIA). Volatility analysis includes fitting various GARCH specification to
study the volatility structure of the return series. The sample data includes daily
prices, obtained for the period March 14, 2003, to October 10, 2011, comprising 2164
data points (Yahoo Finance). We concentrated on this duration because we wanted
to include a period that involves a crash (September 2008).
10.5.1 ANALYSIS ON DOW JONES INDEX (DJIA) RETURNS
The time plot of the series in Figure 10.1 shows that the mean of the series changes
over time with maximum value of 14165 on 10-9-2007 and minimum value of 6547
on 3-9-2009. The histogram and normal Q–Q plot indicate that the emperical
distribution of the series is nonnormal and skewed to the right. From Figure 10.1 we
can see that the log of the prices changes over time, which suggests that the series is
\n\n=== OCR PAGE 345 ===\nI log det ">, — x/ log|2xf,(A)|dA,
n 2x j_,

asn —> oothe first term of (39) is approximated by

1 1 *
On log det T >» = re [ log|2xf,(A)|dA.

and the second approximated by

tyrtyet [POM a,
2n ° An J_, fy(A)
where

n

Y yyexp(iaj)

l
1A) = = 2

is the periodogram of the series {y;}. Thus, the log-likelihood function is
approximated, up to a constant, by

2 1 * * (A) (10.36)
L(0) = -— logf)(A)da + 1a] :
4x U/- “fa on f(A)
with a discrete version given by
(A (20.37)

£(6) = -

1
2n

logfy(A;) + i
py , Die

10.5 Data collection, analysis, and result

The analysis that follows is focused on the daily prices of the Dow Jones Industrial
Average (DJIA). Volatility analysis includes fitting various GARCH specification to
study the volatility structure of the return series. The sample data includes daily
prices, obtained for the period March 14, 2003, to October 10, 2011, comprising 2164
data points (Yahoo Finance). We concentrated on this duration because we wanted
to include a period that involves a crash (September 2008).

10.5.1 ANALYSIS ON DOW JONES INDEX (DJIA) RETURNS

The time plot of the series in Figure 10.1 shows that the mean of the series changes
over time with maximum value of 14165 on 10-9-2007 and minimum value of 6547
on 3-9-2009. The histogram and normal Q—Q plot indicate that the emperical
distribution of the series is nonnormal and skewed to the right. From Figure 10.1 we
can see that the log of the prices changes over time, which suggests that the series is
\n\n=== PAGE 346 ===\nnonstationary. By performing the unit root test on the series, we found that the ADF
test statistic (−2.2801) is higher than the critical value at a 5% significance level
(−2.86431), indicating that we accept the null hypothesis that there is a unit root in
the series. This is also supported by a p-value of 0.4598.
To eliminate the unit root, we found the first difference in ln (Prices), that is,
, and did the test again. ADF test statistic for the
return series is (−12.3213) with a p-value of 0.001, indicating that we reject the null
hypothesis of a unit root in the series. Hence we conclude that the returns series is
stationary.
FIGURE 10.1 Log daily price, distribution, and normal Q–Q plot of the DJIA prices
from March 14, 2003, to October 10, 2011.
FIGURE 10.2 First difference of the log prices (returns series).
\n\n=== OCR PAGE 346 ===\nnonstationary. By performing the unit root test on the series, we found that the ADF

test statistic (-2.2801) is higher than the critical value at a 5% significance level

(-2.86431), indicating that we accept the null hypothesis that there is a unit root in
the series. This is also supported by a p-value of 0.4598.

To eliminate the unit root, we found the first difference in In (Prices), that is,

returns = In(Price), — In(Price),_), and did the test again. ADF test statistic for the
return series is (-12.3213) with a p-value of 0.001, indicating that we reject the null
hypothesis of a unit root in the series. Hence we conclude that the returns series is

stationary.
pon
94 te! yet, oe mts
4 92 gael NE err a ‘\ Fae rN bal
3
3 04 \ ee
as \
2008 2006 2008 2010 2ore
Years
Distribution: Normal Q-Q plot
5
4 gos
be he
° as
ae 90 92 oa 3 2 4 0 4 3
Theoretical quantiles
FIGURE 10.1 Log daily price, distribution, and normal Q—Q plot of the DJIA prices

from March 14, 2003, to October 10, 2011.

0.10

0.05

Returns

0.00

0.05

88s

Probability

88

FI

Distribution

Normal Q-@ plot

2012

0.05 0.00 0.05 0.10

RE 1:

2 First difference of the log

3s 2 4-1 ° 1
Theoretical quantiles

prices (returns series).
\n\n=== PAGE 347 ===\nFrom Figure 10.2, the return series appears to be stationary around its mean, the
histogram looks symmetric with high kurtosis with heavy tails, and the normal q–q
plot indicates nonnormal series with outliers. The autocorrelation and the partial
autocorrelation functions (ACF/PACF) for the returns are illustrated in Figure 10.3.
From Figure 10.3, the ACF and PACF showed dependency in the return series that
required correlation structure in the conditional mean.
FIGURE 10.3 ACF and PACF of returns.
10.5.2 MODEL SELECTION AND SPECIFICATION: CONDITIONAL
MEAN
From Table 10.2, ARMA(2,2) has the minimum AIC( − 12908.44) and AICc( −
12908.39) with four significant parameter estimates shown in Table 10.3.
Table 10.2 Model selection by AICc.
ARMA(p,q)
AIC
AICc
BIC
ARMA(1,0)
−12,896.19 −12,896.18 −12,879.16
ARMA(0,1)
−12,900.45 −12,900.44 −12,883.42
ARMA(1,1)
−12,901.27 −12,901.25 −12,878.55
ARMA(2,1)
−12,906.68 −12,906.65 −12,878.28
ARMA(2,2)
−12,908.44 −12,908.39 −12,874.35
ARMA(3,0)
−12,908.11 −12,908.08 −12,879.71
ARMA(0,3)
−12,906.29 −12,906.26 −12,877.89
\n\n=== OCR PAGE 347 ===\nFrom Figure 10.2, the return series appears to be stationary around its mean, the
histogram looks symmetric with high kurtosis with heavy tails, and the normal q—q
plot indicates nonnormal series with outliers. The autocorrelation and the partial
autocorrelation functions (ACF/PACF) for the returns are illustrated in Figure 10.3.

From Figure 10.3, the ACF and PACF showed dependency in the return series that
required correlation structure in the conditional mean.

ACF
0.05 | < .
4 0.00 ae AS a a,
ae ae a a
-0.10
5 10 15 20
Lag
PACF
2 oof = —— a —
Ll |
5 10 15 20
Lag

FIGURE 10.3 ACF and PACF of returns.

10.5.2 MODEL SELECTION AND SPECIFICATION: CONDITIONAL

MEAN

From Table 10.2, ARMA(2,2) has the minimum AIC( - 12908.44) and AIC,( -—

12908.39) with four significant parameter estimates shown in Table 10.3.

Table 10.2 Model selection by AIC,.

ARMA(p,q) AIC AIC, BIC

ARMAG,0) -12,896.19 -12,896.18 -12,879.16
ARMA(0,1) —12,900.45 -12,900.44 -12,883.42
ARMAG,1)  -12,901.27 -12,901.25 -12,878.55
ARMA(2,1) —12,906.68 -12,906.65 -12,878.28
ARMA(2,2) -12,908.44 -12,908.39 -12,874.35
ARMA(3,0) —12,908.11 —12,908.08 -12,879.71

ARMA(0,3)

-12,906.29 -12,906.26 -12,877.89
\n\n=== PAGE 348 ===\n(10.38)
Table 10.3 ARMA(2,2) model’s parameter estimates and standard errors.
Variable
Coeff
Std. Error t-Stat
Prob
AR(1)
−0.4035 0.1860
−2.437 0.01479
AR(2)
−0.3948 0.1215
−2.930 0.00339
MA(1)
0.2810
0.127
2.213
0.02700
MA(2)
0.2955
0.1283
2.071
0.03835
AIC: −12912    σ = 0.0122.
Table 10.4 Statistics of standardized residual.
Statistics Value
Statistics
Value
Mean
0.0000 SD
1.000
Median
0.0514 Skew
−0.254
Minimum −6.632 Kurtosis
9.340
Maximum 8.097
Jarque–Bera 7908(5.99)
10.5.3 CONDITIONAL MEAN MODEL (RETURNS)
The ARMA(p, q) model states that the current value of some series rt depends
linearly on its own previous values plus a combination of current and previous
values of a white noise error term ϵt.
Our model for the conditional mean of the DJIA returns is ARMA(2, 2) given by
10.5.4 MODEL DIAGNOSTICS: ARMA(2, 2)
The time plot of the standardized residuals in Figure 10.4 shows changing volatility
and outliers in the series. The ACF of the standardized residuals shows no apparent
departure from the model assumptions. The normal q–q plot of the residuals shows
departure from normality at the tails due to the outliers and the changing volatility
in the return series. The empirical distribution of residuals indicated excess kurtosis
(9.34) and skewness (−0.254) with Jarque–Bera higher than 5.99 (7908) at 5%
significance. This indicates nonnormality of standardized residuals.
\n\n=== OCR PAGE 348 ===\nTable 10.3 ARMA(2,2) model’s parameter estimates and standard errors.
Variable Coeff Std. Error t-Stat Prob

ARQ) -0.4035 0.1860 -2.437 0.01479
AR(2) -0.3948 0.1215 —2.930 0.00339
MA() 0.2810 0.127 2.213 0.02700

MA(2) 0.2955 0.1283 2.071 0.03835
AIC: -12912 0 = 0.0122.
Table 10.4 Statistics of standardized residual.

Statistics Value Statistics Value

Mean 0.0000 SD 1.000
Median 0.0514 Skew -0.254
Minimum -—6.632 Kurtosis 9.340

Maximum 8.097 Jarque—Bera 7908(5.99)

10.5.3 CONDITIONAL MEAN MODEL (RETURNS)

The ARMA(p, q) model states that the current value of some series r; depends
linearly on its own previous values plus a combination of current and previous
values of a white noise error term €;.

Our model for the conditional mean of the DJIA returns is ARMA(2, 2) given by

r, = -0.4035r,_, — 0.3948r,_> + 0.2810€,_; + 0.2955e,_» + €, (10.38)

10.5.4 MODEL DIAGNOSTICS: ARMA(2, 2)

The time plot of the standardized residuals in Figure 10.4 shows changing volatility
and outliers in the series. The ACF of the standardized residuals shows no apparent
departure from the model assumptions. The normal q—q plot of the residuals shows
departure from normality at the tails due to the outliers and the changing volatility
in the return series. The empirical distribution of residuals indicated excess kurtosis
(9.34) and skewness (—0.254) with Jarque—Bera higher than 5.99 (7908) at 5%
significance. This indicates nonnormality of standardized residuals.

\n\n=== PAGE 349 ===\nFIGURE 10.4 Time plot, ACF, histogram, and normal q–q plot of standardized
residuals.
The model appears to fit well except for the fact that a distribution with heavier tails
than the normal distribution should be employed. This is our motivation for fitting
GARCH to the model residuals.
10.5.5 TEST FOR ARCH EFFECT
The analysis continued with a test for an ARCH effect presence in the specified
model ARMA(2, 2). First, we looked at ACFs of the squared residuals and squared
returns. Figure 10.5 shows the ACF of the squared residuals of the fitted model and
squared returns, respectively. The ACFs showed dependence in both the squared
residuals and squared returns. We also saw that the residuals are not normally
distributed, which also suggest the presence of ARCH effect in the series. This is
confirmed by the Box–Ljung test statistics X-squared of 1994.54 with 0.00 p-value
for the squared residuals, and 1921.53 with 0.00 p-value for the squared returns.
\n\n=== OCR PAGE 349 ===\nStandardized residuals

g
H 5 t
go Rae T | T
‘
“s
00s
3 oo ~CO SC H 7° Pa Pa
ts
Distribution Normal Q-a plot

os

06

o4

Probability

\

02
es

00

5 ° 5 eo <4 o 1 2 3
Theoretical quantiles

FIGURE 10.4 Time plot, ACF, histogram, and normal q—q plot of standardized
residuals.

The model appears to fit well except for the fact that a distribution with heavier tails
than the normal distribution should be employed. This is our motivation for fitting
GARCH to the model residuals.

10.5.5 TEST FOR ARCH EFFECT

The analysis continued with a test for an ARCH effect presence in the specified
model ARMA(2, 2). First, we looked at ACFs of the squared residuals and squared
returns. Figure 10.5 shows the ACF of the squared residuals of the fitted model and
squared returns, respectively. The ACFs showed dependence in both the squared
residuals and squared returns. We also saw that the residuals are not normally
distributed, which also suggest the presence of ARCH effect in the series. This is
confirmed by the Box—Ljung test statistics X-squared of 1994.54 with 0.00 p-value
for the squared residuals, and 1921.53 with 0.00 p-value for the squared returns.

\n\n=== PAGE 350 ===\nFIGURE 10.5 ACF and PACF of the squared residuals and squared returns.
10.5.6 MODEL SELECTION AND SPECIFICATION: CONDITIONAL
VARIANCE
The model selected by AIC criterior is FIGARCH(1, d, 1) with AIC(−6.554) and the
long memory parameter d = 0.602. Table 10.5 shows the model specification, AIC,
and the long memory parameter. The four significant parameter estimates and their
standard errors are shown in Table 10.6.
Table 10.5 Model selection by AIC.
Model
AIC
d
GARCH(1, 1)
− 6.545
0
IGARCH(1, 1)
− 6.544
1
FIGARCH(0, d, 0) − 6.507 0.215
FIGARCH(1, d, 1) − 6.554 0.602
GARCH(2, 1)
− 6.552
0
IGARCH(2, 1)
− 6.551
1
Table 10.6 FIGARCH(1,d,1) model’s parameter estimates and standard errors.
Variable
Coeff
Std. error t-stat Prob
omega
0.03816 0.01523
2.505 0.012
ARCH(1)
0.05652 0.02680
2.109 0.035
GARCH(1) 0.66672 0.08811
7.567 0.000
d
0.60164 0.11648
5.165 0.000
AIC: −6.554.
\n\n=== OCR PAGE 350 ===\nACF standardized residuals

ACF standardized squared residuals

oa tit Tal)
fT] I TANT
a eS MIMI

FIGURE 10.5 ACF and PACF of the squared residuals and squared returns.

10.5.6 MODEL SELECTION AND SPECIFICATION: CONDITIONAL
VARIANCE

The model selected by AIC criterior is FIGARCH(A, d, 1) with AIC(—6.554) and the
long memory parameter d = 0.602. Table 10.5 shows the model specification, AIC,
and the long memory parameter. The four significant parameter estimates and their
standard errors are shown in Table 10.6.

Table 10.5 Model selection by AIC.

Model AIC d
GARCHA, 1) -6.545 O
IGARCH(, 1) -6.544 1

FIGARCH(0, d, 0) - 6.507 0.215
FIGARCHA(, d, 1) - 6.554 0.602
GARCH(2, 1) -6.552 O
IGARCH(2, 1) -6.551 1
Table 10.6 FIGARCH(1,d,1) model’s parameter estimates and standard errors.
Variable Coeff Std. error t-stat Prob

omega 0.03816 0.01523 2.505 0.012
ARCH(1) 0.05652 0.02680 2.109 0.035
GARCH(1) 0.66672 0.08811 7.507 0.000

d 0.60164 0.11648 5.165 0.000

AIC: -6.554.
\n\n=== PAGE 351 ===\n(10.39)
(10.40)
Table 10.7 Standardized residuals test.
Residual tests Variable Test stat Test value Prob
Jarque–Bera test
R
χ2
265.50
0.000
Ljung–Box test
R
Q(15)
24.03
0.065
Ljung–Box test
R2
Q(15)
13.04
0.445
LM Arch test
R
TR2
13.041
0.4068
The estimates of the FIGARCH(1,d,1) are ω(0.03816), ϕ(0.05652), β(0.66672), and
d(0.60164) with respective probabilities of 0.012, 0.035, 0.00, and 0.00. The
assumed conditional volatility model for the return series is given by
10.5.7 STANDARDIZED RESIDUALS TEST
The Jarque–Bera test for normality was 265.50 and the ARCH LM of 13.041 with
respective p-values of 0.000 and 0.4068, indicating that there is no ARCH in the
standardized squared residuals. The Ljung–Box statistics of standardized residuals
for autocorrelation at lag 15 = 24.03 with a p-value of 0.065, and the standardized
squared residuals at lag 15 = 13.04 with a p-value of 0.445, which indicates no serial
correlation standardized squared residual. These values indicate that the model is
adequate in describing the dynamic volatility. Figure 10.6 shows the conditional
standard deviation (top) that changes over time and the conditional standard
deviation superimposed on the returns (bottom). From the last figure, we can see
that the assumed model captures well the volatility of the returns.
\n\n=== OCR PAGE 351 ===\nTable 10.7 Standardized residuals test.
Residual tests Variable Test stat Test value Prob

Jarque—Bera test R a 265.50 0.000
Ljung—Box test R QG5) 24.03 0.065
Ljung-Box test R2 QG5) 13.04 0.445
LM Arch test R TR2 13.041 0.4068

The estimates of the FIGARCH(1,d,1) are w(0.03816), (0.05652), B(0.66672), and
d(0.60164) with respective probabilities of 0.012, 0.035, 0.00, and 0.00. The
assumed conditional volatility model for the return series is given by

r,=€, €,~ GED(0,07) (10.39)

o> = 0.038 + [1 — (1 — 0.667L)"'(1 — L)°°"(1 — 0.057L)| €? (10.40)

10.5.7 STANDARDIZED RESIDUALS TEST

The Jarque—Bera test for normality was 265.50 and the ARCH LM of 13.041 with
respective p-values of 0.000 and 0.4068, indicating that there is no ARCH in the
standardized squared residuals. The Ljung—Box statistics of standardized residuals
for autocorrelation at lag 15 = 24.03 with a p-value of 0.065, and the standardized
squared residuals at lag 15 = 13.04 with a p-value of 0.445, which indicates no serial
correlation standardized squared residual. These values indicate that the model is
adequate in describing the dynamic volatility. Figure 10.6 shows the conditional
standard deviation (top) that changes over time and the conditional standard
deviation superimposed on the returns (bottom). From the last figure, we can see
that the assumed model captures well the volatility of the returns.
\n\n=== PAGE 352 ===\nFIGURE 10.6 Conditional standard superimposed on the returns.
10.5.8 MODEL DIAGNOSTICS
The time plot of the standardized residuals in Figure 10.7 shows no obvious patterns,
but we notice a spike around the 1000th observation. The ACF of the standardized
residuals and squared standardized residuals in Figure 10.8 shows no apparent
departure from the model assumptions. The histogram and generalized error q–q
plot of the standardized residuals show no departure from model assumptions (i.e.,
the assumed conditional distribution captured the high kurtosis and the heavy tails
of the residuals). This suggests that the residuals are independent generalized error
distribution (GED); hence, the model is adequate to describe the changing volatility
of the returns.
\n\n=== OCR PAGE 352 ===\nConditional SD

0.05
2
3 0.03
oO
> 0.01
i) 500 1000 1500 2000
Time
Series with 2 conditional SD superimposed
0.10 7
¢ 0.05
2
fd
-0.05
Time
FIGURE 10.6 Conditional standard superimposed on the returns.
10.5.8 MODEL DIAGNOSTICS

The time plot of the standardized residuals in Figure 10.7 shows no obvious patterns,
but we notice a spike around the 1000th observation. The ACF of the standardized
residuals and squared standardized residuals in Figure 10.8 shows no apparent
departure from the model assumptions. The histogram and generalized error q-q
plot of the standardized residuals show no departure from model assumptions (i.e.,
the assumed conditional distribution captured the high kurtosis and the heavy tails
of the residuals). This suggests that the residuals are independent generalized error
distribution (GED); hence, the model is adequate to describe the changing volatility
of the returns.

\n\n=== PAGE 353 ===\nFIGURE 10.7 Time plot of standardized residual.
FIGURE 10.8 ACF and distribution of the squared residuals.
\n\n=== OCR PAGE 353 ===\nStandardized residuals

te) 500 1000 1500 2000
Time
FIGURE 10.7 Time plot of standardized residual.
ACF standardized residuals ACF standardized squared residuals
010 010 4
005 4 005 4
§ 000 § 000 Gael SiS! Sl ES all
-0.05 4 -0.08 +
-o104 . -010 4 .
o 5 10 1S 20 2 30 o 5 10 15 20 2 3
lag lag
= ‘Sged Q-Q plot
06
os 24
204 | o4+
os 4 |
oa if t
| i
os 1 1
be a T T T a
“4 2 ° 2 ‘
Theoretical quantiles

FIGURE 10.8 ACF and distribution of the squared residuals.
\n\n=== PAGE 354 ===\nFIGURE 10.9 The conditional standard deviation and standard residuals of
MA(1)+FIGARCH(1,d,1).
10.5.9 RETURNS AND VARIANCE EQUATION
We finally considered ARMA with FIGARCH variance. The assumed mean model for
the DJIA return series was ARMA(2,2) and that of the variance FIGARCH(1,d,1);
hence, we first looked at ARMA(2,2)+ FIGARCH(1,d,1). Its AIC was −6.559, but
three of the parameter estimates were not significant as shown in Table 10.8, so we
considered a simpler model with similar AIC −6.558, the MA(1)+FIGARCH(1,1),
where all the parameter estimates were significant as shown in Table 10.9.
Table 10.8 ARMA(2,2)+FIGARCH(1,d,1) model’s parameter estimates.
Variable
Coeff
Std. error t-stat
Prob
AR(1)
−0.302479 0.17610
−1.718 0.0860
AR(2)
−0.536389 0.20150
−2.662 0.0078
MA(1)
0.250994
0.17683
1.419
0.1559
MA(2)
0.511098
0.20387
2.507
0.0123
omega
0.040077
0.016379
2.447
0.0145
d-Figarch 0.598114
0.11836
5.053
0.0000
ARCH(1)
0.043609
0.052057
0.8377 0.4023
GARCH(1) 0.659513
0.092828
7.105
0.0000
AIC: −6.559.
\n\n=== OCR PAGE 354 ===\nStandardized residuals

2
3 0
2-2
> -4
-6
0 500 1000 1500 2000
Time

Conditional SD

. 0.05 \
3 003 1 VA "
A a PE Oe ee ee
0 500 1000 1500 2000
Time

FIGURE 10.9 The conditional standard deviation and standard residuals of
MA(1)+FIGARCH(1,d,1).

10.5.9 RETURNS AND VARIANCE EQUATION

We finally considered ARMA with FIGARCH variance. The assumed mean model for
the DJIA return series was ARMA(2,2) and that of the variance FIGARCH(1,d,1);
hence, we first looked at ARMA(2,2)+ FIGARCH(1,d,1). Its AIC was —6.559, but
three of the parameter estimates were not significant as shown in Table 10.8, so we
considered a simpler model with similar AIC —6.558, the MA(1)+FIGARCH(1,1),
where all the parameter estimates were significant as shown in Table 10.9.

Table 10.8 ARMA(2,2)+FIGARCH(1,d,1) model’s parameter estimates.

Variable Coeff Std.error t-stat Prob
AR() —0.302479 0.17610 -1.718 0.0860
AR(2) -0.536389 0.20150 -2.662 0.0078
MAQ) 0.250994 0.17683 1.419 0.1559
MA(2) 0.511098 0.20387 2.507 0.0123
omega 0.040077 0.016379 2.447 0.0145
d-Figarch 0.598114 0.11836 5.053 0.0000
ARCH(1) 0.043609 0.052057 0.8377 0.4023
GARCH(1) 0.659513 0.092828 7.105 0.0000
AIC: -6.559-
\n\n=== PAGE 355 ===\n(10.41)
(10.42)
Table 10.9 ARMA(0,1)+FIGARCH(1,d,1) model’s parameter estimates.
Variable
Coeff
Std. error t-stat
Prob
MA(1)
−0.059553 0.018602
−3.201 0.0014
omega
0.039181
0.015568
2.517
0.0119
d-Figarch
0.597023
0.11449
5.215
0.0000
ARCH(Phi1)
0.049773
0.052101
0.9553 0.0395
GARCH(Beta1) 0.663531
0.087988
7.541
0.0000
AIC: −6.558.
Table 10.10 Standardized residuals test.
Residual tests Variable Test stat Test value Prob
Jarque–Bera test
R
χ2
237.12
0.000
Ljung–Box test
R
Q(15)
13.114
0.5176
Ljung–Box test
R2
Q(15)
10.259
0.7430
LM Arch test
R
TR2
7.75041
0.6532
The assumed conditional mean model with conditional variance is MA(1) +
FIGARCH(1, d, 1) with generalized error given by
The MA(1) parameter estimate is −0.059553 with 0.00 p-values. The estimates of
the FIGARCH(1,d,1) are ω(0.039181), ϕ(0.049773), β(0.663531), and d(0.597023)
with respective probabilities 0.012, 0.040, 0.00, and 0.00. The estimated
parameters and their standard error are shown in Table 5.8.
10.5.10 STANDARDIZED RESIDUALS TEST
The Jarque–Bera test for normality was 237.12 and the ARCH LM of 7.75041 with
respective p-values of 0.000 and 0.6532, which indicate nonnormality and no ARCH
in the standardized residuals. The Ljung–Box statistics of standardized residuals for
autocorrelation at lag 15 = 13.114 with a p-value of 0.5176, and the standardized
squared residuals at lag 15 = 10.259 with a p-value of 0.7430, which indicate no
serial correlation in both the standardized residuals and the standardized squared
residual. These values indicate that the model is adequate in describing the returns
series and the dynamic volatility.
\n\n=== OCR PAGE 355 ===\nTable 10.9 ARMA(o,1)+FIGARCH(1,d,1) model’s parameter estimates.

Variable Coeff Std.error t-stat Prob
MA(1) -0.059553 0.018602 —3.201 0.0014
omega 0.039181 0.015568 2.517 0.0119
d-Figarch 0.597023 0.11449 5.215 0.0000

ARCH(Phi1) 0.049773. 0.052101 0.9553 0.0395
GARCH(Beta1) 0.663531 0.087988 7.541 0.0000
AIC: -6.558.
Table 10.10 Standardized residuals test.

Residual tests Variable Test stat Test value Prob

Jarque—Bera test R x? 237.12 0.000
Ljung—Box test R Qs) 13.114 0.5176
Ljung—Box test R2 Qs) 10.259 0.7430
LM Arch test R TR2 7.75041 0.6532

The assumed conditional mean model with conditional variance is MA(1) +
FIGARCH(, d, 1) with generalized error given by

r, = —0.0596€,_;+€,  €, ~ GED(0,07) (10.41)

a; = 0.039 + [1 — (1 — 0.664L)'(1 — L)°°*"(1 — 0.0498L)] €? (10.42)

The MA(1) parameter estimate is —0.059553 with 0.00 p-values. The estimates of
the FIGARCH(1,d,1) are w(0.039181), (0.049773), B(0.663531), and d(0.597023)
with respective probabilities 0.012, 0.040, 0.00, and 0.00. The estimated
parameters and their standard error are shown in Table 5.8.

10.5.10 STANDARDIZED RESIDUALS TEST

The Jarque—Bera test for normality was 237.12 and the ARCH LM of 7.75041 with
respective p-values of 0.000 and 0.6532, which indicate nonnormality and no ARCH
in the standardized residuals. The Ljung—Box statistics of standardized residuals for
autocorrelation at lag 15 = 13.114 with a p-value of 0.5176, and the standardized
squared residuals at lag 15 = 10.259 with a p-value of 0.7430, which indicate no
serial correlation in both the standardized residuals and the standardized squared
residual. These values indicate that the model is adequate in describing the returns
series and the dynamic volatility.
\n\n=== PAGE 356 ===\n10.5.11 MODEL DIAGNOSTIC OF CONDITIONAL RETURNS WITH
CONDITIONAL VARIANCE
The time plot of the standardized residuals in Figure 10.9 shows no obvious patterns,
but we notice a spike closed to the 1000th observation. The ACFs of the standardized
residuals and standardized squared residuals show no apparent departure from the
model assumptions as shown in Figure 10.10. The histogram and the GED-QQ plot
in Figure 10.10 show that the residuals are iid GED errors. Hence, the model seems
to be adequate for the data. Consequently, the MA(1) + FIGARCH(1, d, 1) with GED
errors model in (41), (42) are adequate for describing the conditional mean of the
returns series and prediction at the 5% significance level.
FIGURE 10.10 MA(1)+FIGARCH(1,d,1) standardized residuals distribution.
FIGURE 10.11 S&P 500 returns and squared and absolute returns ACF and PACF.
\n\n=== OCR PAGE 356 ===\n10.5.11 MODEL DIAGNOSTIC OF CONDITIONAL RETURNS WITH
CONDITIONAL VARIANCE

The time plot of the standardized residuals in Figure 10.9 shows no obvious patterns,
but we notice a spike closed to the 1000th observation. The ACFs of the standardized
residuals and standardized squared residuals show no apparent departure from the
model assumptions as shown in Figure 10.10. The histogram and the GED-QQ plot
in Figure 10.10 show that the residuals are iid GED errors. Hence, the model seems
to be adequate for the data. Consequently, the MA(1) + FIGARCH(, d, 1) with GED
errors model in (41), (42) are adequate for describing the conditional mean of the
returns series and prediction at the 5% significance level.

0.04

0.02

A

0.04

06
os

Fos

os
He
01
00

FIGURE 10.10 MA(1)+FIGARCH(G,d,1)

ACF of standardized residuals ACF of squared standardized residuals
oos f
0.02 4
§ 0.00
0.04
1 3 5 7 9 11 13 15 17 19 2 23 25 27 29 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag lag
Empirical density of standardized residuals Ged Q-a plot
+ Normal den: 2
‘© Ged 01 fitted density 8
i]
27]
§-
6
r
“4 2 4

2

Theoretical quantiles

standardized residuals distribution.

ACF of observations PACF of observations
0.05
0.05
0.00
4, 000+ *
$
2 0.05
-0.10 0.10
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 1 3.5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag Log
ACF of squared observations ACF of absolute observations
04 04
03 03
“ 4“
& oe $02
on on
00 0+
1 3 5 7 9 11 13 15 17 19 21 25 25 27 29 31 33 1 3.5 7 8 11 13 15 17 19 21 23 25 27 29 31 33
lag

FIGURE 10.11 S&P 500 returns and squared and absolute returns ACF and PACF.
\n\n=== PAGE 357 ===\nFIGURE 10.12 S&P 500 volatility model diagnostics: Top are the ACF of the
standardized and standardized squared residual. Bottom are the distribution and
GED Q–Q plot for the fitted model.
FIGURE 10.13 BAC high-frequency returns and squared and absolute returns ACF
and PACF.
\n\n=== OCR PAGE 357 ===\nACF standardized residuals ACF standardized squared residuals

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 20 31 33
lag Lag
Emperical density of standardized residuals Ged Q-2 plot

os

04

‘Sample quantities
bhbowe

i os
3 02 5
E
on
2 -wte + J
a 8 68 «s 2 0 2 4@ " 2 3 2 Hi
Values: Theoretical quantities

FIGURE 10.12 S&P 500 volatility model diagnostics: Top are the ACF of the
standardized and standardized squared residual. Bottom are the distribution and
GED Q-Q plot for the fitted model.

ACF of observations PACF of observations
1 3 8 7 9 1113 15 17 19 21 23 25 27 2081 33 1 3.5 7 9 111915 17 19 21 23.25 27 29 31 39
lag Lag
ACF squared of observations ACF absolute of observations
0.08 4 = L PY
0.02
4 4
§ 000 § 010
0.00.
-008 :
TS 5 7 8 111915 17 1921 23 25 7 2 OID 1 3 8 7 9 1119 15 17 19 21 25 25 27 29 31 99
lag Lag

FIGURE 10.13 BAC high-frequency returns and squared and absolute returns ACF
and PACF.
\n\n=== PAGE 358 ===\nFIGURE 10.14 BAC conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q–Q plot for the fitted model.
FIGURE 10.15 JPM high-frequency returns and squared and absolute returns ACF
and PACF.
\n\n=== OCR PAGE 358 ===\nACF of standardized residuals ACF of squared standardized residuals

1 3 5 7 9 1119 15 17 19 21 23 25 27 29 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 38

Lag Lag
Empirical density of standardized residuals Ged Q-0 plot

88 8

‘Sample quantiles

FIGURE 10.14 BAC conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q-Q plot for the fitted model.

ACF of observations PACF of observations
0.20
0.15
0.10
0.05
0.05
1 3 5 7 9 11 13 15 17 19 21 23 25 27 20 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Lag Lag
ACF of squared observations ACF of absolute observations
0.30
0.04 1
0204
0.02
8 oo 8 oro
0.00
0.04
1 3 5 7 O11 19 15 17 19 21 23 25 27 29 31 39 1 3 5 7 O11 13 15 17 19 21 23 25 27 29 31 33
Lag Lag

FIGURE 10.15 JPM high-frequency returns and squared and absolute returns ACF
and PACF.
\n\n=== PAGE 359 ===\nFIGURE 10.16 JPM conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q–Q plot for the fitted model.
FIGURE 10.17 IBM high-frequency returns and squared and absolute returns ACF
and PACF.
\n\n=== OCR PAGE 359 ===\nACF of standardized residuals ACF of squared standardized residuals
008
0.024
*
i
20,04 fanaa eee -
TSS 7 911915 17 1921 25 BH TSS 7 ONS 17 192 TH
Lag Lag
Empirical density of standardized residuals Ged Q-0 plot
0.6 1 Noma dennty 2
os | sonst) tty
z of i"
| 03 20.
& o24 3
a Food
002, - - ————— ooo
-20 ° 20 4 60 3 2 4 09 1 2 3
Values. Theoretical quantiles
FIGURE 10.16 JPM conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q-Q plot for the fitted model.
ACF of observations: PACF of observations
8
3
$
8
z
TOS 7 O11 1921 2 TON
Lag
ACF of squared observations g
S$
+3] 23
S Hy
Sls ee ee eaS eee §
ms 13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33

lag

1 3 5 7 911 13 15 17 19 21 23 25 27 20 31 33
lag

FIGURE 10.17 IBM high-frequency returns and squared and absolute returns ACF

and PACF.
\n\n=== PAGE 360 ===\nFIGURE 10.18 IBM conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q–Q plot for the fitted model.
FIGURE 10.19 WMT high-frequency returns and squared and absolute returns
ACF and PACF.
\n\n=== OCR PAGE 360 ===\nACF of standardized residuals ACF of squared standardized residuals
0.04 0.04
oz 002
7 4 Sere ee eee |
§ 000 § 000
0.04 {= -0.04 :
TS 5 7 O11 1918 17 1921 23 25 OF 2 31 3S TS 5 7 91119 18 17 19 21 23 25 27 29 31 3S
lag Lag
Empirical density of standardized residuals, Ged 0-2 plot
08 Normal denaty »
0.5 +. Ged 01 fited density 3 2
Boa Bw -
os &
i SS
& 02 4
On 8 E
00 - D *
20 © -10 ° 10 20 30 6 4 2 ° 2 4 6
Values ‘Theoretical quantiles

FIGURE 10.18 IBM conditional volatility model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and

GED Q-Q plot for the fitted model.

ACF of observations

PACE of observations

1 3 5 7 O11 13 15 17 19 21 23 25 27 29 31 33

1 3 5 7 911 13 15 17 19 21 23 25 27 29 31 33

lag Lag
ACF of squared observations ACF of absolute observations
0.04 92
ove 0.18
% 4 0104
2 000 4
0.04 om

13 5 7 911 13 15 17 19 21 23 25 27 2951
lag

1 3 5 7 9 11 13 15 17 19 21 29 25 27 29 31 33
lag

FIGURE 10.19 WMT high-frequency returns and squared and absolute returns

ACF and PACF.
\n\n=== PAGE 361 ===\nFIGURE 10.20 WMT high-frequency conditional volatility model diagnostics.
FIGURE 10.21 Explosive series and squared and absolute series ACF and PACF.
\n\n=== OCR PAGE 361 ===\nACF of standardized residuals

ACF of squared standardized residuals

od

‘Sample quantiles

=10

0.04
1 3 5 7 911 19 15 17 19 21 28 25 27 29 31 38 1 3 5 7 911 19 15 17 19 21 29 25 27 29 31 38
Lag lag
Empirical density of standardized residuals Ged 2-0 plot

30
20

.
4 Ey ° 2 4

Theoretical quantiles

FIGURE 10.20 WMT high-frequency conditional volatility model diagnostics.

ACF of observations

PACF of observations

1 3 5 7 911 13 15 17 19 21 23 25 27 29 31 33
Lag

ACF of squared observations

3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33

1 3 5 7 911 13 15 17 19 21 23 25 27 29 31 33
Lag

06

04

ACF

02

00

3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag

FIGURE 10.21 Explosive series and squared and absolute series ACF and PACF.

\n\n=== PAGE 362 ===\nFIGURE 10.22 Explosive series conditional volatility model diagnostics.
10.5.12 ONE-STEP AHEAD PREDICTION OF LAST 10 OBSERVATIONS
We used the MA(1) with FIGARCH(1,d,1) variance model for the returns to predict
the last 10 observations by reconstructing a model each time with one-step-ahead
predictions of the next observation. The last observed return was −0.0218941. The
result is given in Table 10.11 below.
Table 10.11 Actual returns and prices to forecasted returns and prices.
Actual
Mean forecast Standard dev Prices Estimate prices
−0.024019
0.001408
0.01737 10,655
10,929
0.014208
0.001533
0.01778 10,809
10,672
0.011982
−0.0009184
0.01873 10,940
10,799
0.016536
−0.0007733
0.01806
11,123
10,931
−0.001906
−0.001063
0.01748
11,103
11,112
0.029206
0.0001227
0.01738 11,433
11,104
−0.001565
−0.001886
0.01632
11,416
11,412
0.008855
0.0001012
0.01816
11,519
11,417
−0.003629
−0.0005748
0.01702 11,478
11,512
0.014302
0.0002351
0.01621 11,644
11,481
\n\n=== OCR PAGE 362 ===\nACF standardized residuals ACF standardized squared residuals

os
os 004
o2
§ 4 5] 8 om
-02
-004
100 icin eo a]
7 TES TINDUET DANAE Ww
lag lag
Emperical density of standardized residuals Norm Q-0 plot
4 I,
03 3
2 z2
02 2
3 2°
© ai €
$2
oo =e
2 ° 2 4 3 2 «a 0 1 2 3
Valves Theoretical quantiles

FIGURE 10.22 Explosive series conditional volatility model diagnostics.

10.5.12 ONE-STEP AHEAD PREDICTION OF LAST 10 OBSERVATIONS

We used the MA(1) with FIGARCH(1,d,1) variance model for the returns to predict
the last 10 observations by reconstructing a model each time with one-step-ahead
predictions of the next observation. The last observed return was —0.0218941. The
result is given in Table 10.11 below.

Table 10.11 Actual returns and prices to forecasted returns and prices.

Actual Mean forecast Standard dev Prices Estimate prices

-0.024019 0.001408 0.01737 10,655 10,929
0.014208 0.001533 0.01778 10,809 10,672
0.011982 -0.0009184 0.01873 10,940 10,799
0.016536 —0.0007733 0.01806 11,123 10,931

—0.001906 -0.001063 0.01748 11,103 11,112
0.029206 0.0001227 0.01738 11,433 11,104

—0.001565 -0.001886 0.01632 11,416 11,412
0.008855 0.0001012 0.01816 11,519 11,417

-0.003629 -0.0005748 0.01702 11,478 11,512

0.014302 0.0002351 0.01621 11,644 11,481
\n\n=== PAGE 363 ===\nTable 10.12 Augmented Dickey–Fuller (ADF) unit root test values. H0:I(1), critical
values: −2.567 (1%); −1.941 (5%); −1.616 (10%) for the indices, HFD, earthquake
series, and the explosive series.
Symbol
DJIA
SP500
BAC
JPM
IBM
WMT
EQ2
EXP
Return
−12.321 −12.493 −14.75 −14.328 −14.752 −14.325 −11.352 −18.232
Sqd. Ret −4.7648 −4.833 −12.34 −13.001 −12.455 −12.456 −11.367 −5.628 
Abs. Ret −4.8911 −4.845 −9.193 −9.6733 −10.032 −10.032 −11.352 −4.361 
Table 10.13 Kwiatkowski, Phillips, Schmidt, and Shin (KPSS) unit root test values.
H0:I(0), critical values: 0.739 (1%); 0.463 (5%); 0.347 (10%) for the indices, HFD,
earthquake series, and the explosive series.
Symbol DJIA SP500 BAC JPM IBM WMT EQ2 EXP
Return
0.166
0.208 0.167 0.146 0.089 0.089 6.641 0.007
Sqd. Ret 1.549
1.724 0.108 0.099 0.075 0.075 6.850 0.867
Abs. Ret 2.544
2.968 0.979 1.271 0.598 0.598 6.641 1.233
Table 10.14 AutoRegressive Conditional Hectroscedastic test for correlation in
squared and absolute series. Critical values: 32.91 (0.1%); 26.22 (1%); and 21.03
(5%) for the indices, HFD, earthquake series, and the explosive series.
Symbol
DJIA
SP500
BAC
JPM
IBM
WMT
EQ2
EXP
ARCH LM
731.136
703.342 15.454 8.198 21.741 5.649 177.227 1299
Ljung. Box 2508.581 2539.780 18.621 8.838 23.145 5.938 364.196 4266
Pierce. Box 2498.418 2529.511 18.570 8.824 23.088 5.929 363.255 4251
Table 10.15 Whittle estimate of the long-memory parameter (d) for the indices,
HFD, earthquake series, and the explosive series, where d ∈ ( − 0.5, 0.5).
Symbol
DJIA
SP500
BAC
JPM
IBM
WMT
EQ2
EXP
Return
−0.083* −0.085* 0.031
0.091** −0.019 0.005
0.136** 0.499**
Sqd. Ret 0.199** 0.202** 0.042
0.042
0.038
0.035
0.140** 0.419**
Abs. Ret 0.218** 0.215** 0.183** 0.204** 0.145** 0.159** 0.136** 0.385**
* significant at 5%; **significant at 1%.
\n\n=== PAGE 364 ===\nTable 10.16 AIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum AIC is selected.
Symbol
DJIA SP500
BAC
JPM
IBM
WMT
EQ2
EXP
GARCH(1,1)
−6.544 −6.417 −11.1720 −11.042 −12.632 −12.713 2.508 −3.473
ARFIMA+GAR
−6.545 −6.420 −11.1783 −11.051 −12.623 −12.694 2.484 −3.484
FIGARCH(0,d,0) −6.506 −6.375 −11.1750 −11.044 −12.624 −12.695 2.517 −3.472
FIGARCH(0,d,1) −6.541 −6.417 −11.1780 −11.043 −12.623 −12.694 2.518 −3.471
FIGARCH(1,d,1) −6.553 −6.429 −11.1781 −11.041 −12.622 −12.693 2.510 −3.657
IGARCH(1,1)
−6.544 −6.417 −11.1730 −11.043 −12.624 −12.695 2.507 −3.472
Table 10.17 BIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum BIC is selected.
Symbol
DJIA SP500
BAC
JPM
IBM
WMT
EQ2
EXP
GARCH(1,1)
−6.534 −6.407 −11.162 −11.031 −12.612 −12.682 2.517 −3.461
ARFIMA+GAR
−6.533 −6.407 −11.163 −11.038 −12.619 −12.701 2.495 −3.644
FIGARCH(0,d,0) −6.499 −6.367 −11.169 −11.035 −12.615 −12.686 2.523 −3.463
FIGARCH(0,d,1) −6.531 −6.406 −11.167 −11.032 −12.612 −12.682 2.526 −3.459
FIGARCH(1,d,1) −6.541 −6.416 −11.165 −11.032 −12.609 −12.679 2.521 −3.469
IGARCH(1,1)
−6.537 −6.409 −11.164 −11.034 −12.614 −12.685 2.514 −3.464
Table 10.18 Long memory volatility models and their estimated long memory
parameter for the indices, HFD, earthquake series, and the explosive series.
Symbol
DJIA
SP500
BAC
JPM
IBM WMT
EQ2
EXP
ARFIMA+GAR
−0.032* −0.045** 0.10** .236** .107** 0.152* 0.092** 0.861**
FIGARCH(0,d,0) 0.215** 0.214**
0.00
1.24
0.158 0.00
0.097
0.390**
FIGARCH(0,d,1) 0.307** 0.313**
0.00
−0.12 0.246 0.00
0.115
0.390**
FIGARCH(1,d,1) 0.602** 0.634**
0.00
0.00
0.443 0.00
0.331
0.782**
*significant at 5%; **significant at 1%.
10.5.13 ANALYSIS ON HIGH-FREQUENCY, EARTHQUAKE, AND
EXPLOSIVES SERIES
Similar volatility analysis was done on 
 index, Bank of America Corp (BAC),
JPM, International Business Machines Corp (IBM), and Wal-Mart Stores (WMT),
high-frequency data (HFD), earthquake series (EQ2), and EXP series. The results
are shown in Tables 10.12 to 10.18 and Figures 10.11 to 10.22.
The studied HFD corresponds to the collapse of the Bear Stearns in March 2008.
The data used consists of the week (five trading days) March 10–14, 2008, before the
merging announcement over the weekend as well as the two following trading days
March 17 and 18. On Friday, March 14, 2008, at about 9:14 a.m., JP Morgan Chase
\n\n=== OCR PAGE 364 ===\nTable 10.16 AIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum AIC is selected.

Symbol DJIA SP500 BAC JPM IBM WMT EQ2 EXP
GARCHG,1) -6.544 -6.417 11.1720 -11.042 -12.632 -12.713 2.508 -3.473
ARFIMA+GAR > _-6.545 -6.420 -11.1783 -11.051 -12.623 -12.694 2.484 -3.484
FIGARCH(0,d,0) —6.506 -—6.375 11.1750 -11.044 -12.624 -12.695 2.517 -3.472
FIGARCH(0,d,1) -6.541 -6.417 -11.1780 -11.043 -12.623 -12.694 2.518 -3.471
FIGARCH(1,d,1) -6.553 -6.429 11.1781 -11.041 -12.622 -12.693 2.510 —3.657
IGARCH(,1) -6.544 -6.417 -11.1730 -11.043 -12.624 -12.695 2.507 -3.472
Table 10.17 BIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum BIC is selected.

Symbol DJIA SP500 BAC JPM IBM WMT EQ2 EXP
GARCH(G,1) -6.534 -6.407 11.162 -11.031 -12.612 -12.682 2.517 -3.461
ARFIMA+GAR__-6.533 —6.407 -11.163 -11.038 -12.619 —12.701 2.495 -3.644
FIGARCH(0,d,0) -6.499 -6.367 -11.169 -11.035 -12.615 —12.686 2.523 -3.463
FIGARCH(0,d,1) -6.531 -6.406 -11.167 -11.032 -12.612 -—12.682 2.526 -3.459
FIGARCH(1,d,1) -6.541 -6.416 -11.165 -11.032 -12.609 -12.679 2.521 -3.469
IGARCHG,1) -6.537 —6.409 -11.164 -11.034 -12.614 -12.685 2.514 -3.464
Table 10.18 Long memory volatility models and their estimated long memory
parameter for the indices, HFD, earthquake series, and the explosive series.

Symbol DJIA SP500 BAC JPM IBM WMT EQ2 EXP
ARFIMA+GAR = _-0.032* —0.045** 0.10** .236** .107** 0.152* 0.092** 0.861**
FIGARCH(0,d,0) 0.215** 0.214** 0.00 1.24 0.158 0.00 0.097  0.390**
FIGARCH(0,d,1) 0.307** 0.313** 0.00 -0.12 0.246 0.00 0.115 0.390**
FIGARCH(1,d,1) 0.602** 0.634** 0.00 0.00 0.443 0.00 0.331  0.782**

“significant at 5%; *significant at 1%.

10.5.13 ANALYSIS ON HIGH-FREQUENCY, EARTHQUAKE, AND
EXPLOSIVES SERIES

Similar volatility analysis was done on S&P500 index, Bank of America Corp (BAC),
JPM, International Business Machines Corp (IBM), and Wal-Mart Stores (WMT),
high-frequency data (HFD), earthquake series (EQ2), and EXP series. The results
are shown in Tables 10.12 to 10.18 and Figures 10.11 to 10.22.

The studied HFD corresponds to the collapse of the Bear Stearns in March 2008.
The data used consists of the week (five trading days) March 10-14, 2008, before the
merging announcement over the weekend as well as the two following trading days
March 17 and 18. On Friday, March 14, 2008, at about 9:14 a.m., JP Morgan Chase
\n\n=== PAGE 365 ===\n(JPM) and the Federal Reserves Bank of New York announced an emergency loan to
Bear Stearns (of about 29 billion, terms undisclosed) to prevent the firm from
becoming insolvent. This bailout was declared to prevent the very likely crash of the
market as a result of the fall of one of the biggest investment banks at the time. This
measure proved to be insufficient to keep the firm alive and 2 days later, on Sunday
March 16, 2008, Bear Stearns signed a merger agreement with JPM essentially
selling the company for $2 a share (price revised on March 24 to $10/share). The
same stock traded at $172 in January 2007 and $93 a share in February 2007.
Today, this collapse is viewed as the first sign of the risk management meltdown of
investment bank industry in September 2008 and the subsequent global financial
crisis and recession.
The behavior of stocks representing the financial institutions that were affected by
the crisis such as JPM and BAC was studied. We also looked at other institutions,
such as IBM and WMT, that should not have been much affected. All the stocks were
sampled with the period T = 1 min. During the days considered, the BAC and JPM
return fluctuated more than 10% while that of WMT and IBM less than 1%.
The data on earthquakes magnitude was obtained from the US Geological Survey
(USGS) from January 1, 1973, to November 9, 2010. The downloaded data contains
information about the date, longitude, latitude, and the magnitude of each recorded
earthquake. The location of the major earthquake and its distribution defines the
studied area. The earthquake magnitude is the data used in the analysis. The sample
data is from July 24, 2010, to November 9, 2010, containing 3000 observations with
a maximum (7.1), which occurred at August 12, 2011.
We then continue our analyses with the study of seismic traces of a mining
explosion. A seismic trace is a plot of the earth’s motion over time. The data
presented are measurements of the earth’s vertical displacement where the
recording frequency is 40 per second. The data sets are from a recording station in
Scandinavia and are reported by Shumway and Stoffer [NaN].
10.5.13.1 ADF test
10.5.13.2 KPSS test
10.5.13.3 ARCH test
10.5.13.4 Whittle estimate of d
10.5.13.5 AIC
10.5.13.6 BIC
10.5.13.7 d Estimate
\n\n=== PAGE 366 ===\n10.6 Discussion and conclusion
The underlying volatility processes in earthquake series, high-frequency financial
data, financial indices, and explosive data were explored using various GARCH
models in this thesis. The GARCH models applied include basic GARCH, IGARCH,
ARFIMA (0,d,0)-GARCH, and FIGARCH specifications. The methodology is not
new; however, the major contribution of this work comes in the realm of
applications. The methodology was applied to three domains: geophysics
(earthquake data), finance (high-frequency financial data and indices), and
explosives data. In all the applications, the methodology provides insight into
features of the series volatility.
The results show that the indices (DJIA and S&P 500) returns and the explosives
(EXP) series volatility had the highest persistence that were best described by using
FIGARCH a long memory model. This result is in line with the previous works of
Breidt et al. [1], Mariani et al. [35], and Mike So [36] for the indices volatility. In his
work, Mike So [36] applied the modified rescaled range test (R/S) proposed by Lo
[37] and the semiparametric test (GPH) proposed by Geweke and Porker-Hudak
[38] to detect the existence of long-term dependence in volatility in the S & P 500
index and Dow Jones Industrial Average index. He used three proxies of the
variability of returns to achieve this result: the absolute mean deviation, the squared
mean deviation, and the logarithm of the absolute mean deviation. Mariani et al.
[35], in their paper, used the Hurst exponent and the Detrended fluctuation analysis
methodology to show the existence of long-memory effects in the international
Market indices, that is, Morgan Stanley Capital International Europe, Australasia,
and Far East index and the Emerging market index, and compare their result with
S& P 500. They also found that immediately before a crisis the estimate of the long
memory increases, while during the time of the crisis the stocks behave randomly.
Finally Breidt et al. [1] also used their proposed long-memory stochastic volatility
model to indicate the existence of long memory in financial indices. Hence, our
results of long-memory in the indices volatility reinforced previous results by
different methodology.
The ARFIMA(0,d,0)-GARCH specification was preferred for BAC and JPM high-
frequency series whose volatility was found to be intermediate. The intermediate
memory found in the high-frequency data is consistent with previous results of
Barany et al. [39] and Mariani et al. [40] except with the WMT and IBM high-
frequency data volatility that were best described using the GARCH model, which
has short memory. Mariani et al. [40] employed the relationship between the Hurst
parameter (H) and the Detrended fluctuation analysis parameter (α) and compared
with the value 0.5 to investigate the memory behavior of BAC and JPM high-
frequency data from March 10–18, 2008 (Bear Stearns financial crisis). Their results
gave Hurst estimate of 0.63 and 0.62 for BAC and JPM, respectively, which implies
long memory. Our values for the fractional difference parameter estimate 0.1 and
0.236 for the BAC and JPM, respectively, also showed long memory.
The earthquake series was divided into two regions: symmetrically (EQ1) and
nonsymmetriclly (EQ2) distributed. Both regions showed intermediate memory. Our
analysis indicated that the earthquake series showed long memory while the
\n\n=== PAGE 367 ===\nexplosive series showed short memory. On the other hand, both the explosive and
the earthquake series volatility showed long memory, but the persistence in the
explosive volatility was higher than that of the earthquake. The order of persistence
(memory) in series volatility from highest to lowest is:
1. Indices (DJIA and S&P 500) and explosives data
2. Earthquake data
3. BAC and JPM high-frequency financial data
4. WMT and IBM high-frequency financial data
The predictions made from the MA(1) − FIGARCH(1, d, 1) model for the DJIA index
offered good results since all the actual observations were within the prediction
limits. The earthquake series predictions from our assumed model were fairly
accurate since we had 8 out of 10 earthquake directions correctly predicted.
The outliers observed from the generalized error Q–Q plot for the indices
correspond to significant drops in prices in a short time period (1 or 2 days). This is
what happened on October 15, 2008, for both DJIA and S&P 500 indices.
References
1. F. J. Breidt, N. Crato, and P. De Lima (1998). The detection and estimation of
long memory in stochastic volatility. Journal of Econometrics, 83, 325–348.
2. P. M. Robinson (1991). Testing for strong serial correlation and dynamic
conditional heteroskedasticity in multiple regression. Journal of Econometrics,
47, 67–84.
3. N. Shephard (1996). Statistical aspects of ARCH and stochastic volatility. In D.
R. Cox, D. B. Hinkley, and O. E. Barndorff-Nielsen, editors. Time Series Models:
In Econometrics, Finance and Other Fields. Chapman Hall, London.
4. I. N. Lobato and N. E. Savin (1998). Real and spurious long-memory properties
of stock-market data. Journal of Business & Economic Statistics, 16, 261–283.
5. R. T. Baillie (1996). Long memory processes and fractional integration in
econometrics. Journal of Econometrics, 73, 5–59.
6. O. E. Barndorff-Nielsen and N. Shephard (2001). Modelling by Levy processes
for financial econometrics. In Levy Processes. Birkhauser, Boston, MA, pp. 283–
318.
7. G. E. P. Box, G. M. Jenkins, and G. C. Reinsel (1994). Time series analysis:
forecasting and control. Prentice Hall, 1994.
8. R. F. Engle (1982). Autoregressive conditional heteroscedasticity with estimates
of variance of United Kingdom inflation. Econometrica, 50(4), 987–1007.
9. T. Bollerslev (1986). Generalized autoregressive conditional heteroskedasticity.
Journal of Econometrics, 31, 307–327.
10. R. F. Engle and V.K. Ng (1991). Measuring and testing the impact of news on
volatility. Journal of Finance, 48(5), 1749–1778.
11. D. B. Nelson (1991). Conditional heteroskedasticity in asset returns: a new
approach. Econometrica, 59, 347–370.
\n\n=== PAGE 368 ===\n12. A. C. Harvey, E. Ruiz, and N. Shephard (1994). Multivariate stochastic variance
models. Review of Economic Studies, 61, 247–265.
13. R. T. Baillie, T. Bollerslev, and H. O. Mikkelsen (1996). Fractionally integrated
generalized autoregressive conditional heteroskedasticity. Journal of
Econometrics, 74, 3–30.
14. T. Boolerslev and H. Mikkelsen (1996). Modeling and pricing long-memory in
stock market volatility. Journal of Econometrics, 73, 151–184.
15. P. M. Robinson and M. Henry (1999). Long and short memory conditional
heteroskedasticity in estimating the memory parameter of levels. Econometric
Theory, 15, 299–336.
16. M. Henry (2001). Averaged periodogram spectral estimation with long-
memory conditional heteroscedasticity. Journal of Time Series Analysis, 22(43),
1459.
17. K. Aas and I. H. Haff (2006). The generalized hyperbolic skew student's t-
distribution. Journal of Financial Econometrics, 4(2), 275309.
18. R. A. Rigby and D. M. Stasinopoulos (2005). Generalized additive models for
location, scale and shape. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 54(3), 507554.
19. R. Engle and T. Boolerslev (1996). Modelling the persistence of conditional
variances. Econometric Reviews, 5, 1–50.
20. D. Ruppert (2011). Statistics and data analysis for financial engineering.
Springer Science-Business Media.
21. W. Palma (2007). Long-Memory Time Series (Theory and Methods). Wiley
Series in Probability and Statistics, John Wiley & Sons, Inc., New Jersey, 2007.
22. R. S. Tsay (2005). Analysis of financial time series. Vol. 543. John Wiley & Sons,
Inc.
23. T. Bollerslev (1986). Generalized autoregressive conditional heteroskedasticity.
Journal of Econometrics, 31, 307–327.
24. R. F. Engle and T. Bollerslev (1986). Modelling the persistence of conditional
variances. Econometric Reviews, 5(1), 1–50.
25. D. B. Nelson (1990). Stationarity and persistence in the GARCH (l, l) model.
Econometric theory. Cambridge University Press.
26. C. Granger and R. Joyeux (1980). An introduction to long memory time series
models and fractional differencing. Journal of Time Series Analysis, 1, 15–30.
27. J. Hosking (1981). Fractional differencing. Biometrika, 68, 165–176.
28. W. Palma (2007). Long-memory time series: theory and methods. Vol. 662.
John Wiley & Sons, Inc.
29. D. A. Dickey and W. A. Fuller (1979). Distribution of the estimators for
autoregressive time series with a unit root. Journal of the American Statistical
Association, 74(366a), 427–431.
30. N. H. Chan and C. Z. Wei (1988). Limiting distributions of least squares
estimates of unstable autoregressive processes. Annals of Statistics, 16, 367–401.
31. P. C. B. Phillips (1987). Time series regression with unit roots. Econometrica, 55,
2777301.
32. W. A. Fuller (1976). Introduction to Statistical Time Series. John Wiley & Sons,
Inc., New York.
\n\n=== PAGE 369 ===\n33. D. Kwiatkowski, P. Phillips, P. Schmidt, and Y. Shin (1992). Testing the null
hypothesis of stationarity against the alternative of a unit root: How sure we
are that economic time series have a unit root? Journal of Econometrics, 54,
159–178.
34. R. H. Shumway and D. S. Stoffer (2010). Time series analysis and its
applications: with R examples. Springer Science & Business Media.
35. M. Mariani, I. Florescu, M. P. B. Varela, and E. Ncheuguim (2010). Study of
memory effect in international market indices. Physica A, 2010, 389(8), 1653–
1646.
36. M.P. So (2010). Long-term memory in stock market volatility. Applied Financial
Economics, 10(5), 519–524.
37. A. W. Lo (1991). Long-term memory in stock market prices. Econometrica, 59,
1279–1313.
38. J. Geweke and S. Porter-Hudak (1983). The estimation and application of long
memory time series models. Journal of Time Series Analysis, 4, 221–238.
39. E. Barany and M. P. B. Varela (2011). Stochastic Differential Equations and Levy
Models with Applications to High Frequency Data. Handbook of Modeling
High-Frequency Data in Finance, pp. 327–346.
40. M. Mariani, I. Florescu, M. P. B. Varela, and E. Ncheuguim (2009). Long
correlations and Levy models applied to the study of memory effects in high
frequency (tick) data. Physica A, 2009, 388(8), 1659–1664.
\n\n=== PAGE 370 ===\nChapter Eleven
Scale Invariance and Lévy Models Applied to
Earthquakes and Financial High-Frequency
Data
M. P. Beccar-Varela1, Ionut Florescu2, and I. SenGupta3
1Department of Mathematical Sciences, University of Texas at El
Paso, El Paso, TX, USA
2Financial Engineering Division, Stevens Institute of
Technology, Hoboken, NJ, USA
3Department of Mathematics, North Dakota State University,
Fargo, ND, USA
11.1 Introduction
In recent years, we have observed renewed interest in describing
critical phenomena ([1]) using several modeling techniques. Ising
models have been used in [1–3], phase transition [4], fitting the data
with exponential sequence [5] or using the so-called scale invariance
property [6, 7]. In the current work, we generalize the scale
invariance approach, and we further use a technique based on
truncated Lévy models to estimate the first instance when a critical
event may start to be announced.
In Section 11.3, we present a deterministic model and the governing
equations based on general scale invariant functions. We apply this
deterministic model to study geophysical data characteristic for
earthquakes. In Section 11.6, we present a stochastic standardized
Lévy flight model and we apply this stochastic model to the same
geophysical data. In Section 11.9, we apply the scale invariance
model to financial high-frequency data corresponding to the Bear
Stearns collapse in March 2008. In Section 11.12, we present a brief
description of the codes. Finally, in Section 11.13, we summarize the
\n\n=== PAGE 371 ===\n(11.1)
(11.2)
results obtained when applying the two methodologies to both
earthquake and financial data.
11.2 Governing equations for the
deterministic model
Definition 11.1 We say that a function A is scale invariant with a
scale parameter λ if and only if for every t ∈ Domain(A):
where μ is a constant dependent only on the scale factor λ.
As a trivial example of such function, A(t) = t is scale invariant for
any 
 with the constant μ = λ.
In our previous study [3], using geophysical data, we observed that
smaller earthquakes preceding a major event have a possible log-
periodic structure. In the cited work [3], we approximate the periods
between earthquakes up to the major earthquake, using an equation
obtained from a temporal energy evolution in a diamond Ising
model:
In this chapter, we extend this work and use the deterministic scale
invariance property (Definition 11.1) to estimate the time of the
major event.
We define the following function A:
It is not hard to see that the function defined in (11.2) is scale
invariant under the transformation
\n\n=== OCR PAGE 371 ===\nresults obtained when applying the two methodologies to both
earthquake and financial data.

11.2 Governing equations for the
deterministic model

Definition 11.1 We say that a function A is scale invariant with a
scale parameter A if and only if for every t € Domain(A):

A(At) = HA(t) (1.1),
where u is a constant dependent only on the scale factor A.

As a trivial example of such function, A(f) = tis scale invariant for
any A € R with the constant pt =A.

In our previous study [3], using geophysical data, we observed that
smaller earthquakes preceding a major event have a possible log-
periodic structure. In the cited work [3], we approximate the periods
between earthquakes up to the major earthquake, using an equation
obtained from a temporal energy evolution in a diamond Ising
model:

In Ty = EN + In(ar)

In this chapter, we extend this work and use the deterministic scale
invariance property (Definition 11.1) to estimate the time of the
major event.

We define the following function A:

1 whenlInt=C\k+C,,kEZ (11.2),
A(t) = . “
0 otherwise

It is not hard to see that the function defined in (11.2) is scale
invariant under the transformation
\n\n=== PAGE 372 ===\n(11.3)
(11.4)
for 
. Using the notations in (11.1), we see that μ = 1 and
.
We next introduce the following function:
where tc is the critical time of the major event, which we do not know
at this point.
Note that B(t) alternates between 0 and 1, and we define the set S:
The critical time tc is defined as the limit of the accumulation points
of the set; that is, for C1 ≠ 0, the set S has an accumulation point at tc.
A particular type of scale invariance is the one arising in the
existence of intermittences or stationary intervals, constant in the
logarithm of the independent variable [6, 7].
where F is the floor function and C is the ceiling function. From the
form of equations (11.4), it is possible to see that (11.4) is scale
invariant under the transformation:
since
We can see that constant μ = eαn for λ = an. Let us introduce new
one-parametric scale invariant function that generalizes (11.4):
\n\n=== OCR PAGE 372 ===\nt>(e%)"tnEeZ

forn € Z. Using the notations in (11.1), we see that p = 1 and
A=(C)'* n EZ,

We next introduce the following function:

B(t) = A(t, — t), (11.3),

where ¢, is the critical time of the major event, which we do not know
at this point.

Note that B(f) alternates between 0 and 1, and we define the set S:

S= (1B) = 1},

The critical time t, is defined as the limit of the accumulation points
of the set; that is, for C, # 0, the set S has an accumulation point at t,.

A particular type of scale invariance is the one arising in the
existence of intermittences or stationary intervals, constant in the
logarithm of the independent variable [6, 7].

FP (t) = pe a > 0 (11.4),
F(t) = BetClosat)

where F is the floor function and C is the ceiling function. From the
form of equations (11.4), it is possible to see that (11.4) is scale
invariant under the transformation:

toa'tnEeZ

since

fF(a"t) = Beth at) — PetF (log, ttn)
= Bet Flog. t)+n) _ BetFlloeat - em

We can see that constant = e“” for A = a”. Let us introduce new
one-parametric scale invariant function that generalizes (11.4):
\n\n=== PAGE 373 ===\n(11.5)
It is easy to see that
11.2.1 APPLICATION TO GEOPHYSICAL (EARTHQUAKE
DATA)
The geological data was obtained from U.S. Geological Survey
(USGS) from January 1, 1973, to November 9, 2010. Downloaded
data contains information about the date, longitude, latitude, and
magnitude of each recorded earthquake in the region.
The location of the major earthquake chosen defines the area
studied. The area chosen cannot be too small (lack of data) or too big
(noise from unrelated events). The data is obtained using a square
centered at the coordinates of the major event. The sides of the
square were usually chosen as ± 0.1°–0.2° in latitude and ± 0.2°–
0.4° in longitude. A segment 0.1° of latitude at the equator is ≈ 11.11
km ≈ 6.9 miles in length.
The earthquake magnitude is the recorded data used in the analysis.
The policy of the USGS regarding recorded magnitude is the
following [8]:
Magnitude is a dimensionless number between 1 and 12.
The reported magnitude should be moment magnitude, if
available.
The least complicated, and probably most accurate, terminology
is to just use the term magnitude and to use the symbol M.
The coefficients C1, C2 and time tc for (11.2) are obtained by least
square fitting of data. We illustrate the fit in Figures 11.2, 11.3, 11.4,
11.5, 11.6, 11.7, and 11.8. As we may observe from equation (11.2) and
also Table 11.1, the set of coefficients C1, C2 is not unique. For
example, if C1, C2 are valid coefficients, then C1, C2 + C1 or C1, C2 − C1
\n\n=== OCR PAGE 373 ===\nFlt) = pet Flrest—) x ER (5)

It is easy to see that

f(t) =f"
A) =f

11.2.1 APPLICATION TO GEOPHYSICAL (EARTHQUAKE
DATA)

The geological data was obtained from U.S. Geological Survey
(USGS) from January 1, 1973, to November 9, 2010. Downloaded
data contains information about the date, longitude, latitude, and
magnitude of each recorded earthquake in the region.

The location of the major earthquake chosen defines the area
studied. The area chosen cannot be too small (lack of data) or too big
(noise from unrelated events). The data is obtained using a square
centered at the coordinates of the major event. The sides of the
square were usually chosen as + 0.1°—0.2° in latitude and + 0.2°-
0.4° in longitude. A segment 0.1° of latitude at the equator is ~ 11.11
km 6.9 miles in length.

The earthquake magnitude is the recorded data used in the analysis.
The policy of the USGS regarding recorded magnitude is the
following [8]:

¢ Magnitude is a dimensionless number between 1 and 12.

¢ The reported magnitude should be moment magnitude, if
available.

¢ The least complicated, and probably most accurate, terminology
is to just use the term magnitude and to use the symbol M.

The coefficients C,, C, and time t, for (11.2) are obtained by least

square fitting of data. We illustrate the fit in Figures 11.2, 11.3, 11.4,
11.5, 11.6, 11.7, and 11.8. As we may observe from equation (11.2) and
also Table 11.1, the set of coefficients C,, C, is not unique. For

example, if C,, C, are valid coefficients, then C,, C, + C, or C,, C. -— C,
\n\n=== PAGE 374 ===\n(11.6)
is also a valid set of coefficients. To make the selection unique, we
select an arbitrary peak as a peak with k = 1, and this standardizes
the data and makes the choice of the coefficients C1 and C2 unique.
Table 11.1 Least square fitting with (11.2).
Graph tc
Earthquake date C1
11.2
674
677
2.3008
11.3
7,019
7,054
0.7663
11.4
8,201
8,245
0.9858
11.5
8,298
8,291
0.9246
11.6
10,896 10,898
2.7636
11.7
11,519 11,518
1.1537
11.8
12,093 12,054
0.5306
11.2.2 RESULTS
The least square fitting is performed in the following way. Let
 denote the times of earthquakes preceding some
major even. The times are reordered in decreasing order such that ti
> ti + 1. This is done since tc − t is a decreasing sequence. The least
square fitting is performed by minimizing the following error
function:
where vector  was held fixed and was corresponding to data
obtained from USGS. The ith period will be
\n\n=== OCR PAGE 374 ===\nis also a valid set of coefficients. To make the selection unique, we
select an arbitrary peak as a peak with k = 1, and this standardizes
the data and makes the choice of the coefficients C, and C, unique.

Table 11.1 Least square fitting with (11.2).
Graph t, Earthquake date C,

11.2 674 677 2.3008
11.3 7,019 7,054 0.7663
11.4 8,201 8,245 0.9858
11.5 8,298 8,291 0.9246
11.6 10,896 10,898 2.7636
11.7 11,519 11,518 1.1537
11.8 12,093 12,054 0.5306

11.2.2 RESULTS

The least square fitting is performed in the following way. Let

= (t), ---+t+1) denote the times of earthquakes preceding some
major even. The times are reordered in decreasing order such that t;
> t; 44. This is done since t, — tis a decreasing sequence. The least
square fitting is performed by minimizing the following error
function:

7 (41.6)
k
&(C.C.) =) (t; — try) elitr _ pCUDEG,

i=||\—“~—2
T, T,

where vector ¢ was held fixed and was corresponding to data
obtained from USGS. The i" period will be
\n\n=== PAGE 375 ===\n(11.7)
(11.8)
These time periods form an exponential sequence if the estimated C1
is positive.
Then the accumulation point, the critical time tc, will simply be:
where ti’s are the times corresponding to the real events right before
the major one. Figure 11.1 next is illustrating the calculation, which is
also illustrated in Figure 11.3.
FIGURE 11.1 Times t1, …, tk need to correspond to the real
earthquake events.
In the following figures (Figures 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, and
11.8), the first image plots the magnitude of the recorded
earthquakes in time, the second image plots B(t) (as calculated using
equation (11.3)) and finally the 3D plot is displaying the error
function (as given in (11.6)).
\n\n=== OCR PAGE 375 ===\nT; =t— ty) = eliitC — eit tC, _ ie] —e%|, (11.7),

These time periods form an exponential sequence if the estimated C,
is positive.

Then the accumulation point, the critical time t,, will simply be:

k
to = thy + YF

je0

(11.8)

where t;'s are the times corresponding to the real events right before

the major one. Figure 11.1 next is illustrating the calculation, which is
also illustrated in Figure 11.3.

12
tg A) tO tot

12 - To TH}
08 T-$-2}

Bit)

0.6

0.4

0.2

oe 1 1 L L 1 i |
6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100
Days from 1973-01-11 [day]

FIGURE 11.1 Times t,, ..., t; need to correspond to the real
earthquake events.

In the following figures (Figures 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, and
11.8), the first image plots the magnitude of the recorded
earthquakes in time, the second image plots B(t) (as calculated using
equation (11.3)) and finally the 3D plot is displaying the error
function (as given in (11.6)).
\n\n=== PAGE 376 ===\nFIGURE 11.2 Earthquake data and fitting with function (11.2).
Latitude: − 12.5° ± 0.17°
Longitude: − 77.78° ± 0.34°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 376 ===\n>

Magnitude [-]

350 400 450 500 0 600 680 700
Days trom 1973-01-11 (day)

os

a0

os
02
350 400 450 500 $50 600 650 700

Days trom 1973-01-11 [day]

Plot of the error function

Error

116

237 14
26
ami =f? e

FIGURE 11.2 Earthquake data and fitting with function (11.2).

e Latitude: - 12.5° + 0.17°
¢ Longitude: - 77.78° + 0.34°
¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 377 ===\nFIGURE 11.3 Earthquake data and fitting with function (11.2).
Latitude: 40.37° ± 0.1°
Longitude: − 124.32° ± 0.2°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 377 ===\n° I
6100 6200 ©6300 6400 6500 6600 6700 6800 6900 7000 7100
Days trom 1973-01-11 (day)

S os
oa
02
$10 €200 6900 «400 6500 6800 6700 6800 6900 7000 7100
Days trom 1973-01-11 [day]
Prt the err hncton
2500
2000
= 00
2000
pmo 1200
"009.
a 100
Te mes cannes eases SE 500
oO a agoe7oee®
, ga
FIGURE 11.3 Earthquake data and fitting with function (11.2).

e Latitude: 40.37° + 0.1°
¢ Longitude: - 124.32° + 0.2°
¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 378 ===\nFIGURE 11.4 Earthquake data and fitting with function (11.2).
Latitude: − 23.34° ± 0.2°
Longitude: − 70.30° ± 0.4°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 378 ===\n°
€600 6800-7000 7200 400 «7800 «7800-8000 8200 «8400
Days trom 1973-01-11 [day]

°
6000 6800 7000 7200 «7100 ©7800 «78008000200 «400
Days trom 1973-08-11 [day]

Plot of the error tuncton

a
>
2 =. ®
: ;
Be 5
* 5
2 4
1B 3
me 102 oso82

1 405047048

om cgi ;

FIGURE 11.4 Earthquake data and fitting with function (11.2).
e Latitude: — 23.34° + 0.2°
¢ Longitude: - 70.30° + 0.4°

¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 379 ===\nFIGURE 11.5 Earthquake data and fitting with function (11.2).
Latitude: 16.78° ± 0.2°
Longitude: − 98.6° ± 0.4°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 379 ===\n8
7
6
5
4
3
2
1
ry
7850

7900 7950 6000 6050 8100 8150 6200 6250 8900 6350
Days from 1973-01-11 [day]

$eso 700” 78808000 Boo BIO BIS e260 280 ——aD00

380
Days trom 1973-01-11 day]
Prot of the enor function
4000
3500
4800
4000 3000
3500
4000 2500
B 2800
5 2000 ‘2000
1800
1000 1500
500
ost 1000

0.95 -
om
093

09a

2.
“ oat 0.43!
09 4 041042

0.49 Bisco
0.47048)
0.45048
“4

FIGURE 11.5 Earthquake data and fitting with function (11.2).
e Latitude: 16.78° + 0.2°
¢ Longitude: - 98.6° + 0.4°

¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 380 ===\nFIGURE 11.6 Earthquake data and fitting with function (11.2).
Latitude: 63.52° ± 0.17°
Longitude: − 147.44° ± 0.34°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 380 ===\ni 5
i 1
a TT.)
Days trom 1973-01-11 [day] x10"
12
1
os
206
oa
oz
°
a a a i eT eT)
Days from 1973-01-11 (day) x10
Pot of the error function
300
350 250
300
250 200
B 20
© 150 Ee , 180
o Gi.
bed wo
°
s 28 Lad *
ane 2% 195 *
ad 272 w
27 185 %

FIGURE 11.6 Earthquake data and fitting with function (11.2).

¢ Latitude: 63.52° + 0.17°
¢ Longitude: - 147.44° + 0.34°
¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 381 ===\nFIGURE 11.7 Earthquake data and fitting with function (11.2).
Latitude: − 17.66° ± 0.03°
Longitude: − 178.76° ± 0.06°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 381 ===\n°

Ir 2 2
§ *
i,
:
;
‘
Saree wisoest fat sa
ia
}
0
ge
os
as
'
Days trom 1973-01-11 (day) Lal
fete oc tc
~ ne
=
= 1
g
Bi.
* to
r
@
us 0% O*
« 12 om 0™

FIGURE 11.7 Earthquake data and fitting with function (11.2).

¢ Latitude: — 17.66° + 0.03°
¢ Longitude: - 178.76° + 0.06°

¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 382 ===\nFIGURE 11.8 Earthquake data and fitting with function (11.2).
Latitude: − 19.93° ± 0.05°
Longitude: − 178.18° ± 0.1°
Time on the x axis is measured in days from 1973/01/11.
\n\n=== OCR PAGE 382 ===\n$ 9 Q ° !
+
Ey
2
1
9
Pas 16 7 8 19 12 120
Days trom 1973-01-19 {day} x10"
12
1
os
&
os
os
02
315 16 a7 ve 110 12 121
Days trom 1973-01-11 fday] x10"
Pot ofthe error function
000
7000
9000
‘8000
7000 ‘aad
6000
poo 000
5 4000
3000 = ene
2000
1000 . bad
os 2000
0% O55 -- 0.49)
beat) 046047948 Bh 1000
ose 943044045
ost 42

FIGURE 11.8 Earthquake data and fitting with function (11.2).

e Latitude: — 19.93° + 0.05°
¢ Longitude: - 178.18° + 0.1°

¢ Time on the x axis is measured in days from 1973/01/11.
\n\n=== PAGE 383 ===\nFIGURE 11.9 Earthquake data fitting by TLF, corresponding to
11.3.
FIGURE 11.10 Earthquake data fitting by TLF, corresponding to
11.4.
\n\n=== OCR PAGE 383 ===\n10' i T

FOP

10"

107;

10° |

Cumulative distribution

10“ |

410° 1 1

10" 10° 10'
GS (a = 1.70)

FIGURE 11.9 Earthquake data fitting by TLF, corresponding to
11.3.

10! T r

10°

10"

10?

10°

Cumulative distribution

104.

10°

L 1
107 10° 10'
GS (a = 1.80)

FIGURE 11.10 Earthquake data fitting by TLF, corresponding to
11.4.
\n\n=== PAGE 384 ===\nFIGURE 11.11 Earthquake data fitting by TLF, corresponding to
11.5.
FIGURE 11.12 Earthquake data fitting by TLF, corresponding to
11.6.
\n\n=== OCR PAGE 384 ===\n10' T T

40° L

10"

10°

10°

Cumulative distribution

10“ |

10° 1 1

10" 10° 10'
GS (a = 1.80)

FIGURE 11.11 Earthquake data fitting by TLF, corresponding to
11.5.

10! T Tr

10°

10"

10°

10°

Cumulative distribution

104

10°

1 1
10" 10° 10'
GS (0 = 1.70)

FIGURE 11.12 Earthquake data fitting by TLF, corresponding to
11.6.
\n\n=== PAGE 385 ===\nFIGURE 11.13 Earthquake data fitting by TLF, corresponding to
11.7.
FIGURE 11.14 Earthquake data fitting by TLF, corresponding to
11.8.
\n\n=== OCR PAGE 385 ===\n402 +

10° |

Cumulative distribution

10“ |

10°

fi 1
10" 10° 10'
GS (a = 1.70)

FIGURE 11.13 Earthquake data fitting by TLF, corresponding to
11.7.

10! T Tr

10°

10"

10°

10°

Cumulative distribution

1 1
107 10° 10'
GS (a = 1.85)

FIGURE 11.14 Earthquake data fitting by TLF, corresponding to
11.8.
\n\n=== PAGE 386 ===\nFIGURE 11.15 JP Morgan Chase—The solid line represents the best
fit with (11.5).
\n\n=== OCR PAGE 386 ===\nJPMorgan - HFD from March 10 to March 18 2008 (5 trading days)
38 -

37 Ly

Log index

3.55
35 -
° 500 1000 1500 2000 2500 3000
t [min]
Plot of the error function

Error
suSEE8

FIGURE 11.15 JP Morgan Chase—The solid line represents the best
fit with (41.5).
\n\n=== PAGE 387 ===\nFIGURE 11.16 Citi—The solid line represents the best fit with (11.5).
\n\n=== OCR PAGE 387 ===\nCiti - HFD from March 10 to March 18 2008 (5 trading days)
3.25 + + + +

2.95 A be

29 £ wi

2.85

° 500 1000 1500 2000 2500 3000
t {min}
Plot of the error function

500

450

400

pt
5 300
ui 250°

200

150 y

100

ae

oh _ mat

os : no On
3.18 318 5c : oe % 0, May
Log (B) 3.42 i
31 oy Be % ‘%
24 yey % % 0

FIGURE 11.16 Citi—The solid line represents the best fit with (11.5).
\n\n=== PAGE 388 ===\nFIGURE 11.17 IAG—The solid line represents the best fit with
(11.5).
\n\n=== OCR PAGE 388 ===\nLag - HFD from March 10 to March 18 2008 (5 trading days)
22

2.18 |
2.16

217 ;

250 ~

‘ Scere :

oO
2.24

Error

Ss no
og (6) ard <a opg20gg 004 2

2.1 “Q9, 2

a

FIGURE 11.17 [AG—The solid line represents the best fit with
(1.5).
\n\n=== PAGE 389 ===\nFIGURE 11.18 LBC—The solid line represents the best fit with
(11.5).
Table 11.1 presents in a concise form the estimated critical times of
the major event together with the estimated coefficient C1 and the
actual earthquake day.
\n\n=== OCR PAGE 389 ===\nLBC - HFD from March 10 to March 18 2008 (5 trading days)
244--

2.42 ©

24 oi

Log index
n
8

2.36

234

2.32
° 500 1000 1500 2000 2500 3000

t {min}

Plot of the error function

015

“005 a

~O,
log (8) “0.05 21

FIGURE 11.18 LBC—The solid line represents the best fit with
(41.5).

Table 11.1 presents in a concise form the estimated critical times of
the major event together with the estimated coefficient C, and the
actual earthquake day.
\n\n=== PAGE 390 ===\n11.3 Lévy flights and application to
geophysics
In this section, we compute the critical time described in the
previous section, using a different method. Specifically, we consider
the magnitude process as a stochastic process with increments
modeled by a Lévy distribution. This distribution has the scale
invariance property that wascrucial in the deterministic case. We fit
the earthquake data, using a truncated Lévy flight process. We show
that the fitted cumulative distribution curve has some outlining
points. As observed in [9, 10], a market crash may be found as an
outlining point in the cumulative probability distribution of the
stochastic process described by the Lévy model. In the current work,
we use a similar idea to obtain the critical time after which a major
earthquake event may follow.
In the next section, we briefly describe the Truncated Lévy Flight
distribution. Then, we approximate the observed data, using the
appropriate truncated Lévy flight model. We locate the points in the
data set that are outliers to the fitted Lévy flight in the cumulative
probability distribution. If there are more than one such outliers, we
only select the first such point. We then identify from the data set the
time corresponding to this outlier. That is the estimated time (
)
after which the awareness level should be raised significantly that a
major event will follow.
11.3.1 TRUNCATED LéVY FLIGHT DISTRIBUTION
In this section, we briefly describe the theoretical setup for the
computations done in the next section. For a review of stable
distribution and for other definitions and notations, we refer to the
Appendix.
The leptokurtic (heavy tails) property of the stable distributions for α
< 2 is very desirable to model processes where extreme events have a
greater (than normal) chance of appearing. Choosing to use Levy–
Khintchine-type distributions seems like a natural idea; however, the
infinite variance of these distributions for any α < 2 is an issue when
working with real data. In order to avoid this problem, Mantegna [11]
considers a Lévy-type distribution truncated at some parameter l,
\n\n=== OCR PAGE 390 ===\n11.3 Lévy flights and application to
geophysics

In this section, we compute the critical time described in the
previous section, using a different method. Specifically, we consider
the magnitude process as a stochastic process with increments
modeled by a Lévy distribution. This distribution has the scale
invariance property that wascrucial in the deterministic case. We fit
the earthquake data, using a truncated Lévy flight process. We show
that the fitted cumulative distribution curve has some outlining
points. As observed in [9, 10], a market crash may be found as an
outlining point in the cumulative probability distribution of the
stochastic process described by the Lévy model. In the current work,
we use a similar idea to obtain the critical time after which a major
earthquake event may follow.

In the next section, we briefly describe the Truncated Lévy Flight
distribution. Then, we approximate the observed data, using the
appropriate truncated Lévy flight model. We locate the points in the
data set that are outliers to the fitted Lévy flight in the cumulative
probability distribution. If there are more than one such outliers, we
only select the first such point. We then identify from the data set the
time corresponding to this outlier. That is the estimated time (est)
after which the awareness level should be raised significantly that a
major event will follow.

11.3.1 TRUNCATED LéVY FLIGHT DISTRIBUTION

In this section, we briefly describe the theoretical setup for the
computations done in the next section. For a review of stable
distribution and for other definitions and notations, we refer to the
Appendix.

The leptokurtic (heavy tails) property of the stable distributions for a
< 2 is very desirable to model processes where extreme events have a
greater (than normal) chance of appearing. Choosing to use Levy—
Khintchine-type distributions seems like a natural idea; however, the
infinite variance of these distributions for any a < 2 is an issue when
working with real data. In order to avoid this problem, Mantegna [11]
considers a Lévy-type distribution truncated at some parameter I,
\n\n=== PAGE 391 ===\nwhich obviously has finite variance. This distribution was named the
Truncated Lévy flight (henceforth TLF):
where P(x) denotes any symmetric Lévy distribution obtained from
its characteristic function in (11.14) when β = 0, and 1A(x) is the
indicator function of the set A. c is the constant that makes the
definition of probability density. Clearly, as l → ∞, one obtains the
regular Lévy distribution characterized by P( · ) (a stable
distribution). However, the TLF distribution itself is not stable for
any finite truncation level l. Instead, this distribution has finite
variance; thus, independent variables drawn from this distribution
satisfy a regular central limit theorem. If the parameter l is large, the
convergence to the limiting Gaussian may be very slow [11]. If the
parameter l is small (so that the convergence is fast), the cut in its
tails is very abrupt.
In order to have continuous tails, Koponen [12] considered a TLF in
which the cut function is a decreasing exponential characterized by a
separate parameter l. The characteristic function of this distribution,
when α ≠ 1, can be expressed as follows:
with c1 a scale factor:
and
\n\n=== OCR PAGE 391 ===\nwhich obviously has finite variance. This distribution was named the
Truncated Lévy flight (henceforth TLF):

T(x) = cP(x)1(_pp (x)

where P(x) denotes any symmetric Lévy distribution obtained from
its characteristic function in (11.14) when B = 0, and 1,(x) is the
indicator function of the set A. c is the constant that makes the
definition of probability density. Clearly, as 1 > ©, one obtains the
regular Lévy distribution characterized by P(- ) (a stable
distribution). However, the TLF distribution itself is not stable for
any finite truncation level J. Instead, this distribution has finite
variance; thus, independent variables drawn from this distribution
satisfy a regular central limit theorem. If the parameter | is large, the
convergence to the limiting Gaussian may be very slow [11]. If the
parameter / is small (so that the convergence is fast), the cut in its
tails is very abrupt.

In order to have continuous tails, Koponen [12] considered a TLF in
which the cut function is a decreasing exponential characterized by a
separate parameter /. The characteristic function of this distribution,
when a # 1, can be expressed as follows:

(g°+1/P)?

cos(za/2)

pq) exp fc cy cos(a arctan(/|q]))

x (1 + il|g|p tan(qg arctantta)) |
with c, a scale factor:

_ 2xcos(xa/2)
“= al (a) sin(za@)

and

ad 2a a
6 = 1 = A.
cos(za/2) al (a) sin(za)
\n\n=== PAGE 392 ===\nIn the case of symmetric distributions β = 0, and in this case the
variance can be calculated using the characteristic function:
All the following discussion is in this symmetric case (β = 0).
If we use time steps Δt apart, and T = NΔt, following the previous
discussion, at the end of each interval, we must calculate the sum of
N stochastic variables that are independent and identically
distributed. Therefore, the characteristic function of the sum is:
The model can be improved by standardizing it. If the variance is
given by:
we have that
Therefore, a standardized model is:
Next we state a well-known theorem applicable in our case.
\n\n=== OCR PAGE 392 ===\nIn the case of symmetric distributions B = 0, and in this case the
variance can be calculated using the characteristic function:

F 9(Q)
oq?

_ 2An(1 =a) oo

~ T(a@) sin(za@)
q=0

o(th=-

All the following discussion is in this symmetric case (f = 0).

If we use time steps At apart, and T = NAt, following the previous
discussion, at the end of each interval, we must calculate the sum of
N stochastic variables that are independent and identically
distributed. Therefore, the characteristic function of the sum is:

N(@? +1/P)2?

cos(na/2) cos(@ arctan) } .

o(4,N) = gq)" = exp {ov =¢

The model can be improved by standardizing it. If the variance is
given by:

2 2 p(q)
oq" q=0
we have that
_ &e(q/e) _1%e@) _,
oq" | 4-0 a 9g" |,-o

Therefore, a standardized model is:

q (q/oy + 1/Py? lal
In gs(q) = Ing (2) =Cy- © cosa) °° a arctan

> a/2
2rAl-*t ql\~ ql
- 1 +1 cos { @arctan {| — .
al (a) sin(a) o o

Next we state a well-known theorem applicable in our case.

\n\n=== PAGE 393 ===\nTheorem 11.2 Let 
 be the characteristic function (Fourier
transform) of p(x). Let 
, where fa(k) is analytic
and fs(k) ~ −a|k|α as k → 0 with 
 and finite. Then
where
Going back to the TLF distribution, for small q,
Therefore, using the aforementioned theorem 
.
We can also justify this from the viewpoint of the cutoff function
used in [12] given by:
We are considering
Here L(x) is the Lévy distribution.
\n\n=== OCR PAGE 393 ===\nTheorem 11.2 Let P(/) be the characteristic function (Fourier
transform) of p(x). Let log P(K) = f.(K) +£,(0), where f,,(k) is analytic
and f,(k) ~ -a|k|“ as k > 0 with a € Rand finite. Then

P(x) ~ ee’

where

_ asin(az/2)T(a)a
= ~ .
Going back to the TLF distribution, for small q,

22Al“t a {ql > ql
1 (q) ~ 1 1 C ct = .
0g Ys(q) al (a) sin(ra) ( + 5) (£) ) oo («are »(2))|

jw bt
PO) ~ Tr

Therefore, using the aforementioned theorem

We can also justify this from the viewpoint of the cutoff function
used in [12] given by:

A_e Hl yy-ba x<0O
fx) =
Aye HV xf-!-@ x» > 0.
We are considering
F(x) x>l

P(x) =4 cL(x) -l <x <l

F(x) x<-l.

Here L(x) is the Lévy distribution.
\n\n=== PAGE 394 ===\n(11.9)
(11.10)
The decay is faster than the heavy tail decay due to the presence of
the exponential function in f(x). However, the main advantage of
using this form of density p(x) is that we can keep the heavy tail of
the Lévy as far as we want (by adjusting the parameter l), and then
have a sharp decay (faster than heavy tail). In other words, we are
keeping the features of the heavy tail of the Lévy distribution as well
as the finite variance feature given by the exponential decay.
To simulate the standardized truncated Lévy model, a Matlab
module was developed. The parameter l is fixed at 1 and then the
parameter A and the characteristic exponent α are adjusted
simultaneously in order to fit the cumulative function.
11.3.2 RESULTS
We use the model with TLF that we have already discussed in this
chapter. We display the appropriate Lévy fitting in Figures 11.9,
11.10, 11.11, 11.12, and 11.13. The standardized model used is:
The recorded magnitude data is standardized:
where M is magnitude of an earthquake, and E, σ are expected value
and deviance for observed earthquake magnitude M.
In Figures 11.9, 11.10, 11.11, 11.12, and 11.13, we fit the observed
standardized magnitude (11.10) with the cumulative distribution
function corresponding to the characteristic function of the TLF
given by (11.9). Since the empirical distribution is a heavy tailed
distribution, it is natural to expect the standardized Lévy flight
\n\n=== OCR PAGE 394 ===\nThe decay is faster than the heavy tail decay due to the presence of
the exponential function in f(x). However, the main advantage of
using this form of density p(x) is that we can keep the heavy tail of
the Lévy as far as we want (by adjusting the parameter J), and then
have a sharp decay (faster than heavy tail). In other words, we are
keeping the features of the heavy tail of the Lévy distribution as well
as the finite variance feature given by the exponential decay.

To simulate the standardized truncated Lévy model, a Matlab
module was developed. The parameter | is fixed at 1 and then the
parameter A and the characteristic exponent a are adjusted
simultaneously in order to fit the cumulative function.

11.3.2 RESULTS

We use the model with TLF that we have already discussed in this
chapter. We display the appropriate Lévy fitting in Figures 11.9,
11.10, 11.11, 11.12, and 11.13. The standardized model used is:

24 1 /P)y«/2
Ings(q) = Ing ( 4 ) =Co-C} (alo) + W/E" cos (« arctan (“))
o

o cos(za/2)

5 a/2
D) —a 7
= _2rAU St 1- gd +1 cos { warctan a .
al (a) sin(zra@) o o

The recorded magnitude data is standardized:

M — E(M)
o(M)

Gs= (41.10)

where M is magnitude of an earthquake, and E, o are expected value
and deviance for observed earthquake magnitude M.

In Figures 11.9, 11.10, 11.11, 11.12, and 11.13, we fit the observed
standardized magnitude (11.10) with the cumulative distribution
function corresponding to the characteristic function of the TLF
given by (11.9). Since the empirical distribution is a heavy tailed
distribution, it is natural to expect the standardized Lévy flight
\n\n=== PAGE 395 ===\nmodel (red line in the images) to fit better than a regular normal
distribution (green line in the figures). However, recall that the TLF
has exponentially decreasing tails (the variance is finite) unlike the
regular Lévy distribution. Thus, we identify the outlying observations
as the points at which the TLF fails to provide a good fit. The
estimated time of the crash is reported as the date corresponding to
the point where data deviates first from the fitted TLF curve.
We present the estimated crash time in Table 11.2. In the following
figures, the blue dots represent the data points, the red line is the
Lévy CDF, and the green line is the gaussian fit.
Table 11.2 Estimation from the Lévy model.
Graph
test
Earthquake date
11.3
7,064
7,054
11.4
8,249
8,245
11.5
8,399
8,291
11.6
10,777
10,898
11.7
10,438
11,518
11.8
12,249
12,054
11.4 Application to the high-frequency
market data
11.4.1 methodology
In this section, we study high-frequency data corresponding to the
collapse of the Bear Stearns in March 2008. The data used consists
of the week (five trading days) March 10–14, 2008, before the
merging announcement over the weekend as well as the two
following trading days March 17 and 18. On Friday, March 14, 2008,
at about 9:14 a.m., JP Morgan Chase and the Federal Reserves Bank
of New York announced an emergency loan to Bear Stearns (of about
29 billion, terms undisclosed) to prevent the firm from becoming
insolvent. This bailout was declared to prevent the very likely crash
of the market as a result of the fall of one of the biggest investment
\n\n=== PAGE 396 ===\n(11.11)
(11.12)
banks at the time. This measure proved to be insufficient to keep the
firm alive and 2 days later on Sunday March 16, 2008, Bear Stearns
signed a merger agreement with JP Morgan Chase essentially selling
the company for $2 a share (price revised on March 24 to
$10/share). The same stock traded at $172 in January 2007 and $93
a share in February 2007. Today, this collapse is viewed as the first
sign of the risk management meltdown of investment bank industry
in September 2008 and the subsequent global financial crisis and
recession.
If we take a closer look at (11.4) and do the following transformation
t → tc − t, where tc is a crash date.
we can observe that our function has log-periodic structure similar to
the (11.3). At this point, we do not know tc. The distance between two
periods in (11.4) will be:
From this, we can see that for fixed t0, then Nth period will occur at
time
This is exactly what we got in (11.2). If we assume that the periods of
(11.11) match market periods, we can do least square fitting and
estimate coefficient a. Moreover, we can estimate crash date tc by
using same technique as in the case of (11.2). This uses data only
prior to the market crash. By another least square fitting, we can
estimate coefficients α, β. With estimated coefficients α, β, a we can
do another least square fitting for (11.5) with argument tc − t for the
best fit of the data.
\n\n=== OCR PAGE 396 ===\nbanks at the time. This measure proved to be insufficient to keep the
firm alive and 2 days later on Sunday March 16, 2008, Bear Stearns
signed a merger agreement with JP Morgan Chase essentially selling
the company for $2 a share (price revised on March 24 to
$10/share). The same stock traded at $172 in January 2007 and $93
a share in February 2007. Today, this collapse is viewed as the first
sign of the risk management meltdown of investment bank industry
in September 2008 and the subsequent global financial crisis and
recession.

If we take a closer look at (11.4) and do the following transformation
t— t, — t, where t, is a crash date.

f(t, —f= pet ireatt -t)) (11.11),

we can observe that our function has log-periodic structure similar to
the (11.3). At this point, we do not know t,. The distance between two

periods in (11.4) will be:

log.(ty) — log. (tw41) = |
In(ty) — In(ty,,) = In(a)
tyy) = ty/In(a)

From this, we can see that for fixed tg, then Nth period will occur at
time

Inty = N - (—In(a)) + In(to) (41.12)

This is exactly what we got in (11.2). If we assume that the periods of
(11.11) match market periods, we can do least square fitting and
estimate coefficient a. Moreover, we can estimate crash date t, by
using same technique as in the case of (11.2). This uses data only
prior to the market crash. By another least square fitting, we can
estimate coefficients a, 8. With estimated coefficients a, B, a we can
do another least square fitting for (11.5) with argument t, — t for the
best fit of the data.
\n\n=== PAGE 397 ===\n(11.13)
11.4.2 RESULTS
In this part, we present data fitted with equation (11.5) with
argument tc − t.
We will show examples of four indices that experienced biggest
change in price during March 10 and 18.
For parameters α, β, and its least square fitting, we constructed the
following error function:
where 
 is the time where we want fF to touch the plot of
index price p(t). We get coefficients α, β by minimizing e.
11.5 Brief program code description
All programs for Sections 11.4 and 11.9 were written in MATLAB. Let
us introduce few crucial steps to clarify fitting process and whole
procedure. In the first step, we need to take “measured” data and
compute periods. In geophysics scenario, this would correspond to
the times between two earthquakes (preceding some major even), in
market scenario time between two minima or maxima of the stock
price. Since periods in (11.5) and (11.2) are invariant under
transformation t → (tc − t) (except its number). If we denote
measured periods as 
 and since (11.7), we can estimate
coefficients C1, C2 by lest square fitting, that is, minimizing equation
(11.6). The critical time, cumulative point, can be found using (11.8).
The connection to the market scenario is made through (11.12) and
thus estimation of tc will follow the same path. For high-frequency
scenario, we now already know α and tc, and we need to estimate
parameters α, β. This can be done by minimizing error function
(11.13). You can see that this is done only for “points of interest”; that
\n\n=== OCR PAGE 397 ===\n11.4.2 RESULTS

In this part, we present data fitted with equation (11.5) with
argument ¢, — t.

f(t.-)= Pet (Foealt —t)—x)+x)
Ic

We will show examples of four indices that experienced biggest
change in price during March 10 and 18.

For parameters a, B, and its least square fitting, we constructed the
following error function:

k
. 2 (11.13),
e(a, B.) = DY" (In(p(e,)) — In(f*e,)))"
i=]

where / = (t), --- . 44) is the time where we want f" to touch the plot of
index price p(t). We get coefficients a, 8 by minimizing e.

11.5 Brief program code description

All programs for Sections 11.4 and 11.9 were written in MATLAB. Let
us introduce few crucial steps to clarify fitting process and whole
procedure. In the first step, we need to take “measured” data and
compute periods. In geophysics scenario, this would correspond to
the times between two earthquakes (preceding some major even), in
market scenario time between two minima or maxima of the stock
price. Since periods in (11.5) and (11.2) are invariant under
transformation t — (t, — t) (except its number). If we denote
measured periods as T), 7, ...,T,, and since (11.7), we can estimate
coefficients C,, C, by lest square fitting, that is, minimizing equation
(11.6). The critical time, cumulative point, can be found using (11.8).
The connection to the market scenario is made through (11.12) and
thus estimation of t, will follow the same path. For high-frequency

scenario, we now already know a and t,, and we need to estimate

parameters a, B. This can be done by minimizing error function
(11.13). You can see that this is done only for “points of interest”; that
\n\n=== PAGE 398 ===\nis, we will minimize in such a way that final function will touch the
graph for market minima. At last, we will estimate parameter x in
(11.5), using all available data, again using least square fitting for x.
We will minimize the error function e(x) = ∑t(p(t) − fx(tc − t))2 with
parameters tc, a, α, β obtained previously.
11.6 Conclusion
In Section 11.4 and Figures 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, and 11.8,
we use the scale invariant equation (11.2) to estimate the crash date
of a following major event. The least square fitting methodology
helps estimating the crash date tc. We note that the data used when
fitting include all events except the major crash. As such, the
predicted time is typically but not always preceding the major event.
This methodology could be used in real time by looking at the minor
earthquakes since a major event and using the time intervals to make
better and better predictions as new data (new minor events) are
accumulating.
The second method takes all earthquake data surrounding a major
event. By picking the date of the first outlying observation, we are
able to make a prediction about the event time. Clearly since we use
events surrounding the major one and we pick the date of the first
(smallest magnitude) of the outlying observations, sometimes the
predicted date is after the major event. This should not be a problem
if the methodology is run in real time. As new magnitude
earthquakes are recorded, the TLF distribution is fitted and at the
date when the fit is not good anymore, an earthquake warning
should be issued.
We conclude by comparing the estimated crash dates, using the two
methods with the actual crash date in Table 11.3.
\n\n=== PAGE 399 ===\nTable 11.3 Comparison.
Graph
tc
test
Earthquake date
11.3
7,019
7,064
7,054
11.4
8,201
8,249
8,245
11.5
8,298
8,399
8,291
11.6
10,896 10,777
10,898
11.7
11,519 10,438
11,518
11.8
12,093 12,249
12,054
Same situation holds for the data in financial market. In [9, 10], the
financial data has been analyzed using the Lévy model to estimate
the crash date. In this chapter, we used the scale invariance method
to analyze the same. The closeness of the two results shows that both
methods are compatible.
11.A Appendix
11.A.1 STABLE DISTRIBUTIONS
We present a brief introduction of stable distributions.
Consider the sum of n independent identically distributed (i.i.d.)
random variables Xi,
Since the variables are independent, the distribution of their sum
may be obtained as the n-fold convolution,
The original distribution of the Xi’s is called stable if the functional
form of P[X(nΔt)] is the same as the functional form of P[X(Δt)].
Specifically, for any n ≥ 2, there exists a positive Cn and a Dn so that:
\n\n=== OCR PAGE 399 ===\nTable 11.3 Comparison.
Graph t, t.,, Earthquake date

11.3 7,019 7,064 7,054
11.4 8,201 8,249 8,245
11.5 8,298 8,399 8,291
11.6 10,896 10,777 10,898
11.7 11,519 10,438 11,518
11.8 12,093 12,249 12,054

Same situation holds for the data in financial market. In [9, 10], the
financial data has been analyzed using the Lévy model to estimate
the crash date. In this chapter, we used the scale invariance method
to analyze the same. The closeness of the two results shows that both
methods are compatible.

11.A Appendix
11.A.1 STABLE DISTRIBUTIONS

We present a brief introduction of stable distributions.

Consider the sum of n independent identically distributed (i.i.d.)
random variables X;,

X(nAt) =X, +X, +X, + +X,,.
Since the variables are independent, the distribution of their sum
may be obtained as the n-fold convolution,
P[X(nAd)] = P(X;) @ P(X>) ... ® P(X,

The original distribution of the X;’s is called stable if the functional

form of PLX(nAt)] is the same as the functional form of PLX(AD].
Specifically, for any n = 2, there exists a positive C,, and a D,, so that:
\n\n=== PAGE 400 ===\n(11.14)
where X has the same functional form of the distribution as Xi for i =
1, 2, …, n. If Dn = 0, X is said to have a strictly stable distribution. It
can be shown (see [13]) that
for some parameter α, 0 < α ≤ 2.
Stability is a very desirable property of a distribution. Many
stochastic processes can be defined in such a way that their
increments are independent. Specifically,
and if the increments are all distributed according to a stable
distribution, then the process X(t) itself will have a stable
distribution, regardless of the value of n.
11.A.2 CHARACTERIZATION OF STABLE
DISTRIBUTIONS
Lévy [14] and Khintchine [15] found the most general form of the
stable distributions. The general representation is through the
characteristic function φ(q) associated with the distribution. The
most common parametrization is:
where α is called the index of stability (tail index, tail exponent, or
characteristic exponent), γ is a positive scale factor, μ is a location
parameter, and β ∈ [ − 1, 1] is a skewness (asymmetry) parameter.
\n\n=== OCR PAGE 400 ===\nP[X(nAd)] = P[C,X + D,]

where X has the same functional form of the distribution as X; for i =
1, 2, ..., n. If D,, = 0, X is said to have a strictly stable distribution. It
can be shown (see [13]) that

C,=ne

n
for some parameter a, 0 < a < 2.

Stability is a very desirable property of a distribution. Many
stochastic processes can be defined in such a way that their
increments are independent. Specifically,

t n—-1
X(t) = X(0O) + (x (<) -x(0)) tet (xo) -X (~—*)) :
and if the increments are all distributed according to a stable
distribution, then the process X(‘) itself will have a stable
distribution, regardless of the value of n.

11.A.2 CHARACTERIZATION OF STABLE
DISTRIBUTIONS

Lévy [14] and Khintchine [15] found the most general form of the
stable distributions. The general representation is through the
characteristic function @(q) associated with the distribution. The
most common parametrization is:

(11.14),
ing — y“\q\" [! — ip tan()| ifa €(0,21\ {1}

In(g(q)) = 2 we

ing — yla\ [i + ip S >in lal ifa=1

where a is called the index of stability (tail index, tail exponent, or

characteristic exponent), y is a positive scale factor, is a location

parameter, and B ¢€ [ - 1, 1] is askewness (asymmetry) parameter.
\n\n=== PAGE 401 ===\nIt is very easy to understand why this form of characteristic function
determines the stable distributions. The characteristic function of a
convolution of independent random variables is simply the product
of the respective characteristic functions and, thus owing to the
special exponential form in (11.14) all stable distributions, are closed
under convolutions for a fixed value of α and time increments of
same size. All such distributions are heavy tailed (leptokurtic) and in
fact have no second moments for any α < 2. When α < 1 the
distribution does not even have the first moment. Please note that
neither third nor fourth moment exists for these distributions so the
usual measures of skewness and kurtosis are undefined.
The characteristic function always exists and thus the definition
given above is proper; however, finding the associated density
function is not always possible (it may not exist). Typically, one of
the inversion formulae is used, but the analytical form of the stable
distributions is known only for a few values of α and β:
1. When α = 2, one obtains the Gaussian (normal) distribution with
mean μ and variance 2γ2 (the skewness parameter β has no
effect).
2. When 
, β = 1, the distribution is called the Lévy–Smirnov
distribution.
3. When α = 1 and β = 0, we obtain the Cauchy (Lorentz)
distribution
\n\n=== OCR PAGE 401 ===\nIt is very easy to understand why this form of characteristic function
determines the stable distributions. The characteristic function of a
convolution of independent random variables is simply the product
of the respective characteristic functions and, thus owing to the
special exponential form in (11.14) all stable distributions, are closed
under convolutions for a fixed value of a and time increments of
same size. All such distributions are heavy tailed (leptokurtic) and in
fact have no second moments for any a < 2. When a < 1 the
distribution does not even have the first moment. Please note that
neither third nor fourth moment exists for these distributions so the
usual measures of skewness and kurtosis are undefined.

The characteristic function always exists and thus the definition
given above is proper; however, finding the associated density
function is not always possible (it may not exist). Typically, one of
the inversion formulae is used, but the analytical form of the stable
distributions is known only for a few values of a and B:

1. When a = 2, one obtains the Gaussian (normal) distribution with

mean yp and variance 2y? (the skewness parameter B has no
effect).

-o-p)

1 f
F(x) = e +
4ny

_1
2. When “ ~ 3, B =4, the distribution is called the Léevy-Smirnov

distribution.
o—¥ [2x—p)
f(x) = oar" ,ifx>p
V 2n 3
(x — ft)?

3. When a = 1 and B = 0, we obtain the Cauchy (Lorentz)
distribution

1 Y
(x) = -—-——
Fe) my? +(x— yp)?
\n\n=== PAGE 402 ===\nReferences
1. A. Johansen, O. Ledoit, D. Sornette, Crashes as critical points,
International Journal of Theoretical Applied Finance, January
2000, Volume 3, No. 1, pp. 219–255.
2. M.C. Mariani, Y. Liu, A new analysis of the effect of the Asian
crisis of 1997 on emergent markets, Physica A: Statistical
Mechanics and Its Applications, July 1, 2007, Volume 380, pp.
307–316.
3. M.C. Mariani, P. Bezdek, L. Serpa, I. Florescu, Ising type models
applied to geophysics and high frequency market data, Physica A,
November 1, 2011, Volume 390, Issues 23–24, pp. 4396–4402.
4. R.N. Mantegna, H.E. Stanley, An Introduction to Econophysics:
Correlations and Complexity in Finance, Cambridge University
Press, Cambridge, 1999.
5. A. Johansen, D. Sornette, Large financial crashes, Physica A,
1997, Volume 245 (N3-4), pp. 411–422.
6. M.C. Mariani, Y. Liu, A new analysis of intermittence, scale
invariance and characteristic scales applied to the behavior of
financial indices near a crash, Physica A, July 15, 2006, Volume
367, pp. 345–352.
7. M. Ferraro, N. Furman, Y. Liu, M.C. Mariani, D. Rial, Analysis of
intermittence, scale invariance and characteristic scales in the
behavior of major indices near a crash, Physica A, January 1,
2006, Volume 359, pp. 576–588.
8. The USGS Earthquake Magnitude Working Group, USGS
Earthquake Magnitude Policy,USGS. March 29, 2010. Web
November 26, 2010.
http://earthquake.usgs.gov/aboutus/docs/020204mag_policy.p
hp.
9. M.C. Mariani, Y. Liu, Normalized truncated Levy walks applied to
the study of financial indices, Physica A, 2007, Volume 377, pp.
590–598
10. E. Barany, M.P. Beccar Varela, I. Florescu, I. SenGupta, Detecting
market crashes by analysing long-memory effects using high-
frequency data, Quantitative Finance, 2012, Volume 12, Issue 4,
pp. 623–634.
\n\n=== PAGE 403 ===\n11. R.N. Mantegna, H.E. Stanley, Physical Review Letters, 1994,
Volume 73, pp. 2946.
12. I. Koponen, Physical Review, 1995, Volume E 52, pp. 1197.
13. G. Samorodnitsky, M.S. Taqqu, Stable Non-Gaussian Random
Processes: Stochastic Models with Infinite Variance, Chapman
and Hall, New York, 1994.
14. P. Lévy, Calcul des probabilités, Gauthier-Villars, Paris, 1925.
15. A. Ya. Khintchine, P. Lévy, Sur les lois stables, Comptes Rendus
de I’Academie des Sciences Paris, 1936, Volume 202, pp. 374.
16. F. Black, M. Scholes, The pricing of options and corporate
liabilities, Journal of Political Economics, 1973, Volume 81, pp.
637–659, 1973.
17. D. Stauffer, Social applications of two-dimensional Ising models,
American Journal of Physics, April 2008, Volume 76, Issue 4,
pp. 470.
\n\n=== PAGE 404 ===\nChapter Twelve
Analysis of Generic Diversity in the Fossil Record,
Earthquake Series, and High-Frequency Financial Data
M. P. Beccar Varela1, F. Biney1, Maria C. Mariani1, I. SenGupta2, M. Shpak3, and
P. Bezdek4
1Department of Mathematical Sciences, University of Texas at El Paso, El Paso,
TX, USA
2Department of Mathematics, North Dakota State University, Fargo, ND, USA
3NeuroTexas Institute, St. David's Medical Center, Austin, TX, USA
4Department of Mathematics, The University of Utah, Salt Lake City, UT, USA
12.1 Introduction
In order to develop an understanding of the patterns and processes in the history of
plant and animal life on Earth, paleontologists have attempted to quantify and
analyze the diversity of taxonomic diversity through geological time (see Sepkoski,
1977, 1979 [1], and the references therein). Changes in biological diversity are driven
by the origination of new taxa and by the extinction of previously extant taxa. In
describing extinction dynamics, several studies have noted a qualitative difference
between “background” (low mean and variance) and mass extinction (high mean and
variance) events. Similarly, one can speak of background and mass origination
events, where the latter would correspond to large-scale adaptive radiations
characterized by the rapid origin of large numbers of species and higher taxa, while
the former refers to origination rates during more ecologically “normal” periods.
In this chapter, we analyze the paleontological database compiled by Sepkoski [2] on
the origination and extinction of fossil genera during the Phanerozoic (a time
interval that spans from shortly before the Cambrian period, approximately 540
million years ago to the present). The principal goals of this analysis are to (a)
determine what class of probability distributions best characterizes the magnitude of
extinction events, particularly when dealing with extreme values induced by mass
extinctions, and (b) determine what class of distributions provides the most accurate
representation of the overall distribution of generic diversity in geological time.
We consider the following variables when describing the temporal dynamics of
generic diversity in the fossil record: following [3], where age is the time before
present (in epochs and stages measured in millions of years, Ma), diversity is the
number of individuals at time age, orig is the number of genera born at time age,
and ext is the number of genera that became extinct at time age.
This study endeavors to characterize both the distribution of extinction magnitudes
and the overall distribution of generic diversity.
\n\n=== PAGE 405 ===\nWe begin by presenting some theoretical results on convolutions of exponential
waiting times, as this simple model offers a straightforward starting point for
analyzing origination and extinction data. The resulting distributions will be used for
comparison with the data sets. We also analyze high-frequency financial data by
using Lévy models.
With the availability of high-frequency data (HFD) for financial market analysis,
there has been an increase in the studies dealing with the persistence of shocks in
both the mean and variance of financial instruments return. Several studies report
evidence of persistence (long-memory) behavior in squared returns or empirical
volatilities; see Breidt et al. [4], Robinson [5], Shephard [6], Lobato and Savin [7],
and Baillie [8]. Similar features have been observed in data from other fields such as
physics and geophysics. In physics, the presence of strong autocorrelation in the
squares of differences in velocity of the mean wind direction has been explored by
Barndorff-Nielsen and Shephard [9].
The pioneering work of Box et al. [10] in the area of autoregressive moving average
models paved the way for related work in the area of volatility modeling with the
introduction of ARCH and then GARCH models by Engle [11] and Bollerslev [12],
respectively. In terms of the statistical framework, these models provide motion
dynamics for the dependency in the conditional time variation of the distributional
parameters of the mean and variance, in an attempt to capture such phenomena as
autocorrelation in returns and squared returns. Extensions to these models have
included more sophisticated dynamics such as the threshold model (TGARCH) [13]
to capture the asymmetry in the news impact, the NGARCH model [14], the
EGARCH models [15], the stochastic volatility models [16], the FIGARCH and
FIEGARCH models [17], and the long-memory generalized autoregressive
conditionally heteroskedastic (LMGARCH) models [5, 18] as well as distributions
other than the normal to account for the skewness and excess kurtosis observed in
practice such as Student’s t test, Generalized error, Generalized Hyperbolic, the
Normal inverse Gaussian distribution, and Johnson’s SU distribution.
In the literature, the most popular GARCH model is the GARCH(1,1), where the
persistence parameter is less than 1 to ensure covariance stationarity. It turns out
that on modeling using GARCH, most often the persistence parameter is
approximately 1 but yet the model does not adequately capture the persistence in
volatility. This fact motivated the introduction of the integrated GARCH (IGARCH)
model where Bollerslev and Engel [19] allowed for unit persistence in the GARCH
model; that is, the persistence was set to 1. The IGARCH model has some structural
complication in the sense that its unconditional variance does not exist. Baillie et al.
[8] extended the IGARCH to the fractional IGARCH (FIGARCH) by allowing for
high persistence (long memory) directly in the conditional variance while avoiding
the complications of IGARCH; that is, they allowed the integration coefficient to vary
between [0,1].
Our main interest is to investigate the underlying volatility process in earthquake
series, explosive series, high-frequency financial data, and financial indices and
examines the applicability of a range of GARCH specifications for modeling volatility
of these series to identify similarities and differences in the volatility structure.
\n\n=== PAGE 406 ===\n12.2 Statistical preliminaries and results
In this section, we present some theoretical results, which will be used later in this
chapter for the analysis of the data.
Theorem 12.1 If Xi >are independent and identically distributed exponential
random variables with rate parameter λ
then
Sum of Xi has Gamma distribution with shape parameter n and scale parameter 1/
λ.
As a result, if we assume that generic extinction time is being exponentially
distributed with mean 1/λ, then by Theorem 12.1, the waiting time for the extinction
of an ensemble of genera will approximately follow a gamma distribution.
12.2.1 SUM OF EXPONENTIAL RANDOM VARIABLES WITH
DIFFERENT PARAMETERS
Suppose X and Y follow exponential distributions with parameters β1,β2. Let X has
probability density function 
 and Y has p.d.f. 
, where X and
Y are independent. Applying the change of variables u = x + y and v = x, we obtain
the Jacobian of this transformation is 1. Thus, the joint pdf in new variables is given
by
Therefore, when β1 ≠ β2,
When β1 = β2,
\n\n=== OCR PAGE 406 ===\n12.2 Statistical preliminaries and results
In this section, we present some theoretical results, which will be used later in this
chapter for the analysis of the data.

Theorem 12.1 If X; >are independent and identically distributed exponential
random variables with rate parameter A

X; ~ Exponential(1/A)

then

Y= yx ~ Gamma(n, 1/4).
i=l
Sum of X; has Gamma distribution with shape parameter n and scale parameter 1/
A
As a result, if we assume that generic extinction time is being exponentially

distributed with mean 1/A, then by Theorem 12.1, the waiting time for the extinction
of an ensemble of genera will approximately follow a gamma distribution.

12.2.1 SUM OF EXPONENTIAL RANDOM VARIABLES WITH
DIFFERENT PARAMETERS

Suppose X and Y follow exponential distributions with parameters B,,B,. Let X has
- ele ay elle

probability density function fx) = 4, and Yhas p.df. Fr) = 8, , where X and

Y are independent. Applying the change of variables u = x + y and v = x, we obtain

the Jacobian of this transformation is 1. Thus, the joint pdf in new variables is given

by
od (11 _u

Therefore, when B, # B.,

ru ; ;
f w= | fu, v)dv = eA: eH/Pa)
v 0 By — py |

When B, = Bo,
\n\n=== PAGE 407 ===\n(12.1)
So u ~ Γ(2, β1). Similarly with random variables X, Y, Z with probability density
functions 
, 
, and 
, then using a change of variable
we arrive to
so that
and if β1 = β2 = β3, 
, then
Generally, if we have n random variables Xi with probability density functions 
,
then the probability density function of X = ∑n
i = 1Xi will have the probability density
function given by a convolution of probability density functions [20].
In the case of n independent variables following p.d.f. 
 with different
coefficient β, it is more convenient to work with the characteristic function, which is
just a Fourier transform of a probability density function. Convolution theorem
states that Fourier transform of convolution of two functions is the product of their
Fourier transform.
thus if we use this theorem on (12.1), we get
\n\n=== OCR PAGE 407 ===\nuel

fy(u) = R =T1(2, A).
Sou ~ I(2, B,). Similarly with random variables X, Y, Z with probability density
ella ay entlls
functions Sxl) = ~ fi O)= ,and Sa) 6; , then using a change of variable
u=xtyt+zZ vexty, w=x,
we arrive to
f(u, v) = ————— Fy xP v(1/B, — 1/63) — exp(—v(1/f, — 1/8;))] .
ne
so that
ru Bo few a
f(u, v)dv = —— | ———_
0 B ~ By B ~ Bs
ps [“ _ a
-— —— | = fu.
Bi — By Py — Bs 0
. f(u,v) = Lewy
and if B, = B, = B,, & , then

“ 1 uw
Sulu) -[ fu, v)dv = Se"? — = 13, f).
[a o Bo > B

Generally, if we have n random variables X; with probability density functions Sx, 09, >
then the probability density function of X = 5”; _ ,X; will have the probability density
function given by a convolution of probability density functions [20].

F(x) = (fx, * (fx, * (fx, * ...)))[x]. (12.1)
el
In the case of n independent variables following p.d.f. f@) = “? with different
coefficient B, it is more convenient to work with the characteristic function, which is
just a Fourier transform of a probability density function. Convolution theorem
states that Fourier transform of convolution of two functions is the product of their
Fourier transform.

Pif * g} =FUf}- Fla}

thus if we use this theorem on (12.1), we get
\n\n=== PAGE 408 ===\n(12.2)
(12.3)
(12.4)
(12.5)
but 
 is just characteristic function of Xi denoted by 
. We showed that
In other words, characteristic function of sum of independent random variables is
product of their characteristic functions.
12.2.1.1 General version for n random variables
In this subsection, we assume the following parametrization, using scale parameter
β, of the exponential random variable probability density function.
The characteristic function for this random variable is [20]
We also know, using (12.2), that sum of independent identically distributed
exponential random variables will have characteristic function
and this defines the gamma distribution by Theorem 12.1.
Similarly, the sum of independent exponential random variables X1, …, Xn with scale
parameters β1, …, βn will have the characteristic function
Definition 12.1 (Hypoexponential distribution) [21, Ch. 7.6.3.] We say that the
random variable X is hypoexponentially distributed if its characteristic function is
of form (12.5). Sometimes it is also called a generalized Erlang distribution.
We will show that the sum of exponential random variables with different
parameters βk (12.5), where βk differs by a small variation from some , can be
reasonably approximated by the gamma distribution (12.4).
We assume the following scale parameters βk:
\n\n=== OCR PAGE 408 ===\nFf) = Fx, * Ox, * Fx, * DI = T] Pte)
i=l

but * Ux.) ig just characteristic function of X; denoted by ?%, We showed that

Px = Il Px,
i=l

In other words, characteristic function of sum of independent random variables is
product of their characteristic functions.
12.2.1.1 General version for n random variables

In this subsection, we assume the following parametrization, using scale parameter
B, of the exponential random variable probability density function.

fis) = Lee > 0 (42.3)
B
The characteristic function for this random variable is [20]

ox(t) = (1 — itp).

We also know, using (12.2), that sum of independent identically distributed
exponential random variables will have characteristic function

: 12.
93x, = [] ox, = - itp) (12.4)
k=I
and this defines the gamma distribution by Theorem 12.1.

Similarly, the sum of independent exponential random variables X,, ..., X,, with scale
parameters B,, ..., B,, will have the characteristic function

. ' a (42.5)
esx, = [] ox,0 = [a - ite
kl k=1

Definition 12.1 (Hypoexponential distribution) /21, Ch. 7.6.3.] We say that the
random variable X is hypoexponentially distributed if its characteristic function is
of form (12.5). Sometimes it is also called a generalized Erlang distribution.

We will show that the sum of exponential random variables with different
parameters [; (12.5), where [, differs by a small variation from some B, can be
reasonably approximated by the gamma distribution (12.4).

We assume the following scale parameters [;:
\n\n=== PAGE 409 ===\n(12.6)
(12.7)
(12.8)
(12.9)
(12.10)
where Δβk is a small perturbation from the mean value . If we substitute βk into
(12.3), we get
This function is holomorphic in Δβk for small values of Δβk; thus, it has power series
expansion in Δβk at zero.
Using this result and substituting (12.8) into (12.5), we get
multiplying this expression gives us
and from ∑n
k = 0Δβk = 0 from (12.6), we finally get
Therefore, the characteristic function (12.10) differs from that of the gamma
distribution (12.4) by a term depending on a square of perturbation Δβk. This
justifies our fit using the gamma distribution with a scale parameter equal to the
mean value .
\n\n=== OCR PAGE 409 ===\nB, = B+ Ap,,k € (1, ...,.n} (12.6)
p= 70s
ne

o= Yah

where Af; is a small perturbation from the mean value B. If we substitute 6, into
(12.3), we get

ox, = (1 — itp — itAp,)'. (12.7)

This function is holomorphic in Af; for small values of AB;; thus, it has power series
expansion in Af; at zero.

ox, = (1 = itp)! + il — itp? AP, + ORD (12.8)
Using this result and substituting (12.8) into (12.5), we get
eyx, (0 = (a — itp) + itl — itp)? AP, + OAK;
(a — itp)! + itl — itpy 7 AP, + OAR;

)
)
((1 = ied)" + incl — itfy2ap, + OC—2) -
(a — inpy"! + itl — itpy2 Ap, + Ap}))
))

(a — itp)! + inl — itpy AB, + OCA?

multiplying this expression gives us

n

Pyx,0 = (= itp)" + it — itp! Dy Af, + O(Af?) G29)
k=1
and from >"; - >AB, = 0 from (12.6), we finally get
@y x,() = (1 = if)" + OAL). (12.10),

Therefore, the characteristic function (12.10) differs from that of the gamma
distribution (12.4) by a term depending on a square of perturbation AB; This
justifies our fit using the gamma distribution with a scale parameter equal to the
mean value /.

\n\n=== PAGE 410 ===\n12.3 Statistical and numerical analysis
The first analysis involves constructing a frequency distribution of the number of
genera over all geological stages. At first, we transform the diversity data set, using
the Johnson transformation (JT) function. The transformation is applied because
the raw data is nonnormal, principally as a consequence of the net increase in the
number of taxa (both actual and as a sampling artifact of the “pull of the recent”)
over recent intervals. The transformed function becomes
where x represents the number of genera.
Fits of the generic diversity data against different classes of distributions (normal,
lognormal, two-parameter exponential, and Weibull) are shown in Figure 12.1 given
later. The normal distribution gives the best fit, corresponding to the highest p-value
under an Anderson–Darling (AD) test. In Figure 12.2, a histogram for the JT
diversity data is given. It is clear from Figure 12.3 that a JT for the diversity data gets
provides a closer to the normal distribution, particularly following two applications
of the transformation.
p-value for the best fit: 0.247484
Z for best fit: 0.84
Best transformation type: SB
Transformation function equals 
\n\n=== OCR PAGE 410 ===\n12.3 Statistical and numerical analysis

The first analysis involves constructing a frequency distribution of the number of
genera over all geological stages. At first, we transform the diversity data set, using
the Johnson transformation (JT) function. The transformation is applied because
the raw data is nonnormal, principally as a consequence of the net increase in the
number of taxa (both actual and as a sampling artifact of the “pull of the recent”)
over recent intervals. The transformed function becomes

f(x) = -0.292746 + 0.616307 sinh™!((x — 1080.08) /297.129)

where x represents the number of genera.

Fits of the generic diversity data against different classes of distributions (normal,
lognormal, two-parameter exponential, and Weibull) are shown in Figure 12.1 given
later. The normal distribution gives the best fit, corresponding to the highest p-value
under an Anderson—Darling (AD) test. In Figure 12.2, a histogram for the JT
diversity data is given. It is clear from Figure 12.3 that a JT for the diversity data gets
provides a closer to the normal distribution, particularly following two applications
of the transformation.

¢ p-value for the best fit: 0.247484
¢ Z for best fit: 0.84

¢ Best transformation type: SB

0.4440.95 + 1.13368 In (sams

¢ Transformation function equals 2.39370-X
\n\n=== PAGE 411 ===\nFIGURE 12.1 Plot of the frequency of the number of genera compared with
different distribution (with p-values computed from AD test indicating goodness of
fit).
\n\n=== OCR PAGE 411 ===\nProbability plot for JT of diversity

Goodness of fit test

Normal - 95% CI 3-parameter lognormal - 95% CI
Ze Normal
9 AD =0.619
20 P-value = 0.105
z 0 3-Parameter lognormal
4
10
1
0.1
0 z5 4 6 8 10
'T of diversity of diversity - threshold
H , a u 3-Parameter Weibull
2-Parameter exponential - 95% Cl 2-Parameter exponential -95% C1 | AD = 0.768
99.9 P-value = 0.045
n La
90
50
A 10 J
«
1
1 0.1
0001 001 O01 1 10 0.1 1 10
IT of diversity - threshold IT of diversity - threshold

FIGURE 12.1 Plot of the frequency of the number of genera compared with
different distribution (with p-values computed from AD test indicating goodness of
fit).
\n\n=== PAGE 412 ===\nFIGURE 12.2 Histogram of (a) JT for diversity data and (b) two applications of JT
for diversity data.
\n\n=== OCR PAGE 412 ===\n‘Histogram (with norml curve) of JT of diversity

(@) JT for diversity data

‘Histogram of two applications of JT
Normal

“1 i 1 2
‘Two applications of JT
(b) Two applications of JT for diversity data

FIGURE 12.2 Histogram of (a) JT for diversity data and (b) two applications of JT
for diversity data.

\n\n=== PAGE 413 ===\nFIGURE 12.3 Plot of two applications of Johnson’s transformation to the diversity
data.
We also fitted distributions of the magnitude of extinction events to the different
classes of hypothesized distributions; as shown in Figure 12.4, this data is plotted
with different distributions. In view of all these, the best fit, defined again as having
the highest p-value under the AD test, is provided by the maximum extreme value
distribution. The second best fit is to the Gamma distribution, as we would expect
from the theory presented in the previous sections. Figures 12.5, 12.6, 12.7 and 12.10
provide other statistical studies for the extinction magnitude data and the diversity
data.
\n\n=== OCR PAGE 413 ===\nProbability plot of two applications of JT
Normal - 95% CI

0.02993
1.146

107
0.466
0.247

Percent

v4
w
+

oY eh =) tlt I
‘Two applications of JT

FIGURE 12.3 Plot of two applications of Johnson’s transformation to the diversity
data.

We also fitted distributions of the magnitude of extinction events to the different
classes of hypothesized distributions; as shown in Figure 12.4, this data is plotted
with different distributions. In view of all these, the best fit, defined again as having
the highest p-value under the AD test, is provided by the maximum extreme value
distribution. The second best fit is to the Gamma distribution, as we would expect
from the theory presented in the previous sections. Figures 12.5, 12.6, 12.7 and 12.10
provide other statistical studies for the extinction magnitude data and the diversity
data.

\n\n=== PAGE 414 ===\nFIGURE 12.4 Histogram of extinction magnitude fitting with (a) the largest
extreme value distribution; (b) the Gamma distribution; and (c) the 3-parameter
Gamma distribution.
\n\n=== OCR PAGE 414 ===\n(b) The Gamma distribution

(©) The 3-parameter Gamma distribution

FIGURE 12.4 Histogram of extinction magnitude fitting with (a) the largest
extreme value distribution; (b) the Gamma distribution; and (c) the 3-parameter
Gamma distribution.
\n\n=== PAGE 415 ===\nFIGURE 12.5 Probability plot of extinction magnitude data.
\n\n=== OCR PAGE 415 ===\n99.9
99

90
50
10

I

0.1

10
1

0.1

ZA.

7
1000

Fa

Probability plot for extinction magnitude

Smallest extreme value - 95% Cl

Largest extreme value - 95% CI

2000-1000 0

Extinction magnitude

Gamma - 95% CL

99.9
99

Eo
C4

50
10
0.1

e
e
0 500 1000 1500

Extinction magnitude

3-Parameter gamma - 95% Cl

10 100 1000
Extinction magnitude

99.9.
99,
90
50

l

10

gi

Extinction magnitude - threshold

Probability plot for extinction magnitude

Logistic - 95% CI

Loglogistic - 95% CI

e
e
Bee
0 800 1600

3-parameter loglogistic - 95% CI

100 1000 10000
Extinction magnitude - threshold

10
1

0.1

After Johns

10 100 000
Extinction magnitude

10000

Normal - 95% CI

D 6 2
Extinction magnitude
son transformation

Goodness of fit test

Smallest extreme value
AD = 13.674
P-value < 0.010

Largest extreme value
AD = 0.625
P-value = 0.100

Gamma
AD =0.799
P-value = 0.043

3-Parameter Gamma
AD = 0.818
P-value = *

Goodness of fit test

Logistic
AD = 1.697
P-value < 0,005

Loglogistic
AD = 0.743
P-value < 0.031

3-Parameter loglogistic

Johnson transformation
AD = 0.525
P-value = 0.177

\n\n=== PAGE 416 ===\nFIGURE 12.6 Probability plot of extinction magnitude data.
FIGURE 12.7 JT for extinction magnitude data.
\n\n=== OCR PAGE 416 ===\nFIGURE 12.6 Probability plot of extinction magnitude data.

Johnson transformation for extinction magnitude
Probability plot for original data Select a transformation

P-value for AD test

(P-value = 0.005 means < = 0.005)

Probability plot for transformed data
99.9:

P-value for best fit: 0.177287

Z for best fit: 0.54

Best transformation type: SU

Transformation function equals

0.886564 + 1.16076 * Asinh ((X - 138.096) / 116.804)

FIGURE 12.7 JT for extinction magnitude data.
\n\n=== PAGE 417 ===\n(12.11)
FIGURE 12.8 Diversity (red dots) and its linear interpolation (blue line).
12.4 Analysis with Lévy distribution
In this section, we present an analysis of the total number of genera through
geological time, using truncated Lévy distributions. It is proposed that this class of
distributions may optimally characterize diversity dynamics, which are driven by low
background origination and extinction events punctuated by rare events (mass
extinctions or adaptive radiations) of much larger magnitude.
12.4.1 CHARACTERIZATION OF STABLE DISTRIBUTIONS
Lévy [22] and Khintchine [23] found the most general form of the stable
distributions. The general representation is through the characteristic function φ(q)
associated with the distribution. The most common parametrization is:
where α is called the index of stability (tail index, tail exponent, or characteristic
exponent), γ is a positive scale factor, μ is a location parameter, and β ∈ [ − 1, 1] is a
skewness (asymmetry) parameter. For the index α > 2, the inverse Fourier transform
\n\n=== OCR PAGE 417 ===\n6000 -

5000

4000

3000

Number of fossil genera

1000

0
-600 -500 -400 -300 -200 -100 ie)
Millions of years before present

FIGURE 12.8 Diversity (red dots) and its linear interpolation (blue line).

12.4 Analysis with Lévy distribution

In this section, we present an analysis of the total number of genera through
geological time, using truncated Lévy distributions. It is proposed that this class of
distributions may optimally characterize diversity dynamics, which are driven by low
background origination and extinction events punctuated by rare events (mass
extinctions or adaptive radiations) of much larger magnitude.

12.4.1 CHARACTERIZATION OF STABLE DISTRIBUTIONS

Lévy [22] and Khintchine [23] found the most general form of the stable
distributions. The general representation is through the characteristic function @(q)
associated with the distribution. The most common parametrization is:

ing —y“lql* [ - ip Eran (22)] irae o,21\(1) 2D
In(g(q)) = q a ~ :
ing — ylal 1 +19 22 nal ifa=1
lq z

where a is called the index of stability (tail index, tail exponent, or characteristic
exponent), y is a positive scale factor, p is a location parameter, and B € [-1,1]isa
skewness (asymmetry) parameter. For the index a > 2, the inverse Fourier transform
\n\n=== PAGE 418 ===\nof φ gives a function that is not positive semidefinite and thus cannot be considered
as probability density function [24].
It is easily demonstrated that the form of the characteristic function determines the
stable distributions. The characteristic function of a convolution of independent
random variables is simply the product of the respective characteristic functions and
thus, owing to the special exponential form above, all stable distributions are closed
under convolutions for a fixed value of α and time increments of the same size. All
such distributions are heavy tailed (leptokurtic) and in fact have no second moments
for any α < 2. When α < 1, the distribution does not even have the first moment. It is
remarked that neither third nor fourth moment exists for these distributions so the
usual measures of skewness and kurtosis are undefined.
The characteristic function always exists and thus the definition given in (12.11)
always holds; however, finding the associated density function is not always possible
(i.e., it may not exist). Typically, one of the inversion formulae is used but the
analytical form of the stable distributions is known only for a few values of α and β:
1. When α = 2, one obtains the Gaussian (normal) distribution with mean μ and
variance 2γ2 (the skewness parameter β has no effect).
2. When 
, β = 1, the distribution is called the Lévy–Smirnov distribution.
3. When α = 1 and β = 0, we obtain the Cauchy (Lorentz) distribution,
12.4.2 TRUNCATED LéVY FLIGHT (TLF) DISTRIBUTION
The leptokurtic property of the stable distributions for α < 2 has wide applicability to
model processes where extreme events have a greater probability of occurring in
comparison to normal or exponential distributions. Choosing to use Lévy–
Khintchine-type distributions is seemingly straightforward; however, the infinite
variance of these distributions for any α < 2 becomes a source of complication when
working with real data. In order to avoid this problem, Mantegna and Stanley [25]
consider a Lévy-type distribution truncated at some parameter l, which gives a
distribution with finite variance. This distribution was named the Truncated Lévy
flight (henceforth TLF):
\n\n=== OCR PAGE 418 ===\nof @ gives a function that is not positive semidefinite and thus cannot be considered
as probability density function [24].

It is easily demonstrated that the form of the characteristic function determines the
stable distributions. The characteristic function of a convolution of independent
random variables is simply the product of the respective characteristic functions and
thus, owing to the special exponential form above, all stable distributions are closed
under convolutions for a fixed value of a and time increments of the same size. All
such distributions are heavy tailed (leptokurtic) and in fact have no second moments
for any a < 2. When a < 1, the distribution does not even have the first moment. It is
remarked that neither third nor fourth moment exists for these distributions so the
usual measures of skewness and kurtosis are undefined.

The characteristic function always exists and thus the definition given in (12.11)
always holds; however, finding the associated density function is not always possible
(i.e., it may not exist). Typically, one of the inversion formulae is used but the
analytical form of the stable distributions is known only for a few values of a and B:

1. When a = 2, one obtains the Gaussian (normal) distribution with mean p and
variance 2y? (the skewness parameter f has no effect).

e

fa=
4ny

=!
2. When “ ~ 3, f = 1, the distribution is called the Lévy-Smirnov distribution.

7 /2(x—p)
f(x) = y eae ifx>yp
“(X= fh)?

3. When a = 1 and B = 0, we obtain the Cauchy (Lorentz) distribution,

l y
x) = -—>—— ..
fe) ry t(x— py)?

12.4.2 TRUNCATED LéVY FLIGHT (TLF) DISTRIBUTION

The leptokurtic property of the stable distributions for a < 2 has wide applicability to
model processes where extreme events have a greater probability of occurring in
comparison to normal or exponential distributions. Choosing to use Lévy—
Khintchine-type distributions is seemingly straightforward; however, the infinite
variance of these distributions for any a < 2 becomes a source of complication when
working with real data. In order to avoid this problem, Mantegna and Stanley [25]
consider a Lévy-type distribution truncated at some parameter /, which gives a
distribution with finite variance. This distribution was named the Truncated Lévy
flight (henceforth TLF):
\n\n=== PAGE 419 ===\nwhere P(x) denotes any symmetric Lévy distribution obtained from its characteristic
function in (12.11) when β = 0, 1A(x) is the indicator function of the set A and c is
normalizing constant.
Clearly, as l → ∞, one obtains the regular Lévy distribution characterized by P( · ) (a
stable distribution). However, the TLF distribution itself is not stable for any finite
truncation level l. Instead, this distribution has finite variance; thus, independent
variables drawn from this distribution satisfy a regular central limit theorem. If the
parameter l is large, the convergence to the limiting Gaussian may be very slow [25].
If the parameter l is small (so that the convergence is fast), the cut in its tails is very
abrupt.
In order to have continuous tails, Koponen [26] considered a TLF in which the cut
function is a decreasing exponential characterized by a separate parameter l. The
characteristic function of this distribution, when α ≠ 1, can be expressed as:
with c1 a scale factor:
and
In the case of symmetric distributions β = 0, the variance can be calculated using the
characteristic function:
All the following discussion is in this symmetric case (β = 0).
If we use time steps Δt, and T = NΔt, following the discussion in the previous
section, at the end of each interval, we must calculate the sum of N stochastic
variables that are independent and identically distributed. Therefore, the
characteristic function of the sum is:
The model can be improved by standardizing it. If the variance is given by:
\n\n=== OCR PAGE 419 ===\nT(x) = cP(x)1_p)(),

where P(x) denotes any symmetric Lévy distribution obtained from its characteristic
function in (12.11) when B = 0, 1,(x) is the indicator function of the set A and c is

normalizing constant.

Clearly, as | — «, one obtains the regular Lévy distribution characterized by P( - ) (a
stable distribution). However, the TLF distribution itself is not stable for any finite
truncation level /. Instead, this distribution has finite variance; thus, independent
variables drawn from this distribution satisfy a regular central limit theorem. If the
parameter / is large, the convergence to the limiting Gaussian may be very slow [25].
If the parameter / is small (so that the convergence is fast), the cut in its tails is very
abrupt.

In order to have continuous tails, Koponen [26] considered a TLF in which the cut
function is a decreasing exponential characterized by a separate parameter /. The
characteristic function of this distribution, when a # 1, can be expressed as:

(P+1/2)?
Pq) = exp cy — 6 ee costa arctan(llql))(1 + il|q|B tan(q arctan /|q|))
cos(za/2)

with c, a scale factor:

_ _ 2xcos(xa/2)
a= al (a) sin(za)

and

re 2a a

~ cos(ra/2)‘! ~ al (a) sin(ra)

In the case of symmetric distributions B = 0, the variance can be calculated using the
characteristic function:

5 & -@) »
)=- #4) _  2An(1 —a@) 94
dq?

~ “T(@) sin(za)
g=0

All the following discussion is in this symmetric case (B = 0).

If we use time steps At, and T = NAt, following the discussion in the previous
section, at the end of each interval, we must calculate the sum of N stochastic
variables that are independent and identically distributed. Therefore, the
characteristic function of the sum is:

Ng +1/P ye?

\N)= N N-
(gq, N) = pq) = exp {6 “1 cos(za/2)

cos(a urtandlad)

The model can be improved by standardizing it. If the variance is given by:
\n\n=== PAGE 420 ===\n(12.12)
we have that
Therefore, a standardized model is:
Next we state a well-known theorem applicable in our case.
Theorem 12.2 Let 
 be the characteristic function (Fourier transform) of p(x).
Let 
, where fa(k) is analytic and fs(k) ~ −a|k|α as k → 0 with
 and finite. Then
where
Going back to the TLF distribution, for small q,
Therefore, using the above theorem, 
.
We can also justify this from the viewpoint of the cutoff function used in [26] given
by:
\n\n=== OCR PAGE 420 ===\nc= 2
dq? 0
we have that
Pyq/e)| __ 1%e@| _,
2 ~~ 62 2 ~
oq 4-0 o- og 0

Therefore, a standardized model is:

2 4 1 /Py/2 .
Ings(q) =In@g (2) =Co- er cos («arctan (#)) (12.12)

cos(za/2) o

a/2
2nAl*t ( (“) ) ( (*))
= ———_|1 - —) +1 cos{ a@arctan | — .
al (a) sin(za) o o

Next we state a well-known theorem applicable in our case.
Theorem 12.2 Let (kK) be the characteristic function (Fourier transform) of p(x).
Let log Atk) = f.(K) + (4), where f,(k) is analytic and f,(k) ~ -a|k|* as k > o with
a € Rand finite. Then

P(x) ~ lee
where

Az asin(az/2)I(a)a
= - .

Going back to the TLF distribution, for small q,

2nAl“t aft\\ (fal
log gs(q) ~ a (a) sincera) , - (: + 3 (¢) ) os (« arctan (“)) :

1
Therefore, using the above theorem, PO) bP,

We can also justify this from the viewpoint of the cutoff function used in [26] given

by:
\n\n=== PAGE 421 ===\nWe are considering
Here L(x) is the Lévy distribution.
The decay is faster than the heavy tail decay because of the presence of the
exponential function in f(x). However, the main advantage of using this form of
density p(x) is that we can keep the heavy tail of the Lévy for arbitrarily large cutoff
values (by adjusting the parameter l) and then have a sharp decay (faster than heavy
tail). In other words, we are keeping the features of the heavy tail of the Lévy
distribution as well as the finite variance feature given by exponential decay and
truncation.
To simulate the standardized truncated Lévy model, a Matlab module (available
from the first author upon request) was developed. The parameter l is fixed at 1 and
then the parameter A and the characteristic exponent α are adjusted simultaneously
to fit the cumulative function. On the same grid, the cumulative distribution of the
simulated data is plotted for different time lags T in order to visually evaluate the
goodness of fit. Time lag T = 1 means that the fit is done by using consecutive data
points, while for a general lag T = k, the fit was generated by using points k
observations apart.
12.4.2.1 Kurtosis
In general, 
 moment of distribution, if it is finite, can be computed from
characteristic function φ(q) in a similar way we would compute 
 moment from the
moment generating function. The 
 moment computed from characteristic function
φ will be
For all parameters of α except α = 2 of the Lévy stable distribution, we have infinite
fourth moment. For a truncated Lévy distribution, we can compute the 
\n\n=== OCR PAGE 421 ===\nAe PV x|--@ oy 0

f(a) = Ae Vx * x > 0,
We are considering
F(x) x>l
cL(x) -l<x<l
P(x) =

F(x) x<-—l.

Here L(x) is the Lévy distribution.

The decay is faster than the heavy tail decay because of the presence of the
exponential function in f(x). However, the main advantage of using this form of
density p(x) is that we can keep the heavy tail of the Lévy for arbitrarily large cutoff
values (by adjusting the parameter /) and then have a sharp decay (faster than heavy
tail). In other words, we are keeping the features of the heavy tail of the Lévy
distribution as well as the finite variance feature given by exponential decay and
truncation.

To simulate the standardized truncated Lévy model, a Matlab module (available
from the first author upon request) was developed. The parameter / is fixed at 1 and
then the parameter A and the characteristic exponent a are adjusted simultaneously
to fit the cumulative function. On the same grid, the cumulative distribution of the
simulated data is plotted for different time lags T in order to visually evaluate the
goodness of fit. Time lag T = 1 means that the fit is done by using consecutive data
points, while for a general lag T = k, the fit was generated by using points k
observations apart.

12.4.2.1 Kurtosis

In general, n'" moment of distribution, if it is finite, can be computed from
characteristic function @(q) in a similar way we would compute n' moment from the
moment generating function. The n'" moment computed from characteristic function
@ will be

ny _ 1
My = EX") = .
uw 0g" \q=0

For all parameters of a except a = 2 of the Lévy stable distribution, we have infinite
fourth moment. For a truncated Lévy distribution, we can compute the fourth
\n\n=== PAGE 422 ===\n(12.13)
moment of (12.12). A lengthy but straightforward computation gives us
Since (12.12) is normalized, the kurtosis will be [20]:
Kurtosis is useful as a measure of the degree to which there is “excess” probability
density in comparison to a normal distribution. Specifically, a leptokurtic (
) distribution is one where extreme values have a higher probability than
predicted from a Gaussian model. The distribution of extinction magnitudes is
leptokurtic, and we propose that a truncated Lévy distribution may provide a better
fit than a normal.
12.4.2.2 Infinite divisibility
We say that random variable Y is infinitely divisible if it can be written as sum or n
independent copies of some random variable X. To see whether TLS is infinitely
divisible, we introduce the following theorem.
Theorem 12.3 Let Xi be independent and identically distributed random variables
with characteristic function φX. Then random variable Y given by
has characteristic function φY given by
From the form of (12.12), we propose a characteristic function showing that TLF is
infinitely divisible
Similarly, we can see that the random variable given by the equation above is again
TLF. For sum of random variables given by φS, N and using above theorem, we get:
\n\n=== OCR PAGE 422 ===\nmoment of (12.12). A lengthy but straightforward computation gives us

acol*(a — 1)(3acy — Sa — 3a7cy + a? + 6)

My =
4 o

Since (12.12) is normalized, the kurtosis will be [20]:

Kurt[X] = y2 = #4 —3 (12.13),
acol*(a — 1)(3acy — Sa — 3a7cy + a? + 6) 3
- 5 -3

Kurtosis is useful as a measure of the degree to which there is “excess” probability
density in comparison to a normal distribution. Specifically, a leptokurtic (

Kurt[X] > 0) distribution is one where extreme values have a higher probability than
predicted from a Gaussian model. The distribution of extinction magnitudes is
leptokurtic, and we propose that a truncated Lévy distribution may provide a better
fit than a normal.

12.4.2.2 Infinite divisibility

We say that random variable Y is infinitely divisible if it can be written as sum or n
independent copies of some random variable X. To see whether TLS is infinitely
divisible, we introduce the following theorem.

Theorem 12.3 Let X; be independent and identically distributed random variables
with characteristic function py. Then random variable Y given by

n
y=)Xx,
i=l
has characteristic function @y given by
, yn
Py = Tl = (ox) .
i=1

From the form of (12.12), we propose a characteristic function showing that TLF is
infinitely divisible

5 a/2
2nAl“t ql\~ gl
log gy = ——>——— 1 - | {[-— ]} +1 t = :
O08 Ps.n Nala) sin(xa) ((: ) ) cos (« arctan ( ;

Similarly, we can see that the random variable given by the equation above is again
TLF. For sum of random variables given by @s x and using above theorem, we get:
\n\n=== PAGE 423 ===\n(12.14)
(12.15)
(12.16)
This means that TLF is infinitely divisible, giving us (12.12).
12.4.3 DATA ANALYSIS WITH TLF DISTRIBUTION
In this subsection, we present an analysis of temporal change in diversity from [1].
Lévy stable distribution assumes data that are equally spaced in time, that is not the
case in the data set [2]. To adjust for heterogeneous time intervals (a necessary
consequence of differences in the length of geological epochs and stages at which
generic diversity was estimated), we need to make the following assumption.
Assumption 12.1 Diversity can be linearly interpolated with only a small error.
This assumption is not unrealistic, as in Figure 12.8, one can see the interpolation of
diversity (blue line) compared with the original data (red dots). It can be seen
throughout that the interpolation line does not deviate from the data and thus
Assumption 12.1 is supported by the data.
Now we can proceed to analyze diversity time series D(ti), which is defined for any
time 
 due to
interpolation. Diversity is assumed to behave exponentially, like any other
population dynamics model. We will analyze distribution of logarithm of changes in
diversity D for specific time lag T
where D(i) is diversity at time ti and D(i + T) is diversity at time ti + T, where T is time
lag. For our purposes, we will standardize (12.14) to
where term 
 will have mean zero and variance equal to 1.
12.4.4 SUM OF LéVY RANDOM VARIABLES WITH DIFFERENT
PARAMETERS
We present a result for the Lévy distribution that is analogous to the one found in
the previous sections for convolutions of exponential distributions. Let us assume
random variables X1, …, Xn with characteristic function (12.12) and coefficients β =
0, μ = 0, γk = γ. The characteristic function for Xk is
We will use a transformation of parameters αk
\n\n=== OCR PAGE 423 ===\nNy
Ps= (sy)
This means that TLF is infinitely divisible, giving us (12.12).

12.4.3 DATA ANALYSIS WITH TLF DISTRIBUTION

In this subsection, we present an analysis of temporal change in diversity from [1].
Lévy stable distribution assumes data that are equally spaced in time, that is not the
case in the data set [2]. To adjust for heterogeneous time intervals (a necessary
consequence of differences in the length of geological epochs and stages at which
generic diversity was estimated), we need to make the following assumption.

Assumption 12.1 Diversity can be linearly interpolated with only a small error.

This assumption is not unrealistic, as in Figure 12.8, one can see the interpolation of
diversity (blue line) compared with the original data (red dots). It can be seen
throughout that the interpolation line does not deviate from the data and thus
Assumption 12.1 is supported by the data.

Now we can proceed to analyze diversity time series D(t;), which is defined for any
time fj © {fo = fins 4 = f9 + Atty = th + 2AL,... ty = tax = fo + MA} due to
interpolation. Diversity is assumed to behave exponentially, like any other
population dynamics model. We will analyze distribution of logarithm of changes in
diversity D for specific time lag T

Dii+T) (12.14),

G,(i) = log Da
L

where D(i) is diversity at time t; and D(i + T) is diversity at time t; , 7, where Tis time

lag. For our purposes, we will standardize (12.14) to

G, — E(Gy)
o(Gr)

GS; = (42.15)

G,-EG;)
where term «G;) will have mean zero and variance equal to 1.

12.4.4 SUM OF LéVY RANDOM VARIABLES WITH DIFFERENT
PARAMETERS

We present a result for the Lévy distribution that is analogous to the one found in
the previous sections for convolutions of exponential distributions. Let us assume
random variables X,, ..., X, with characteristic function (12.12) and coefficients B =

O, = 0, yz = y. The characteristic function for X; is

oxg =e". (12.16),

We will use a transformation of parameters a,
\n\n=== PAGE 424 ===\n(12.17)
(12.18)
(12.19)
(12.20)
where Δαk is a small perturbation from the mean value . If we substitute αk into
(12.16), we get
Function (12.18) is holomorphic (it is also entire) in Δαi and has a power series
expansion in terms of Δαi at zero, that is,
Again, the sum of random variables Xk will have the characteristic function given by
a product of characteristic functions
Substituting (12.19) into the equation above, we get
multiplying this expression gives us
and using that ∑n
j = 0Δαk = 0 from (12.17), we finally get
\n\n=== OCR PAGE 424 ===\na, =&+ Ag,k € {1,...,n} (12.17),

where Aq, is a small perturbation from the mean value @. If we substitute a, into

(12.16), we get
ox, = en lraltate (42.18)

Function (12.18) is holomorphic (it is also entire) in Aa; and has a power series
expansion in terms of Aa, at zero, that is,

Gx, = eal” 4 e-lral” (—[yq]®) In(\yql) « Aa, + O(Aa?) (12.19),

Again, the sum of random variables X;, will have the characteristic function given by
a product of characteristic functions

n

k=1

Substituting (12.19) into the equation above, we get
yx, = (erat + ell (\yql®) In(lyql) “Aa, + O(a?)
(erat + e7l¥4l (yg) In(lyql) “Aa, + O(a?)
(erat + ell (Jyql) In(lyql) - Aa; + O(Aa?)

(en + eal" (—|yq|*) In(\yq]) - Aay + O(Aaz)

a Ww Wa a a

(erat + Wal" (—|yql%) In(lyql) - Aa, + O(Aa?)

multiplying this expression gives us

n

Osx, = ena + emlral"(—[y ql") In(lyql) - Y) Aa, + O(Aa?)
k=1

(12.20)

and using that 5”, - pAq;, = 0 from (12.17), we finally get
\n\n=== PAGE 425 ===\n(12.21)
which is almost a characteristic function for Lévy stable distribution with a scale
factor Nγ.
12.5 Analysis of the Stock Indices, high-frequency
(tick) data, and explosive series
We now present applications to the three different sets of data: simulated data from
a population model (Figure 12.11), the stock prices comprising the Dow Jones
Industrial Average (DJIA) Index, along with the index itself (Figures 12.12, 12.13,
and 12.14), and to the study of HFD from several influential companies (Figures
12.15, 12.16, 12.17, 12.18, and 12.19).
The analyzed stochastic variable is the return rt, defined as the difference of the
logarithm of two consecutive stock (or index) prices. In this case, we plot on the
same grid the cumulative distribution of the observed returns for different time lags
T in order to visualize how good the fitting is. Time lag T =1 means that the returns
are calculated by using two consecutive observations; for a general T, the returns are
calculated by using 
. Now, Xt = It, where It denotes the stock (or
index) price at time t, and T is the difference (in labor days) between two values of
the stock or index. We study the behavior of stock prices comprising the DJIA Index,
along with the index itself. The values are from 1985 to 2010. We finally analyzed
high-frequency (minute) data from 2008. In this case, T is the difference in minutes
between two values of the stock.
We conclude that the Lévy flights are appropriate for modeling the three different
sets of data. We recall that a value close to 2.0 indicates Gaussian behavior of the
stochastic variable.
Finally, volatility analysis was done on 
 index, Bank of America Corp (BAC),
JP Morgan Chase (JPM), International Business Machines Corp (IBM), and Wal-
Mart Stores (WMT) HFD, earthquake series (EQ2), and explosives series (EXP). The
results are shown in Tables 12.2, 12.3, 12.4, 12.5, 12.6, 12.7, and 12.8 and Figures
12.20, 12.21, 12.22, 12.23, 12.24, 12.25, 12.26, 12.27, 12.28, and 12.29. The studied
HFD corresponding to the collapse of the Bear Stearns in March 2008 consists of the
week (five trading days) March 10–14, 2008, before the merging announcement over
the weekend as well as the two following trading days March 17 and 18. On Friday,
March 14, 2008, at about 9:14 a.m., JPM and the Federal Reserves Bank of New
York announced an emergency loan to Bear Stearns (of about 29 billion, terms
undisclosed) to prevent the firm from becoming insolvent. This bailout was declared
to prevent the very likely crash of the market as a result of the fall of one of the
biggest investment banks at the time. This measure proved to be insufficient to keep
the firm alive and 2 days later, on Sunday, March 16, 2008, Bear Stearns signed a
merger agreement with JPM essentially selling the company for $2 a share (price
revised on March 24 to $10/share). The same stock traded at $172 in January 2007
and $93 a share in February 2007. Today, this collapse is viewed as the first sign of
\n\n=== OCR PAGE 425 ===\n®yx, = enna” 4 O(Aa?) (12.21)

which is almost a characteristic function for Lévy stable distribution with a scale
factor Ny.

12.5 Analysis of the Stock Indices, high-frequency
(tick) data, and explosive series

We now present applications to the three different sets of data: simulated data from
a population model (Figure 12.11), the stock prices comprising the Dow Jones
Industrial Average (DJIA) Index, along with the index itself (Figures 12.12, 12.13,
and 12.14), and to the study of HFD from several influential companies (Figures
12.15, 12.16, 12.17, 12.18, and 12.19).

The analyzed stochastic variable is the return r;, defined as the difference of the
logarithm of two consecutive stock (or index) prices. In this case, we plot on the
same grid the cumulative distribution of the observed returns for different time lags
T in order to visualize how good the fitting is. Time lag T =1 means that the returns
are calculated by using two consecutive observations; for a general T, the returns are
calculated by using " = log(Xt/Xt — T), Now, X; = I,, where I, denotes the stock (or
index) price at time t, and T is the difference (in labor days) between two values of
the stock or index. We study the behavior of stock prices comprising the DJIA Index,
along with the index itself. The values are from 1985 to 2010. We finally analyzed
high-frequency (minute) data from 2008. In this case, T is the difference in minutes
between two values of the stock.

We conclude that the Lévy flights are appropriate for modeling the three different
sets of data. We recall that a value close to 2.0 indicates Gaussian behavior of the
stochastic variable.

Finally, volatility analysis was done on S&P500 index, Bank of America Corp (BAC),
JP Morgan Chase (JPM), International Business Machines Corp (IBM), and Wal-
Mart Stores (WMT) HFD, earthquake series (EQ2), and explosives series (EXP). The
results are shown in Tables 12.2, 12.3, 12.4, 12.5, 12.6, 12.7, and 12.8 and Figures
12.20, 12.21, 12.22, 12.23, 12.24, 12.25, 12.26, 12.27, 12.28, and 12.29. The studied
HFD corresponding to the collapse of the Bear Stearns in March 2008 consists of the
week (five trading days) March 10-14, 2008, before the merging announcement over
the weekend as well as the two following trading days March 17 and 18. On Friday,
March 14, 2008, at about 9:14 a.m., JPM and the Federal Reserves Bank of New
York announced an emergency loan to Bear Stearns (of about 29 billion, terms
undisclosed) to prevent the firm from becoming insolvent. This bailout was declared
to prevent the very likely crash of the market as a result of the fall of one of the
biggest investment banks at the time. This measure proved to be insufficient to keep
the firm alive and 2 days later, on Sunday, March 16, 2008, Bear Stearns signed a
merger agreement with JPM essentially selling the company for $2 a share (price
revised on March 24 to $10/share). The same stock traded at $172 in January 2007
and $93 a share in February 2007. Today, this collapse is viewed as the first sign of

\n\n=== PAGE 426 ===\nthe risk management meltdown of investment bank industry in September 2008 and
the subsequent global financial crisis and recession.
The behavior of stocks representing the financial institutions that were affected by
the crisis such as JPM and BAC was studied. We also looked at other institutions
such as IBM and WMT that should not have been much affected. All the stocks were
sampled with the period T = 1 min. During the days considered, the BAC and JPM
return fluctuated more than 10% while that of WMT and IBM below 1%.
The data on earthquakes magnitude was obtained from the US Geological Survey
(USGS) from 1 January 1, 1973, to November 9, 2010. The downloaded data contains
information about the date, longitude, latitude, and the magnitude of each recorded
earthquake. The location of the major earthquake and its distribution defines the
studied area. The earthquake magnitude is the data used in the analysis. The sample
data is from July 24, 2010, to November 9, 2010, containing 3000 observations with
a maximum (7.1), which occurred on August 12, 2011.
We then continue our analyses with the study of seismic traces of a mining
explosion. A seismic trace is a plot of the earth’s motion over time. The data
presented are measurements of the earth’s vertical displacement where the
recording frequency is 40 per second. The data sets are from a recording station in
Scandinavia and are reported by Shumway and Stoffer [27].
12.6 Results and discussion
In this chapter, we performed different statistical analyses to evaluate the fit of
temporal generic diversity and extinction data to different model distributions. From
the fact that largest p-values in an AD test are associated with the distributions with
the best fit, it was concluded that the best fit to the distribution of extinction
magnitudes is the maximum extreme value distribution. The second best fit comes
from the Gamma distribution, as one would expect from the approximation of
extinction waiting times by an exponential.
In Figure 12.9, there are two solid lines representing cumulative distribution of
diversity summed over time plotted against the empirical data. The y-axis represents
cumulative distribution function and the x-axis is the absolute value of standardized
changes in diversity. The dotted line represents the empirical distribution function
of GST (see (12.15)); the absolute value of standardized changes in diversity. The
solid green line is the cumulative distribution function corresponding to a standard
normal random variable (zero mean, variance equal to 1). The red line represents the
best fit for absolute value of random variable defined by the characteristic function
(12.12). In other words, the best fit is provided by a standardized TLF.
\n\n=== PAGE 427 ===\n\n\n=== OCR PAGE 427 ===\nCumulative distribution function F(x)

o 1 2 3 4 § 6 7 8 9
GS, - Absolute value of standardized changes in diversity for time lag T=1

Py

o 1 2 3 4 5 6 7 8 9
GS, - Absolute valve of standardized changes in diversity for time lag T = 2

°6 1 2 3 4 &§ 6 7 6 9 0
(5, Abele ve of standardzed changes in erty for ie lag T = 4
\n\n=== PAGE 428 ===\nFIGURE 12.9 Data fitted with distribution (12.12), empirical distribution for GST,
and standard normal distribution. Time lag T = 1, 2, 4, 8, and 16.
\n\n=== OCR PAGE 428 ===\nCumulative distribution function F(x)

0 1 2 3 4 5 6 r 8 9 10

GS - Absolute value of standardized changes in diversity for time lag T = 8

1
09
0.8 >
07
0.6 >
05
0.4}
03
0.2}
O41

0

Cumulative distribution

0 1 2 3 4 5 6 7 8 9 10
GS}¢ - Absolute value of standardized changes in diversity for time lag T = 16

FIGURE 12.9 Data fitted with distribution (12.12), empirical distribution for GS‘,
and standard normal distribution. Time lag T = 1, 2, 4, 8, and 16.
\n\n=== PAGE 429 ===\nFIGURE 12.10 Distribution of log change in diversity for time lag T plotted against
various distributions. (a) T=1; and (b) T=4.
\n\n=== OCR PAGE 429 ===\nProbability plot for GS-T1
feta ea ots
Lo bed toga
2 fpr\ Ve
} ial } al Loge
LZ A
a fen
ceo ee

:
N

a Py) 3
(G5-T1- Threshold ost
[Aner Johnson trantarmaton
ey T=4
Probability plot for GSiag4
Smallest exvere vale 98% Cl Largest extreme va - 98% Ct
mo me
90]
0] | st
} 0 } i
4 3 +0]
10
oy or
a ee} er ae
staat staat
Gamma - 96% 1 ‘Parameter gamma - 05% Cl
| 0]
} ro fe
Aig Né
Groves om or 7 «Ch or WO
Gstep¢ G5iagt «Threshold

FIGURE 12.10 Distribution of log change in diversity for time lag T plotted against
various distributions. (a) T=1; and (b) T=4.
\n\n=== PAGE 430 ===\nFIGURE 12.11 The figure shows the estimates of the Lévy Flight parameter for the
solution of the SDE.
FIGURE 12.12 The figure shows the estimates of the Lévy Flight parameter for JP
Morgan.
\n\n=== OCR PAGE 430 ===\nSDE

Cumulative distribution
a

10°
10"

Normalized data (T = 8, «= 1.80)

Normalized data (T = 16, «= 1.99)

10! SDE

s 10°
Es
210
3 ge
2 10%
3 10>
5

o 104
10

10° 10" 107 10° 10'

Normalized data (T= 1, «= 1.99) Normalized data (T= 4, «= 1.99)
SDE SDE

10" 10"
§ 10° § 10°
210 Z 10"
3 ioe 3 ioe
10“ 10“
§ 10° \ 3 10°
8 10 8 10“
106

10° 10" 10" 10° 10!

FIGURE 12.11 The figure shows the estimates of the Lévy Flight parameter for the

solution of the SDE.
JP Morgan JP Morgan

10"
§ 10° §
3 3
2 10" 2
3 ioe 3
Z H
310° 3
é i
6 10+ 3

10% 1

107 10° 10! 107 10° 10°
Normalized returns (7 = 1, = 1.68) Normalized retums (T = 4, «= 1.67)
JP Morgan JP Morgan

10" 10°
§ 10° § 10°
2 10" 2 107°
3 3 gz
3 102 8 10+
§ 10° 3 10°
5 E
© 10% © 10%

108 10°

10" 10° 1077 10° 10°

Normalized returns (T= 8, = 1.40)

Normalized returns (T= 16, = 1.27)

FIGURE 12.12 The figure shows the estimates of the Lévy Flight parameter for JP

Morgan.

\n\n=== PAGE 431 ===\nFIGURE 12.13 The figure shows the estimates of the Lévy Flight parameter for The
Walt Disney Company.
FIGURE 12.14 The figure shows the estimates of the Lévy Flight parameter for the
DJIA index.
\n\n=== OCR PAGE 431 ===\nS

aes

§ 10° §
20 2
3 ioe 3 ig?
10% 10
g 2
5 10? 5 10
5 10 5 10“
105 10°
10 10° 10° 107 10° 10!
Normalized returns (T= 1, a= 1.71) Normalized returns (T = 4, c= 1.70)
Dis bis
10! 10°
§& 10° & 10°
: 10°" ‘ 107
102 10?
in md fie >
1 10
= = \
8 10 3 10+ ,
os wawer aerecee os
" 10" 10° 10" " 107 10° 10°
Normalized returns (T= 8, a= 1.37) Normalized retums (T= 16, = 1.27)
FIGURE 12.13 The figure shows the estimates of the Lévy Flight parameter for The
Walt Disney Company.
DuIA UIA
10° 40°
§ 10° § 10°
210 2 0
& &
3 10? 3 10?
3 107 3 10 ™
10% 105
107 10° 10° 107 10° 10°
Normalized returns (T= 1, = 1.60) Normalized returns (T= 4, «= 1.60)
DUA DuIA
= &
3 3
g 4
& a
@ 2
3 3
10° were 105 pean
10" 10° 10! 10" 10° 10!
Normalized returns (T = 8, a= 1.50) Normalized returns (T= 16, a= 1.33)

FIGURE 12.14 The figure shows the estimates of the Lévy Flight parameter for the
DJIA index.
\n\n=== PAGE 432 ===\nFIGURE 12.15 The figure shows the estimates of the Lévy Flight parameter for
IBM. These are high-frequency (tick) data.
\n\n=== OCR PAGE 432 ===\nCumulative distribution

‘Cumulative distribution

Cumulative distribution

‘Cumulative distribution

IBM

10° §
3
s
3
10? °°
> 2
e § 3
g
104 \ oO
107 10° 10° 107 10° 10°
Normalized returns (T= 1, «= 1.0) Normalized returns (T= 4, «= 1.40)
IgM IBM

10° g 10°
$
3B
=

10° 3 102
:

104 S 104

107 10° 10" 10" 10° 10"

Normalized retums (T= 8, «= 1.30)

The figure shows the estimates of the Lévy Flight parameter for
IBM. These are high-frequency (tick) data.

Normalized retums (T = 16, a= 1.20)

Google Google
10° ] § 0
3
2
a
10° 4D yoe
g
. 3 .
é
104 1 8 40
107 10° 10! 107 10° 10"
Normalized returns (T = 1, o.= 1.50) Normalized retums (T= 4, «= 1.40)
Google Google
10° 1 F 10°
=
107 et 3 10°]
Sy 3
3
. —
5
10 4 So 104}
107 10° 10" 107 10° 10!

Normalized returns (T = 8, «= 1.30)

Normalized retums (T = 16, a= 1.20)

\n\n=== PAGE 433 ===\nFIGURE 12.16 The figure shows the estimates of the Lévy Flight parameter for
Google. These are high-frequency (tick) data.
FIGURE 12.17 The figure shows the estimating of the Levy Flight parameter for
Walmart. These are high frequency (tick) data.
\n\n=== OCR PAGE 433 ===\nFIGURE 12.16 The figure shows the estimates of the Lévy Flight parameter for

Google. These are high-frequency (tick) data.

WMT WMT

e
g 10! § 10°
s 3
2 2
3 3
aoe 3 ge
3 10 ~ > g 10
5 E ‘3
8 104 ° 104

1071 10° 10° 107 10° 10"

Normalized returns (T= 1, = 1.50) Normalized returns (T = 4, a= 1.30)
WMT WMT

ig 10° s 10°
g 3
z 2
2
3 3
3 10? 3 102 4
Z 5 z
3 10 8 104

107 10° 10" 10" 10° 10"

Normalized returns (T = 8, «= 1.20)

Normalized retums (T = 16, a= 1.20)

FIGURE 12.17 The figure shows the estimating of the Levy Flight parameter for
Walmart. These are high frequency (tick) data.

\n\n=== PAGE 434 ===\nFIGURE 12.18 The figure shows the estimates of the Lévy flight parameter for The
Walt Disney Company. These are high-frequency (tick) data.
\n\n=== OCR PAGE 434 ===\nCumulative distribution

‘Cumulative distribution

FIGURE 12.18 The figure shows the estimates of the Lévy flight parameter for The

ois ois
10° § 10°
3
3
102 — 2 102
hie 3
10 8 104 1
107 10° 10! 107 10° 10!
Normalized retums (T= 1, c= 1.40) Normalized retums (T= 4, «= 1.30)
ois ois
10° i 10°
102 3 10° 4
10+ 8 10+ 4

10°
Normalized retums (T= 8, «= 1.20)

107 10!

10°
Normalized retus (T = 16, a= 1.20)

4

10!

Walt Disney Company. These are high-frequency (tick) data.

Cumulative distribution

Cumulative distribution

INTC INTC
10° 7 i 10° 7
&
3
10? 1 107
g SA |
10 \ 7 3 10
10" 10° 10! 10" 10° 10!
Normalized returns (T = 1, a= 1.60) Normalized returns (T= 4, a = 1.30)
INTC INTC
10° J g§ 10°
Ss
2
3
102 | A 107 |
3
10 7 © 10%
107 10° 10° 10" 10° 10!

Normalized returns (T = 8, a = 1.20)

Normalized returns (T = 16, «= 1.20)
\n\n=== PAGE 435 ===\nFIGURE 12.19 The figure shows the estimates of the Lévy flight parameter for Intel
Corporation. These are high-frequency (tick) data.
FIGURE 12.20 S&P 500 return and squared and absolute returns ACF and PACF.
FIGURE 12.21 S&P 500 volatility model diagnostics: On top are the ACF of the
standardized and standardized squared residual and on bottom are the distribution
and GED Q–Q plot for the fitted model.
\n\n=== OCR PAGE 435 ===\nFIGURE 12.19 The figure shows the estimates of the Lévy flight parameter for Intel
Corporation. These are high-frequency (tick) data.

0.05

oa
os.

4

$02
01
00.

ACF of observations PACF of observations
0.05.
0.00.
§
2 -005
-o.104
1 3.5 7 9 11 1915 17 1921 29 25 O7 29 91 38 1 3.5 7 9 11 19 15 17 19 21 29 25 oF 20 31 98
Lag Lag
ACF of squared observations ACF of absolute observations
oa
os
4%
$ o2
on
oot
1 35 7 9 11 19 15 17 19 21 29 25 27 20 91 99 1 35 7 8 11 19 15 17 19 21 29 25 27 20 91 99
Lag Lag

FIGURE 12.20 S&P 500 return and squared and absolute returns ACF and PACF.

0.04

0.00

ACF

0.04

ACF standardized residuals

ACF standardized squared residuals

~006 4

1 3 5 7 9 11 13 15 17 19 21 23 25 27 2 31 33
Lag

Emperical density of standardized residuals

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Lag

Ged Q-@ plot

bhrhowe

‘Sample quantities

Values

r r

boa!

‘Theoretical quantities

FIGURE 12.21 S&P 500 volatility model diagnostics: On top are the ACF of the
standardized and standardized squared residual and on bottom are the distribution
and GED Q-Q plot for the fitted model.
\n\n=== PAGE 436 ===\nFIGURE 12.22 BAC high-frequency returns and squared and absolute returns ACF
and PACF.
FIGURE 12.23 BAC conditional volatility model diagnostics: On top are the ACF of
the standardized and standardized squared residual, and on bottom are the
distribution and GED Q–Q plot for the fitted model.
\n\n=== OCR PAGE 436 ===\nACF of observations PACF of observations
0.10 010
0.05 0.05
8 oon 2 000+
0.05 os
18.5 7 9 1 19 15 17 1921 28 25 OF 29 8D 1 3.5 7 8 11915 17 19 21 2005 7 298
Lag Lag
ACF squared of observations ACF absolute of observations
0.04 0.20
§ 000 4 § o10
0.00 +
-004
18.5 7 9 11 19 15 17 1021 23 25 27 20 31 38 1 8 5 7 8 11 19 15 17 19 21 23 25 27 29 31 99
lag Lag
FIGURE 12.22 BAC high-frequency returns and squared and absolute returns ACF
and PACF.
ACF of standardized residuals ACF of squared standardized residuals
0.04 0.04 ”
0.02 4 0.02
§ 000 Be ee
0.04 4 0.04
TISTONDET UAB aD AS 73.5 7 9 11915 17 19.21 00 25 oT DOO
Lag Lag
Empirical density of standardized residuals Ged Q-Q plot
0.6 > Wormal density 60 -
os | cnvot ae ey P
Fos : ee ‘
i 03 3 2
& o2 OE —]
on ol
8 T - L Tr T T .
-20 ° 20 “0 © 3 2 4 09 + 2 8
Values: ‘Theoretical quantiles:

FIGURE 12.23 BAC conditional volatility model diagnostics: On top are the ACF of
the standardized and standardized squared residual, and on bottom are the
distribution and GED Q—Q plot for the fitted model.
\n\n=== PAGE 437 ===\nFIGURE 12.24 JPM high-frequency returns and squared and absolute returns ACF
and PACF.
FIGURE 12.25 JPM conditional volatility model diagnostics: On top are the ACF of
the standardized and standardized squared residual, and on bottom are the
distribution and GED Q–Q plot for the fitted model.
\n\n=== OCR PAGE 437 ===\nACF of observations PACF of observations

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag Lag

ACF of squared observations ACF of absolute observations

0.04 4

13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag lag

FIGURE 12.24 JPM high-frequency returns and squared and absolute returns ACF
and PACF.

ACF of standardized residuals ACF of squared standardized residuals
0.04
0.04 02 4
$ a
0.00
~008 -008
TESsSTONDET UN Bora NS TOSTON DB 0m BN
lag Lag
Empirical density of standardized residuals Ged 2-2 plot
0.6 + ioral denaty sa be
5 | 2000 tre Snaty ¢
40
Fos4 =
is 4
as ——————
01 ie
oo L -20
-20 ° 20 40 60 os 2 4 0 1 @ 8
Values Theoretical quantiles:

FIGURE 12.25 JPM conditional volatility model diagnostics: On top are the ACF of
the standardized and standardized squared residual, and on bottom are the
distribution and GED Q—Q plot for the fitted model.
\n\n=== PAGE 438 ===\nFIGURE 12.26 IBM high-frequency returns and squared and absolute returns ACF
and PACF.
FIGURE 12.27 IBM conditional volatility Model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and
GED Q–Q plot for the fitted model.
\n\n=== OCR PAGE 438 ===\nACF of observations

PACF of observations

Lag

ACF of squared observations

13 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33

1.3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
Lag

ACF of absolute observations

20104 [ajenensenennesnahuerennaancedpremssscseneneleinveennenentlas ee Ce ae ee
1 3.5 7 9 1119 15 17 10 21 29.05 7 BOD 1 3 5 7 9 11 13 15 17 19 21 29 25 27 29 91 39
lag lag
FIGURE 12.26 IBM high-frequency returns and squared and absolute returns ACF
and PACF.
ACF of standardized residuals ACF of squared standardized residuals
0.08
2
§ 000 4 ob see SS
~008

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33

1 3 5 7 9 1113 18 17 19 21 23 25 27 29 31 33

Lag Lag
Empirical density of standardized residuals Ged Q-Q plot
0.8 {>a oreny of =
pee tr 23
Bos 5 10 :
Pe s Se ee |
oa 5 an
00 — —— - +
2-10 an er) 4a 8 ° 2 4 6
Values: Theoretical quantiles:

FIGURE 12.27 IBM conditional volatility Model diagnostics: Top are the ACF of
the standardized and standardized squared residual. Bottom are the distribution and

GED Q-Q plot for the fitted model.
\n\n=== PAGE 439 ===\nFIGURE 12.28 WMT high-frequency returns and squared and absolute returns
ACF and PACF.
FIGURE 12.29 WMT high-frequency conditional volatility model diagnostics.
Note that for small time lag T (consequent changes in diversity) the data are far from
standard normal distribution and follow the truncated Lévy flights closely. For larger
values of time lag T, the data are almost normally distributed, probably because
interpolation over longer time periods “smooths out” shorter-term fluctuations in
magnitude that lead to deviations from normality. Curiously, this is opposite to the
behavior of stock market returns, where for large time lag we have Lévy stable
distribution and with smaller time lag one obtains a close fit to a normal distribution
[28, 29].
\n\n=== OCR PAGE 439 ===\nACF of observations PACF of observations
0.04 saad
0.02 one
% 4
§ 000 3 000
0.04 004
1 3 5 7 9 11 13 15 17 19 21 29 25 27 29 31 33 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
lag lag
ACF of squared observations ACF of absolute observations
0.04 om
a 015
rs 4 0.10
2 9.00 Q
0.05
0.00
0.04
1 3 5 7 8 11 19 15 17 19 21 23 25 27 29 31 33 1 3 5 7 9 11 13:15 17 19 21 23 25 27 29 31 33
lag lag

FIGURE 12.28 WMT high-frequency returns and squared and absolute returns

ACF and PACF.
ACF of standardized residuals ACF of squared standardized residuals
010 0.04
02
4, 005
§ Bo | ee ee
0.00
-008
1 3 5 7 9 11 1915 17 1021 29 26 27 29 31 39 13 5 7 9 11 1915 17 19 21 23 25 7 2031 0D
Lag Lag
Empirical density of standardized residuals Ged Q-Q plot
0.6 {= Norma doraty if
2 Sedor tees 20
os | 00 mod nny ;
2o4 5 20
gos & 104 :
€ 02 i :
01
00 ide
-10 ° 10 20 30 “4 2 ° 2 4

Values ‘Theoretical quantiles

FIGURE 12.29 WMT high-frequency conditional volatility model diagnostics.

Note that for small time lag T (consequent changes in diversity) the data are far from
standard normal distribution and follow the truncated Lévy flights closely. For larger
values of time lag T, the data are almost normally distributed, probably because
interpolation over longer time periods “smooths out” shorter-term fluctuations in
magnitude that lead to deviations from normality. Curiously, this is opposite to the
behavior of stock market returns, where for large time lag we have Lévy stable
distribution and with smaller time lag one obtains a close fit to a normal distribution
[28, 29].
\n\n=== PAGE 440 ===\nTable 12.1 shows the summary for Figures 12.9a, 12.9b, 12.9c, 12.9d, and 12.9e,
together with the Kolmogorov–Smirov (KS) test for hypothesis H0 : GST has
standardized TLF distribution. In the same table, we include the p-values for KS test
and computed the kurtosis for TLF.
Table 12.1 Results summary
Time
KS
p-
TLF fit Sample
Figure 12.9. lag, T α
statistics value Kurt.
Kurt.
a
1
1.646 0.0823
0.464
167
8.75
b
2
1.635 0.0698
0.68
176
13.51
c
4
1.745 0.0568
0.894
28.8
6.75
d
8
1.745 0.059
0.8767 14.8
8.59
e
16
2.0
0.1360
0.0664 0
4.229
Table 12.2 Augmented Dickey–Fuller (ADF) unit root test values. H0:I(1), critical
values: −2.567 (1%); −1.941 (5%); −1.616 (10%) for the indices, HFD, earthquake
series, and the explosive series.
Symbol DJIA
SP500 BAC
JPM
IBM
WMT
EQ2
EXP
Return
−12.321 −12.493 −14.75 −14.328 −14.752 −14.325 −11.352 −18.232
Sqd. Ret −4.7648 −4.833 −12.34 −13.001 −12.455 −12.456 −11.367 −5.628
Abs. Ret −4.8911 −4.845 −9.193 −9.6733 −10.032 −10.032 −11.352 −4.361
Table 12.3 Kwiatkowski, Phillips, Schmidt, and Shin (KPSS) unit root test values.
H0:I(0), critical values: 0.739 (1%); 0.463 (5%); 0.347 (10%) for the indices, HFD,
earthquake series, and the explosive series.
Symbol DJIA SP500 BAC JPM IBM WMT EQ2 EXP
Return
0.166 0.208
0.167 0.146 0.089 0.089 6.641 0.007
Sqd. Ret 1.549 1.724
0.108 0.099 0.075 0.075 6.850 0.867
Abs. Ret 2.544 2.968
0.979 1.271 0.598 0.598 6.641 1.233
Table 12.4 AutoRegressive Conditional Hectroscedastic test for correlation in
squared and absolute series. Critical values: 32.91 (0.1%); 26.22 (1%); 21.03 (5%) for
the indices, HFD, earthquake series, and the explosive series.
Symbol
DJIA
SP500
BAC
JPM IBM
WMT EQ2
EXP
ARCH LM 731.136
703.342
15.454 8.198 21.741 5.649 177.227 1299
Ljung. Box 2508.581 2539.780 18.621 8.838 23.145 5.938 364.196 4266
Pierce. Box 2498.418 2529.511 18.570 8.824 23.088 5.929 363.255 4251
\n\n=== PAGE 441 ===\nTable 12.5 Whittle estimate of the long-memory parameter (d) for the indices,
HFD, earthquake series, and the explosive series where d ∈ ( − 0.5, 0.5).
Symbol DJIA
SP500 BAC
JPM
IBM
WMT
EQ2
EXP
Return
−0.083* −0.085* 0.031
0.091** −0.019 0.005
0.136** 0.499**
Sqd. Ret 0.199** 0.202** 0.042
0.042
0.038
0.035
0.140** 0.419**
Abs. Ret 0.218** 0.215** 0.183** 0.204** 0.145** 0.159** 0.136** 0.385**
*significant at 5%; **significant at 1%.
Table 12.6 AIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum AIC is selected.
Symbol
DJIA
SP500 BAC
JPM
IBM
WMT
EQ2 EXP
GARCH(1,1)
−6.544 −6.417 −11.1720 −11.042 −12.632 −12.713 2.508 −3.473
ARFIMA+GAR
−6.545 −6.420 −11.1783 −11.051 −12.623 −12.694 2.484 −3.484
FIGARCH(0,d,0) −6.506 −6.375 −11.1750 −11.044 −12.624 −12.695 2.517 −3.472
FIGARCH(0,d,1) −6.541 −6.417 −11.1780 −11.043 −12.623 −12.694 2.518 −3.471
FIGARCH(1,d,1) −6.553 −6.429 −11.1781 −11.041 −12.622 −12.693 2.510 −3.657
IGARCH(1,1)
−6.544 −6.417 −11.1730 −11.043 −12.624 −12.695 2.507 −3.472
Table 12.7 BIC of models fitted to the indices, HFD, earthquake series, and the
explosive series. The model with minimum BIC is selected.
Symbol
DJIA
SP500 BAC
JPM
IBM
WMT
EQ2 EXP
GARCH(1,1)
−6.534 −6.407 −11.162 −11.031 −12.612 −12.682 2.517 −3.461
ARFIMA+GAR
−6.533 −6.407 −11.163 −11.038 −12.619 −12.701 2.495 −3.644
FIGARCH(0,d,0) −6.499 −6.367 −11.169 −11.035 −12.615 −12.686 2.523 −3.463
FIGARCH(0,d,1) −6.531 −6.406 −11.167 −11.032 −12.612 −12.682 2.526 −3.459
FIGARCH(1,d,1) −6.541 −6.416 −11.165 −11.032 −12.609 −12.679 2.521 −3.469
IGARCH(1,1)
−6.537 −6.409 −11.164 −11.034 −12.614 −12.685 2.514 −3.464
Table 12.8 Long-memory volatility models and their estimated long-memory
parameter for the indices, HFD, earthquake series, and the explosive series.
Symbol
DJIA
SP500
BAC
JPM
IBM
WMT EQ2
EXP
ARFIMA+GAR
−0.032* −0.045** 0.10** .236** .107** 0.152* 0.092** 0.861**
FIGARCH(0,d,0) 0.215** 0.214**
0.00
1.24
0.158 0.00
0.097
0.390**
FIGARCH(0,d,1) 0.307** 0.313**
0.00
−0.12 0.246 0.00
0.115
0.390**
FIGARCH(1,d,1) 0.602** 0.634**
0.00
0.00
0.443 0.00
0.331
0.782**
*significant at 5%; **significant at 1%.
We can see from Table 12.1 that our sample (GT) has kurtosis above zero and
modeling with normal distribution is not appropriate. The normal distribution can
\n\n=== PAGE 442 ===\nbe easily rejected by the KS test. Note that the kurtosis for the TLF is also function of
time lag T. For T → ∞, we have that γ2 is of 
. In Section 12.14, we presented a
study of the statistical behavior of data arising in population models, of a financial
index along with the rate of return of specific companies within the index, and of
HFD by using a standardized truncated Lévy flight model. In all the cases, we
obtained that the evolution of data can be described by the model. We can see that
all the values obtained for the exponent α are lower than 2.
In previous works (see, e.g., [30] and the references therein), it was found that the
exponents calculated for market indices were strictly greater than 2. Weron [30]
concluded that these values could be a consequence of working with finite samples.
This behavior was compatible with a slow convergence to a Gaussian distribution but
it was not possible to conclude that the Lévy distribution was the appropriate
stochastic process for explaining the financial indices evolution. The authors believe
that the standardized Lévy model that was used in this work, together with
computation of the constants involved in the model, allowed them to more
accurately complete a numerical analysis. This standardized Lévy model is suitable
for better working with finite samples, and it offers a new way for analyzing financial
indices, as well as other phenomena with similar behavior.
Figure 12.9 shows the log–log plot of the cumulative distribution of the normalized
return for four different values of time scale. The red line is the best fit of the Lévy
distribution. The green line indicates the Gaussian distribution. In the cumulative
distribution curve of each fund, there are some outlying points. Those outlying
points correspond to the significant drops in a very short period (1 or 2 days, or 1 or
2 min, depending on the data) that happened to the data. This is exactly the way in
which a market crash is defined: A market crash is an outlying point in the
cumulative probability distribution of the stochastic process described by the Lévy
model. It should be noted that outlying points exactly reflect the crash of the
corresponding financial indices/stocks.
Finally, the underlying volatility processes in earthquake series, high-frequency
financial data, financial indices, and explosive data were explored using various
GARCH models in this thesis. The GARCH models applied include basic GARCH,
IGARCH, ARFIMA (0,d,0)-GARCH, and FIGARCH specifications. The methodology
is not new; however, the major contribution of this work comes in the realm of
applications. The methodology was applied to three domains: geophysics
(earthquake data), finance (high-frequency financial data, and indices), and
explosives data. In all the applications, the methodology provides insight into
features of the series volatility.
The results show that the indices (DJIA and S&P 500) returns and the explosives
(EXP) series volatility had the highest persistence that were best described by using
FIGARCH, a long-memory model. This result is in line with the previous work of
Breidt et al. [4], Mariani et al. [31], and Mike So [32] for the indices volatility. In his
work, Mike So [32] applied the modified rescaled range test (R/S) proposed by Lo
(1991) and the semiparametric test (GPH) proposed by Geweke and Porker-Hudak
[33] to detect the existence of long-term dependence in volatility in the S & P 500
index and DJIA index. He used three proxies of the variability of returns to achieve
\n\n=== OCR PAGE 442 ===\nbe easily rejected by the KS test. Note that the kurtosis for the TLF is also function of
time lag T. For T— ©, we have that y, is of O(7 ~~). In Section 12.14, we presented a
study of the statistical behavior of data arising in population models, of a financial
index along with the rate of return of specific companies within the index, and of
HFD by using a standardized truncated Lévy flight model. In all the cases, we
obtained that the evolution of data can be described by the model. We can see that
all the values obtained for the exponent a are lower than 2.

In previous works (see, e.g., [30] and the references therein), it was found that the
exponents calculated for market indices were strictly greater than 2. Weron [30]
concluded that these values could be a consequence of working with finite samples.
This behavior was compatible with a slow convergence to a Gaussian distribution but
it was not possible to conclude that the Lévy distribution was the appropriate
stochastic process for explaining the financial indices evolution. The authors believe
that the standardized Lévy model that was used in this work, together with
computation of the constants involved in the model, allowed them to more
accurately complete a numerical analysis. This standardized Lévy model is suitable
for better working with finite samples, and it offers a new way for analyzing financial
indices, as well as other phenomena with similar behavior.

Figure 12.9 shows the log—log plot of the cumulative distribution of the normalized
return for four different values of time scale. The red line is the best fit of the Lévy
distribution. The green line indicates the Gaussian distribution. In the cumulative
distribution curve of each fund, there are some outlying points. Those outlying
points correspond to the significant drops in a very short period (1 or 2 days, or 1 or
2 min, depending on the data) that happened to the data. This is exactly the way in
which a market crash is defined: A market crash is an outlying point in the
cumulative probability distribution of the stochastic process described by the Lévy
model. It should be noted that outlying points exactly reflect the crash of the
corresponding financial indices/stocks.

Finally, the underlying volatility processes in earthquake series, high-frequency
financial data, financial indices, and explosive data were explored using various
GARCH models in this thesis. The GARCH models applied include basic GARCH,
IGARCH, ARFIMA (0,d,0)-GARCH, and FIGARCH specifications. The methodology
is not new; however, the major contribution of this work comes in the realm of
applications. The methodology was applied to three domains: geophysics
(earthquake data), finance (high-frequency financial data, and indices), and
explosives data. In all the applications, the methodology provides insight into
features of the series volatility.

The results show that the indices (DJIA and S&P 500) returns and the explosives
(EXP) series volatility had the highest persistence that were best described by using
FIGARCH, a long-memory model. This result is in line with the previous work of
Breidt et al. [4], Mariani et al. [31], and Mike So [32] for the indices volatility. In his
work, Mike So [32] applied the modified rescaled range test (R/S) proposed by Lo
(1991) and the semiparametric test (GPH) proposed by Geweke and Porker-Hudak
[33] to detect the existence of long-term dependence in volatility in the S & P 500
index and DJIA index. He used three proxies of the variability of returns to achieve
\n\n=== PAGE 443 ===\nthis result: the absolute mean deviation, the squared mean deviation, and the
logarithm of the absolute mean deviation. Mariani et al. [31], in their paper, used the
Hurst exponent and the Detrended fluctuation analysis methodology to show the
existence of long-memory effects in the international market indices, that is, Morgan
Stanley Capital International Europe, Australasia, and Far East index (MSCI EAFE),
and the Emerging market index, and compare their result with S&P 500. They also
found that immediately before the crisis the estimate of the long memory increases,
while during the time of the crisis, the stocks behave randomly. Finally, Breidt et al.
[4] also used their proposed long-memory stochastic volatility model to indicate the
existence of long memory in financial indices. Hence, our results of long memory in
the indices volatility reinforced previous results by different methodology.
The ARFIMA(0,d,0)-GARCH specification was preferred for BAC and JPM high-
frequency series whose volatility was found to be intermediate. The intermediate
memory found in the HFD is consistent with previous results of Barany et al. (2011)
[34] and Mariani et al. (2009) [35] except with the WMT and IBM HFD volatility
that was best described using the GARCH model, which has short memory. Mariani
et al. (2009) [35] employed the relationship between the Hurst parameter (H) and
the Detrended fluctuation analysis parameter (α) and compared with the value 0.5 to
investigate the memory behavior of BAC and JPM HFD from March 10 to March 18,
2008 (Bear Stearns financial crisis). Their results gave Hurst estimate of 0.63 and
0.62 for BAC and JPM, respectively, which implies long memory. Our values for the
fractional difference parameter estimate 0.1 and 0.236 for the BAC and JPM,
respectively, also showed long memory.
The earthquake series was divided into two regions: symmetrically (EQ1) and
nonsymmetriclly (EQ2) distributed. Both regions showed intermediate memory. Our
analysis indicated that the earthquake series showed long memory while the
explosive series showed short memory. On the other hand, both the explosive and
the earthquake series volatility showed long memory, but the persistence in the
explosive volatility was higher than that of the earthquake. The order of persistence
(memory) in series volatility from highest to lowest is as follows:
1. Indices (DJIA and S&P 500) and explosives data
2. Earthquake data
3. BAC and JPM high-frequency financial data
4. WMT and IBM high-frequency financial data
The predictions made from the MA(1) − FIGARCH(1, d, 1) model for the DJIA index
offered good results since all the actual observations were within the prediction
limits. The earthquake series predictions from our assumed model were fairly
accurate since we had 8 out of 10 earthquake’s directions correctly predicted.
The outliers observed from the generalized error Q–Q plot for the indices
correspond to significant drops in prices in a short time period (1 or 2 days). This is
what happened on October 15, 2008, for both DJIA and S&P 500 indices.
In Table 12.1, we have the following set of parameters:
\n\n=== PAGE 444 ===\n1. α—parameter α as in (12.12).
2. KS statistics—value of the KS statistics.
3. P-value—p-value for the KS test and hypothesis H0: Data are following TLF
distribution.
4. TFL fit kurtosis—kurtosis computed according to the formula (12.13).
5. Sample kurtosis—sample kurtosis computed in the following way:
12.6.1.1   ADF test
12.6.1.2   KPSS test
12.6.1.3   ARCH test
12.6.1.4   Whittle estimate of d
12.6.1.5   AIC
12.6.1.6   BIC
12.6.1.7     Estimate
Acknowledgments
The authors thank Steve Wang for providing data sets on generic diversity in the
fossil record. We are especially grateful to Dr. Ionut Florescu for having shared high-
frequency data with us.
12.A   Appendix A—Big ‘O’ notation
Big O notation describes limit behavior of a function. Either where we are taking
limit toward infinity or some finite value. For example, if we say that
this precisely means that
In other words, f(x) behaves like g(x) for large values of x.
Similarly we can think of an example in which Δx goes to 0:
\n\n=== OCR PAGE 444 ===\n1. a—parameter a as in (12.12).
2. KS statistics—value of the KS statistics.

3. P-value—p-value for the KS test and hypothesis H,: Data are following TLF
distribution.

4. TFL fit kurtosis—kurtosis computed according to the formula (12.13).

5. Sample kurtosis—sample kurtosis computed in the following way:
Lyn x4
= DX, - X)
(4 DE, - XP
12.6.1.1 ADF test
12.6.1.2 KPSS test
12.6.1.3 ARCH test
12.6.1.4 Whittle estimate of d
12.6.1.5 AIC

12.6.1.6 BIC
12.6.1.7 d Estimate

Acknowledgments

The authors thank Steve Wang for providing data sets on generic diversity in the
fossil record. We are especially grateful to Dr. Ionut Florescu for having shared high-
frequency data with us.

12.A Appendix A—Big ‘O’ notation

Big O notation describes limit behavior of a function. Either where we are taking
limit toward infinity or some finite value. For example, if we say that

f(x) = O(g(x)) as x goes to infinity

this precisely means that

lim £@) =M,M < +o.
x00 g(x)
In other words, f(x) behaves like g(x) for large values of x.

Similarly we can think of an example in which Ax goes to 0:
\n\n=== PAGE 445 ===\nthis precisely means that
In other words, f(Δx) behaves like (Δx)2 for x going to 0.
References
1. J.J. Sepkoski (2001). Mass Extinctions, Concept. Encyclopedia of Biodiversity,
Volume 4, Academic Press.
2. R.K. Bambach, A.H. Knoll, S. Wang (2004). Origination, extinction, and mass
depletions of marine diversity. Paleobiology 30, 522–542.
3. S.M. Ross (2009). Introduction to Probability Models. Academic Press.
4. F.J. Breidt, N. Crato, P. De Lima (1998). The detection and estimation of long
memory in stochastic volatility. Journal of Econometrics 83, 325–348.
5. P.M. Robinson. (1991) Testing for strong serial correlation and dynamic
conditional heteroskedasticity in multiple regression. Journal of Econometrics
47, 67–84.
6. N. Shephard. (1996) Statistical aspects of ARCH and stochastic volatility. In D.
R. Cox, D. B. Hinkley, and O. E. Barndorff-Nielsen, editors. Time Series Models:
In Econometrics, Finance and Other Fields. Chapman Hall, London.
7. I.N. Lobato, N.E. Savin. (1998) Real and spurious long-memory properties of
stock-market data. Journal of Business & Economic Statistics 16, 261–283.
8. R.T. Baillie. (1996) Long memory processes and fractional integration in
econometrics. Journal of Econometrics 73, 5–59.
9. O.E. Barndorff-Nielsen, N. Shephard. (2001). Modelling by Levy processes for
financial econometrics. In Levy Processes. Birkhauser, Boston, MA, pp. 283–
318.
10. G.E.P. Box, G.M. Jenkins, G.C. Reinsel (1994). Time Series Analysis: Forecasting
and Control. Prentice Hall.
11. R.F. Engle (1982). Autoregressive conditional heteroscedasticity with estimates
of variance of United Kingdom inflation. Econometrica, 50 (4), 987–1007.
12. T. Bollerslev (1986). Generalized autoregressive conditional heteroskedasticity.
Journal of Econometrics 31, 307–327.
13. J.M. Zakoian (1994). Threshold heteroskedastic models. Journal of Economic
Dynamics and Control 18, 931–955.
14. R.F. Engle, V.K. Ng (1991). Measuring and testing the impact of news on
volatility. Journal of Finance 48(5), 1749–1778.
15. D.B. Nelson. (1991). Conditional heteroskedasticity in asset returns: A new
approach. Econometrica 59, 347–370.
16. A.C. Harvey, E. Ruiz, N. Shephard. (1994). Multivariate stochastic variance
models. Review of Economic Studies 61, 247–265.
17. R.T. Baillie, T. Bollerslev, H. O. Mikkelsen. (1996). Fractionally integrated
generalized autoregressive conditional heteroskedasticity. Journal of
\n\n=== OCR PAGE 445 ===\nf(Ax) = O(Ax)?) as Ax goes to 0

this precisely means that

Ax

tim £22 ~ yu < 400.
Ar0 (Ax)?

In other words, f(Ax) behaves like (Ax)? for x going to o.

References

1. J.J. Sepkoski (2001). Mass Extinctions, Concept. Encyclopedia of Biodiversity,
Volume 4, Academic Press.

2. R.K. Bambach, A.H. Knoll, S. Wang (2004). Origination, extinction, and mass
depletions of marine diversity. Paleobiology 30, 522-542.

3. S.M. Ross (2009). Introduction to Probability Models. Academic Press.

4. F.J. Breidt, N. Crato, P. De Lima (1998). The detection and estimation of long
memory in stochastic volatility. Journal of Econometrics 83, 325-348.

5. P.M. Robinson. (1991) Testing for strong serial correlation and dynamic
conditional heteroskedasticity in multiple regression. Journal of Econometrics
47, 67-84.

6. N. Shephard. (1996) Statistical aspects of ARCH and stochastic volatility. In D.
R. Cox, D. B. Hinkley, and O. E. Barndorff-Nielsen, editors. Time Series Models:
In Econometrics, Finance and Other Fields. Chapman Hall, London.

7. IN. Lobato, N.E. Savin. (1998) Real and spurious long-memory properties of
stock-market data. Journal of Business & Economic Statistics 16, 261-283.

8. R.T. Baillie. (1996) Long memory processes and fractional integration in
econometrics. Journal of Econometrics 73, 5-59.

g. O.E. Barndorff-Nielsen, N. Shephard. (2001). Modelling by Levy processes for
financial econometrics. In Levy Processes. Birkhauser, Boston, MA, pp. 283-
318.

10. G.E.P. Box, G.M. Jenkins, G.C. Reinsel (1994). Time Series Analysis: Forecasting
and Control. Prentice Hall.

u1. R.F. Engle (1982). Autoregressive conditional heteroscedasticity with estimates
of variance of United Kingdom inflation. Econometrica, 50 (4), 987-1007.

12. T. Bollerslev (1986). Generalized autoregressive conditional heteroskedasticity.
Journal of Econometrics 31, 307-327.

13. J.M. Zakoian (1994). Threshold heteroskedastic models. Journal of Economic
Dynamics and Control 18, 931-955.

14. R.F. Engle, V.K. Ng (1991). Measuring and testing the impact of news on
volatility. Journal of Finance 48(5), 1749-1778.

15. D.B. Nelson. (1991). Conditional heteroskedasticity in asset returns: A new
approach. Econometrica 59, 347-370.

16. A.C. Harvey, E. Ruiz, N. Shephard. (1994). Multivariate stochastic variance
models. Review of Economic Studies 61, 247-265.

17. R.T. Baillie, T. Bollerslev, H. O. Mikkelsen. (1996). Fractionally integrated
generalized autoregressive conditional heteroskedasticity. Journal of
\n\n=== PAGE 446 ===\nEconometrics 74, 3–30.
18. P.M. Robinson, M. Henry (1999). Long and short memory conditional
heteroskedasticity in estimating the memory parameter of levels. Econometric
Theory 15, 299–336.
19. R.F. Engle, T. Bollerslev (1986). Modelling the persistence of conditional
variances. Econometric Reviews 5, 1-50, 81–87.
20. G. Casella, R.L. Berger (2002). Statistical inference, Duxbury advanced series in
statistics and decision sciences. Thomson Learning.
21. W.J. Stewart (2009). Probability, Markov Chains, Queues, and Simulation: The
Mathematical Basis of Performance Modeling, Princeton University Press.
22. P. Lévy (1925). Calcul des probabilités, Gauthier-Villars, Paris.
23. A. Ya. Khintchine, P. Lévy (1936). Sur les lois stables, C. R. Acad. Sci. Paris 202,
374.
24. J. Voit (2005). The Statistical Mechanics of Financial Markets. Springer.
25. R.N. Mantegna, H.E. Stanley (1994). Physical Review Letters 73, 2946.
26. I. Koponen (1995). Physical Review E 52, 1197.
27. R.H. Shumway, D.S. Stoffer (1999). Time Series Analysis and Its Application.
Springer Science, 2011.
28. M.C. Mariani, Y. Liu (2007, April 15). Normalized truncated Levy walks applied
to the study of financial indices. Physica A: Statistical Mechanics and Its
Applications 377(2), 590–598, ISSN 0378-4371, 10.1016/j.physa.2006.11.066.
29. R.N. Mantegna, H.E. Stanley (1999). An Introduction to Econophysics:
Correlations and Complexity in Finance. Cambridge University Press,
Cambridge.
30. R. Weron (2001). Levy-stable distributions revisited: Tail index 2 does not
exclude the Levy-stable regime. International Journal of Modern Physics C 12,
209–223.
31. M. Mariani, I. Florescu, M.P.B. Varela, and E. Ncheuguim (2010). Study of
memory effect in international market indices. Physica A 2010, 389(8), 1653–
1646.
32. M.P. So (2010). Long-term memory in stock market volatility. Applied Financial
Economics 10(5), 519–524.
33. J. Geweke, S. Porter-Hudak (1983). The estimation and application of long
memory time series models. Journal of Time Series Analysis, 4(4), 221–238.
34. E. Barany, M.P Varela, I. Florescu, I. Sengupta (2011). Detecting market crashes
by analysing long-memory effect using high-frequency data. Quantitative
Finance 12(4), 515–518.
35. M. Mariani, I. Florescu, M.P.B. Varela, E. Ncheuguim (2009). Long correlations
and Levy models applied to the study of memory effects in high frequency (tick)
data. Physica A 388(8), 1659–1664.
\n\n=== PAGE 447 ===\nIndex
Note: Page numbers followed by f and t refer to figures and tables,
respectively.
ABD test
list of studies on
ACF, see autocorrelation function (ACF)
additive regression model
ADF test, see augmented Dickey–Fuller test (ADF test)
adiabatic quantum computing
age, generic diversity in fossil record
AIC of models
algorithms
classes
complexity
performance
ARCH effect
ARFIMA(p,d,q)-GARCH(r,s) process
ARFIMA(p,d,q) model
ARMA(2,1) model
ARMA(p,q) model
augmented Dickey–Fuller test (ADF test)
autocorrelation function (ACF)
backward-looking test
Bai–Perron test
of coal/WTI log prices ratio
\n\n=== PAGE 448 ===\nBayes methodology
BIC of models
bicubic interpolation
biharmonic interpolation
bilinear interpolation
algorithm
nonlinear
unit square
bipower variation (BP)
BNS tests
BP, see bipower variation (BP)
Brownian distance correlation
and Granger causality test
significance level of
Cauchy (Lorentz) distribution
CDO, see collateralized debt obligation (CDO)
Chimera graph
classical optimization techniques
class NP algorithms
class NP-complete algorithms
class NP-hard algorithms
class P algorithms
collateralized debt obligation (CDO)
tranche loss function and
combinatorial optimization
conditional mean model (returns)
correlation sensitivities, for tranche loss
\n\n=== PAGE 449 ===\ncritical region selection, high-frequency prices jumps
cumulative sum (CUSUM) timing
examples
lazy simple random walk
random walk expected gain over subperiod
simple random walk
overview
process
scheme
stopping time
two-sided
CUSUM statistic process
CUSUM stopping rule
CUSUM strategy Monte Carlo
data-driven testing procedure, for high-frequency prices jumps
microstructure noise
SPY data
d Estimate, long-memory volatility models
deterministic model, governing equations for
application to geophysical (earthquake data)
results
distance correlation
distance covariance
diversified portfolio
diversity, in fossil record
DJIA, see Dow Jones Index (DJIA)
Dow Jones Index (DJIA) returns
\n\n=== PAGE 450 ===\nD-wave simulator
D-wave system
mapping for
results from
electricity generation from fossil fuels
embedding
Expectation Maximization (EM) algorithm
explosive series, analysis of
ext, generic diversity in fossil record
filtered empirical distributions at t1,…, tT
evolution step for
selection step
financial model for optimization
forward-looking tests
fossil fuels prices
Bai–Perron test for
data for
electricity generation and
log prices, correlation matrix of
fractional Brownian motion
fractional IGARCH (FIGARCH) model
\n\n=== PAGE 451 ===\nfunctional central limit theorem
for multivariate linear Hawkes processes
for univariate nonlinear Hawkes processes
gain over subperiod
calculation for
random walk expected
GARCH(p,q) model
geophysics and finance using
Gaussian copula model
tranche loss function and
Gaussian inequalities
generalized extreme value (GEV) distribution
generalized method of moments
generic diversity in fossil record, analysis of
introduction
Johnson transformation (JT) function for
Lévy distribution for
data analysis with TLF distribution
stable distributions, characterization of
sum of random variables with different parameters
truncated
statistical and numerical analysis
statistical preliminaries and results
general version for n random variables
sum of exponential random variables with different
parameters
genetic algorithm formulation
\n\n=== PAGE 452 ===\ngeophysics and finance using GARCH models, volatility structures in
data collection, analysis, and result
ARCH effect
ARMA(2,2)
conditional mean model (returns)
on Dow Jones Index (DJIA) returns
on high-frequency, earthquake, and explosives series
model diagnostic of conditional returns with conditional
variance
model diagnostics
model selection and specification
one-step ahead prediction of last 10 observations
returns and variance equation
standardized residuals test
long memory models
ARFIMA(p,d,q)-GARCH(r,s)
ARFIMA(p,d,q) model
detection and estimation of
short memory models
ARMA(p,q) model
GARCH(p,q) model
IGARCH(1,1) model
geophysics (earthquake data)
deterministic model, application to
Lévy flights application to
MATLAB program code description for
using GARCH models
\n\n=== PAGE 453 ===\nGranger causality
and Brownian distance correlation
definition of
significance level of
testing of
vector autoregressive (VAR) model for
graph-theoretic combinatorial optimization models
Grey relational analysis approach
Gumbel distribution
HAR-MA model, least-square estimation of
\n\n=== PAGE 454 ===\nHawkes processes
applications of
measuring endogeneity (reflexivity)
modeling jump-diffusion
modeling order arrivals
modeling price jumps
branching structure representation
brief history of
convergence of
functional central limit theorem for multivariate linear
Hawkes processes
functional central limit theorem for univariate nonlinear
Hawkes processes
law of large numbers for multivariate linear Hawkes processes
nearly unstable univariate linear Hawkes processes
estimation
expectation maximization
generalized method of moments
maximum likelihood estimation
nonparametric
in high frequency financial data modeling
hypothesis testing of
approximate thinning
random time change
linear
multivariate marked
\n\n=== PAGE 455 ===\nsimulation
by branching structure
inverse CDF transform
Ogata's modified thinning
stationarity
statistical inference of
Hermite multifractal random walk (HMRW)
simulation of
Hermite processes
financial applications
simulation of HMRW
statistical properties
fractional Brownian motion and
infinitely divisible cascading noise
multifractal random walk driven by
definition
existence
properties of
properties of
Wiener integrals with respect to
Hermite random variable
heuristic optimization techniques
hidden Markov process
high-frequency market data, application to
methodology
results
\n\n=== PAGE 456 ===\nhigh-frequency prices jumps, stochastic volatility
data-driven testing procedure
microstructure noise
SPY data
empirical results
backward-looking test
interpolated test
generalized testing procedure
critical region selection
spot volatility estimation
intraday jump tests
ABD test
BNS test
LM test
realized volatility measure test
simulation study
model specification
results
high-frequency (tick) data, analysis of
HMRW, see Hermite multifractal random walk (HMRW)
idle time, of trading strategy
index of stability
infinitely divisible cascading noise
integrated GARCH (IGARCH) model
integrated volatility (IV)
intermediate memory process
interpolated test
\n\n=== PAGE 457 ===\ninterpolation methods
bicubic interpolation
biharmonic interpolation
bilinear interpolation
algorithm
nonlinear
unit square
nearest-neighbor interpolation
numerical applications of
thin plate splines
physical analogy
radial basis function
smoothness measure
intraday price jump tests
ABD test
BNS test
LM test
realized volatility measure test
intraday volatility pattern (IVP)
inverse CDF transform
Ising model
IV, see integrated volatility (IV)
IVP, see intraday volatility pattern (IVP)
Johnson transformation (JT) for diversity data
kernel function
Kolmogorov–Smirnov (KS) test
\n\n=== PAGE 458 ===\nKPSS test
Lagrangian relaxation
law of large numbers for multivariate linear Hawkes processes
lazy simple random walk, CUSUM timing
least-square estimation of HAR-MA model
Lévy distribution, for analysis of generic diversity in fossil record
data analysis with TLF distribution
stable distributions, characterization of
sum of random variables with different parameters
truncated
Lévy flights model
application to geophysics
to estimate crash dates
results
truncated distribution
Lévy–Smirnov distribution
Limit Order Books (LOB)
characteristics of
resiliency of
Linear Hawkes process
convergence of nearly unstable univariate
functional central limit theorem for multivariate
law of large numbers for multivariate
LM test
list of studies on
LOB, see Limit Order Books (LOB)
\n\n=== PAGE 459 ===\nLocal polynomial regression models
multiple regression
simple regression
Loess curve
long-memory generalized autoregressive conditionally
heteroskedastic (LMGARCH) models
long memory models
ARFIMA(p,d,q)-GARCH(r,s)
ARFIMA(p,d,q) model
detection and estimation of
augmented Dickey–Fuller test (ADF test)
KPSS test
Whittle method
Lowess curve
Lowess/loess method, nonparametric regression models
marked point process (MPP)
Markov chain
continuous time
obtaining parameters of
parameter estimation
Markov processes
Markowitz mean-variance model
Matlab code
in geophysics
WMIS
maximum independent set (MIS)
maximum likelihood estimation (MLE)
\n\n=== PAGE 460 ===\nminimum error thresholding method
minorize-maximization (MM) algorithm
MIS, see maximum independent set (MIS)
mixed models
MLE, see maximum likelihood estimation (MLE)
Monofractal process
Monte Carlo, CUSUM strategy
MPP, see marked point process (MPP)
MRW, see multifractal random walk (MRW)
multifractality
multifractal random walk (MRW)
defined
by Hermite processes
definition
existence
properties of
overview
properties
statistical properties of
multivariate marked Hawkes process
multivariate point process
nearest-neighbor interpolation
negative signals
nonparameteric estimation of Hawkes process
\n\n=== PAGE 461 ===\nnonparametric regression models
application to
financial data sampled with high frequency
geophysics
highlights and discussions
generalized
local polynomial regression
multiple regression
simple regression
lowess/loess method
NP algorithms
NP-complete algorithms
NP-hard algorithms
Ogata's modified thinning method
\n\n=== PAGE 462 ===\noptimization, portfolio
background of
binary optimization on
discussion
diversified
financial model for
future research on
graph-theoretic combinatorial models for
hardware limitations in
implementation limitations in
Ising model for
methods for
input data
mapping implementation
mean-variance calculations
model implementation
risk measures implementation
mixed models for
model limitations in
models
QUBO model for
results of
restricted minimum-risk model
simple correlation model
WMIS minimum-risk, max return model
Orig, generic diversity in fossil record
Ornstein–Uhlenbeck (OU) process
\n\n=== PAGE 463 ===\nP algorithms
Phanerozoic
point processes. See also Hawkes processes
definition
marked
moments
multivariate
Poisson processes
random time change
stochastic intensities and
Poisson processes
\n\n=== PAGE 464 ===\nportfolio optimization
background of
binary optimization on
discussion
diversified
financial model for
future research on
graph-theoretic combinatorial models for
hardware limitations in
implementation limitations in
Ising model for
methods for
input data
mapping implementation
mean-variance calculations
model implementation
risk measures implementation
mixed models for
model limitations in
models
QUBO model for
results of
restricted minimum-risk model
simple correlation model
WMIS minimum-risk, max return model
positive signals
proximal interpolation, see nearest-neighbor interpolation
\n\n=== PAGE 465 ===\nquadratic unconstrained binary optimization (QUBO)
quadratic variation (QV), of price
QUBO, see quadratic unconstrained binary optimization (QUBO)
random walk expected gain over subperiod
realized volatility measure test
regime switching volatility model
restricted minimum-risk model
Rosenblatt process
scale invariance model, application to
methodology
results
scale invariance model, application to high-frequency market data
methodology
results
scatterplot smoothing, see nonparametric regression models
short memory models
ARMA(p,q) model
GARCH(p,q) model
IGARCH(1,1) model
Sidàk correction
signals
negative
positive
properties
sequence of
and subperiod
\n\n=== PAGE 466 ===\nsimple correlation model
simple random walk, CUSUM timing
simulation
of Hawkes processes
by branching structure
inverse CDF transform
Ogata's modified thinning
of HMRW
speed of reaction, CUSUM
spot volatility estimation, high-frequency prices jumps
square-root effect, threshold parameter
stable distributions
characterization of
introduction of
standard maximum likelihood method
stochastic intensities, point processes and
\n\n=== PAGE 467 ===\nstochastic volatility model
in analysis of
during earthquake
earthquake signal
end of earthquake signal, aftershocks
seismometer readings during earthquake
empirical testing of
in finance area
with hidden Markov process
historical note on
jumps in high-frequency prices under
methodology
obtaining filtered empirical distributions at t1,…, tT
obtaining parameters of Markov chain
in physical data application
and problem
theoretical results of
about convergence
about parameter estimation
Markov chain parameter estimation
particle filter, working of
stock indices, analysis of
stopping time, CUSUM
subperiod
gain over
signal and
\n\n=== PAGE 468 ===\nSV2FJ_2ρ model
calibration of ξ under
minimized loss function loss for
parameter values of, 156t
thin plate splines (TPSs), 282–285
physical analogy
radial basis function
smoothness measure
threshold GARCH (TGARCH) model
TPSs, see thin plate splines (TPSs)
tranche
tranche loss function
correlation sensitivities for
trend-based trading strategy
gain over subperiod
signaling
threshold parameter, effect of
trends
for US treasury notes
trends
truncated Lévy flights (TLF) distribution
data analysis with
infinite divisibility
kurtosis
two-sided CUSUM (2-CUSUM)
US Clean Air Act
\n\n=== PAGE 469 ===\nUS treasury notes, trading strategy for
variance–covariance matrix
vector autoregressive (VAR) model
volatility
clustering
structures in geophysics and finance using GARCH models
data collection, analysis, and result
long memory models
short memory models
weighted MIS (WMIS)
Matlab Code
White/Terasvirta test
Whittle estimate of long-memory parameter (d)
Whittle method
Wiener–Hopf equation
Wiener integrals with respect to Hermite processes
Wiener process
WMIS, see weighted MIS (WMIS)
WMIS minimum-risk, max return model
\n\n=== PAGE 470 ===\nWiley Handbooks in
FINANCIAL ENGINEERING AND
ECONOMETRICS
Advisory Editor
Ruey S. Tsay
The University of Chicago Booth School of Business, USA
The dynamic and interaction between financial markets around the
world have changed dramatically under economic globalization. In
addition, advances in communication and data collection have
changed the way information is processed and used. In this new era,
financial instruments have become increasingly sophisticated and
their impacts are far-reaching. The recent financial (credit) crisis is a
vivid example of the new challenges we face and continue to face in
this information age. Analytical skills and ability to extract useful
information from mass data, to comprehend the complexity of
financial instruments, and to assess the financial risk involved
become a necessity for economists, financial managers, and risk
management professionals. To master such skills and ability,
knowledge from computer science, economics, finance, mathematics
and statistics is essential. As such, financial engineering is cross-
disciplinary, and its theory and applications advance rapidly.
The goal of this Handbook Series is to provide a one-stop source for
students, researchers, and practitioners to learn the knowledge and
analytical skills they need to face today's challenges in financial
markets. The Series intends to introduce systematically recent
developments in different areas of financial engineering and
econometrics. The coverage will be broad and thorough with balance
in theory and applications. Each volume will be edited by leading
researchers and practitioners in the area and covers state-of-the-art
methods and theory of the selected topic.
\n\n=== PAGE 471 ===\nPublished Wiley Handbooks in Financial Engineering and
Econometrics
Bauwens, Hafner, and Laurent · Handbook of Volatility Models and
Their Applications
Brandimarte · Handbook in Monte Carlo Simulation: Applications
in Financial Engineering, Risk Management, and Economics
Chan and Wong · Handbook of Financial Risk Management:
Simulations and Case Studies
Cruz, Peters, and Shevchenko · Fundamental Aspects of Operational
Risk and Insurance Analytics: A Handbook of Operational Risk
Florescu, Mariani, Stanley, and Viens · Handbook of High-
Frequency Trading and Modeling in Finance
James, Marsh, and Sarno · Handbook of Exchange Rates
Peters and Shevchenko · Advances in Heavy Tailed Risk Modeling:
A Handbook of Operational Risk
Viens, Mariani, and Florescu · Handbook of Modeling High-
Frequency Data in Finance
Szylar · Handbook of Market Risk
Veronesi · Handbook of Fixed-Income Securities
Forthcoming Wiley Handbooks in Financial Engineering
and Econometrics
Bali and Engle · Handbook of Asset Pricing
Chacko · Handbook of Credit and Interest Rate Derivatives
Jacquier · Handbook of Econometric Methods for Finance:
Bayesian and Classical Perspecitves
Longin · Handbook of Extreme Value Theory and Its Applications to
Finance and Insurance
Starer · Handbook of Equity Portfolio Management: Theory and
Practice
Szylar · Handbook of Hedge Fund Risk Management and
Performance: In a Challenging Regulatory Environment
\n\n=== PAGE 472 ===\nSzylar · Handbook of Macroeconomic Investing
\n\n=== OCR PAGE 472 ===\nSzylar - Handbook of Macroeconomic Investing
\n\n=== PAGE 473 ===\nWILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.
\n